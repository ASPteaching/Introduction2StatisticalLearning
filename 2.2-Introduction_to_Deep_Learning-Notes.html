<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-05-13">

<title>Introduction to Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="2.2-Introduction_to_Deep_Learning-Notes_files/libs/clipboard/clipboard.min.js"></script>
<script src="2.2-Introduction_to_Deep_Learning-Notes_files/libs/quarto-html/quarto.js"></script>
<script src="2.2-Introduction_to_Deep_Learning-Notes_files/libs/quarto-html/popper.min.js"></script>
<script src="2.2-Introduction_to_Deep_Learning-Notes_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="2.2-Introduction_to_Deep_Learning-Notes_files/libs/quarto-html/anchor.min.js"></script>
<link href="2.2-Introduction_to_Deep_Learning-Notes_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="2.2-Introduction_to_Deep_Learning-Notes_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="2.2-Introduction_to_Deep_Learning-Notes_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="2.2-Introduction_to_Deep_Learning-Notes_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="2.2-Introduction_to_Deep_Learning-Notes_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#towards-deep-learning" id="toc-towards-deep-learning" class="nav-link active" data-scroll-target="#towards-deep-learning">Towards Deep Learning</a>
  <ul class="collapse">
  <li><a href="#automatic-tuning" id="toc-automatic-tuning" class="nav-link" data-scroll-target="#automatic-tuning">Automatic tuning</a></li>
  </ul></li>
  <li><a href="#convolutional-neural-networks" id="toc-convolutional-neural-networks" class="nav-link" data-scroll-target="#convolutional-neural-networks">Convolutional Neural networks</a>
  <ul class="collapse">
  <li><a href="#how-and-what-do-computers-see" id="toc-how-and-what-do-computers-see" class="nav-link" data-scroll-target="#how-and-what-do-computers-see">How and What do computers see</a></li>
  <li><a href="#convolutional-layer" id="toc-convolutional-layer" class="nav-link" data-scroll-target="#convolutional-layer">Convolutional Layer</a>
  <ul class="collapse">
  <li><a href="#one-layer-of-a-convolutional-neural-network" id="toc-one-layer-of-a-convolutional-neural-network" class="nav-link" data-scroll-target="#one-layer-of-a-convolutional-neural-network">One-layer of a convolutional neural network</a></li>
  <li><a href="#deep-convolutional-network" id="toc-deep-convolutional-network" class="nav-link" data-scroll-target="#deep-convolutional-network">Deep Convolutional Network</a></li>
  </ul></li>
  <li><a href="#pooling-layers" id="toc-pooling-layers" class="nav-link" data-scroll-target="#pooling-layers">Pooling Layers</a>
  <ul class="collapse">
  <li><a href="#average-pooling" id="toc-average-pooling" class="nav-link" data-scroll-target="#average-pooling">Average pooling</a></li>
  </ul></li>
  <li><a href="#fully-connected-layer" id="toc-fully-connected-layer" class="nav-link" data-scroll-target="#fully-connected-layer">Fully Connected Layer</a></li>
  <li><a href="#cnn-definition-in-keras" id="toc-cnn-definition-in-keras" class="nav-link" data-scroll-target="#cnn-definition-in-keras">CNN definition in Keras</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="2.2-Introduction_to_Deep_Learning-Notes.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introduction to Deep Learning</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Alex Sanchez </p>
             <p>Esteban vegas </p>
             <p>Ferran Reverter </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 13, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="towards-deep-learning" class="level1">
<h1>Towards Deep Learning</h1>
<p>When one looks at how the concept of Neural Network is built, it is clear that it is characterized (accompanied) by an increase in complexity, that is the number of neurons, layers and connexions.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Shallow2Deep_NN.png" class="img-fluid figure-img"></p>
<figcaption>Source: ‘Deep Learning’ course, by Andrew Ng in Coursera &amp; deeplearning.ai</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="images/Shallow2Deep_NN(2).png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>However, the real shift, when going from shallow to deep neural networks, is not (only) the number of layers. This is mainly because, very often, Deep NNs do not have so many hidden layers that <em>per se</em> would justify the term deep.</p>
<p>The difference came rather from realizing, as happened at some point, that while some tasks as digit recognition, could be solve decently well using a “brute force” approach, for instance, with a NN on the pixels of the images other more complex tasks, such as distinguishing a human face in an image, where hard to solve witht that “brute” force approach.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/HumanvsNonHuman.png" class="img-fluid figure-img"></p>
<figcaption>Source: ‘Neural Networkls and Deep Learning’ course, by Michael Nielsen</figcaption>
</figure>
</div>
</div>
</div>
<p>This problem can be attacked as the digit recognition proble, with an output of “yes” and “no”, although the cost of training the network would be much higher.</p>
<p>An alternative approach may be to try to solve the problem hierarchically.</p>
<ul>
<li>We start by tying to find edges in the figure</li>
<li>In the parts with edges we “look around” to find face pieces, a nose, an eye, an eyebrow …</li>
<li>As we locate the pieces we look for their optimal combination.</li>
</ul>
<p>Each layer has a more complex task, but it receives better information.The rationale underlying this idea is that, if we can solve the sub-problems using neural networks, then perhaps we can build a neural network for face-detection, by combining the networks for the sub-problems.</p>
<p>If we are able to break the questions down, further and further through multiple layers we end-up working with sub-networks that answer questions so simple they can easily be answered at the level of single pixels.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/FaceRecognitionIntuiotion.png" class="img-fluid figure-img"></p>
<figcaption>Source: ‘Deep Learning’ course, by Andrew Ng in Coursera &amp; deeplearning.ai</figcaption>
</figure>
</div>
</div>
</div>
<p>According to <a href="http://neuralnetworksanddeeplearning.com/chap1.html">Michael Nielsen</a> this results in networks that break down complicated questions, such as “does the image show a face”, into much simpler questions answerable at the level of single pixels.</p>
<p>It does this through a series of many layers, with early layers answering very simple and specific questions about the input, and later layers building up a hierarchy of ever more complex and abstract concepts.</p>
<p>Networks with this kind of many-layer structure - two or more hidden layers - are called <em>deep neural networks</em>.</p>
<section id="automatic-tuning" class="level3">
<h3 class="anchored" data-anchor-id="automatic-tuning">Automatic tuning</h3>
<p>In order for these networks to succeed it is important not having to hand-craft the complicated structure of weights and biases required for such hierarchy of layers and functions.</p>
<p>It was not until 2006 that techniques enableing learning in deep neural nets were developed. These deep learning techniques are based on stochastic gradient descent and backpropagation, but also introduce new ideas.</p>
<p>It turns out that equiped with such techniques, deep neural networks perform much better on many problems than shallow neural networks. The reason, of course, is the ability of deep nets to build up a complex hierarchy of concepts.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ShallowVSDeepNN.png" class="img-fluid figure-img"></p>
<figcaption>Source: ‘Deep Learning’ course, by Andrew Ng in Coursera &amp; deeplearning.ai</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="convolutional-neural-networks" class="level1">
<h1>Convolutional Neural networks</h1>
<section id="how-and-what-do-computers-see" class="level2">
<h2 class="anchored" data-anchor-id="how-and-what-do-computers-see">How and What do computers see</h2>
<p>Computer vision is an exciting field, which has evolved quickly thanks to deep learning. Researchers in this area have been experimenting many neural-network architectures and algorithms, which have influenced other fields as well. In computer vision, images are the training data of a network, and the input features are the pixels of an image. These features can get really big. For example, when dealing with a 1megapixel image, the total number of features in that picture is 3 million (<span class="math inline">\(=1000\times 1000 \times 3\)</span> color channels). Then imagine passing this through a neural network with just 1000 hidden units, and we end up with some weights of 3 billion parameters! These numbers are too big to be managed, but, luckily, we have the perfect solution: Convolutional neural networks (CNN), see Figure 1.</p>
<p><code>{r, fig.align='center', out.width='100%', fig.cap='Source: Generative Deep Learning. David Foster (Fig. 2.13)'}\nknitr::include_graphics(\"![Convolutional Neural Network](images/cnn.png{width=60%, fig.pos=\"h\"}\")</code></p>
<p>There are 3 main types of layers in a convolutional network:</p>
<ul>
<li>Convolution (CONV)</li>
<li>Pooling (POOL)</li>
<li>Fully connected (FC)</li>
</ul>
</section>
<section id="convolutional-layer" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-layer">Convolutional Layer</h2>
<p>A “convolution” is one of the building blocks of the Convolutional network. The primary purpose of a “convolution” in the case of a CNN is to extract features from the input image (Fig. 2).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/abs_cat.png" class="img-fluid figure-img" data-fig.pos="h"></p>
<figcaption>Spatial hierarchy of visual modules</figcaption>
</figure>
</div>
<p>Every image can be represented as a matrix of pixel values. An image from a standard digital camera will have three channels: red, green and blue. You can imagine those as three 2d-matrices stacked over each other (one for each color), each having pixel values in the range 0 to 255.</p>
<p>Applying a convolution to an image is like running a filter of a certain dimension and sliding it on top of the image. That operation is translated into an element-wise multiplication between the two matrices and finally an addition of the multiplication outputs. The final integer of this computation forms a single element of the output matrix. Let’s review this via an example, where we want to apply a filter (kernel) to detect vertical and horizontal edges from a 2D original image (Fig. 3).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/cnn_conv.png" class="img-fluid figure-img" data-fig.pos="h"></p>
<figcaption>Convolution filter</figcaption>
</figure>
</div>
<p>In this example, we used a value of a stride equal to 1, meaning the filter moves horizontally and vertically by one pixel (see Figure 4).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/img_1.PNG" class="img-fluid figure-img" data-fig.pos="h"></p>
<figcaption>Sliding the filter</figcaption>
</figure>
</div>
<p>In this example the values of the filter were already decided in the convolution. <strong>The goal of a convolutional neural network is to learn the values in the filters</strong>. We treat them as <strong>weights</strong> of the neural network, which the network learns from data using backpropagation.</p>
<p>You might be wondering how to calculate the output size, based on the filter dimensions and the way we slide it though the image. We will get to the formula, but first We want to introduce a bit of terminology.</p>
<p>You saw in the earlier example how the filter moved with a stride of 1 and covered the whole image from edge to edge. This is what it’s called a <strong>valid</strong> convolution since the filter stays within the borders of the image.</p>
<p>However, one problem quickly arises. When moving the filter this way we see that the pixels on the edges are “touched” less by the filter than the pixels within the image. That means we are throwing away some information related to those positions. Furthermore, the output image is shrinking on every convolution, which could be intentional, but if the input image is small, we quickly shrink it too fast.</p>
<p>A solution to those setbacks is the use of <strong>padding</strong>. Before we apply a convolution, we pad the image with zeros all around its border to allow the filter to slide on top and maintain the output size equal to the input. The result of padding in the previous example will be (Figure 5).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/img_2.PNG" class="img-fluid figure-img" data-fig.pos="h"></p>
<figcaption>Padding</figcaption>
</figure>
</div>
<p>Padding will result in a <strong>same</strong> convolution. We talked about <strong>stride</strong>, which is essentially how many pixels the filter shifts over the original image. Great, so now We can introduce the formula to quickly calculate the output size, knowing the filter size (<span class="math inline">\(f\)</span>), stride (<span class="math inline">\(s\)</span>), pad (<span class="math inline">\(p\)</span>), and input size (<span class="math inline">\(n\)</span>):</p>
<p>Output size</p>
<p><span class="math display">\[
\Big(\frac{n+2p-f}{s}+1\Big)\times \Big(\frac{n+2p-f}{s}+1\Big)
\]</span></p>
<p>Keep in mind that the filter size is usually an odd value, and if the fraction above is not an integer you should round it down.</p>
<p>The previous example was on a 2D matrix, but we mentioned earlier that images are composed of three channels (R-red, G-green, B-blue). Therefore the input is a volume, a stack of three matrices, which forms a depth identified by the number of channels. If we apply only one filter the result would be (Figure 6), where the cube filter of 27 parameters now slides on top of the cube of the input image.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/img_3.PNG" class="img-fluid figure-img" data-fig.pos="h"></p>
<figcaption>Output</figcaption>
</figure>
</div>
<p>So far we have only applied one filter at a time, but we can apply multiple filters to detect several different features. This is what brings us to the crucial concept for building convolutional neural networks. Now each filter brings us its own output We can stack them all together and create an output volume, such as, see Figures 7, 8 and 9.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/img_4.PNG" class="img-fluid figure-img" data-fig.pos="h"></p>
<figcaption>Output volume</figcaption>
</figure>
</div>
<p>Therefore, in general terms we have:</p>
<p>Input: <span class="math inline">\((n \times n \times n_c)\)</span></p>
<p>Filter: <span class="math inline">\((f \times f \times n_c)\)</span></p>
<p>Output: <span class="math inline">\(\Big(\big(\frac{n+2p-f}{s}+1\big)\times \big(\frac{n+2p-f}{s}+1\big)\times n'_c\Big)\)</span></p>
<p>(with <span class="math inline">\(n'_c\)</span> as the number of filters, which are detecting different features)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/cnn_filter_1.png" class="img-fluid figure-img" data-fig.pos="h"></p>
<figcaption>Representing a full color RGB image as a volume and applying a convolutional filter</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/cnn_filter_2.png" class="img-fluid figure-img" data-fig.pos="h"></p>
<figcaption>A three-dimensional visualization of a convolutional layer, where each filter corresponds to a slice in the resuting output volumen.</figcaption>
</figure>
</div>
<section id="one-layer-of-a-convolutional-neural-network" class="level3">
<h3 class="anchored" data-anchor-id="one-layer-of-a-convolutional-neural-network">One-layer of a convolutional neural network</h3>
<p>The final step that takes us to a convolutional neural layer is to add the bias and a non-linear function (Figure 10).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/img_5.PNG" class="img-fluid figure-img" data-fig.pos="h"></p>
<figcaption>Bias term</figcaption>
</figure>
</div>
<p>Remember that the parameters involved in one layer are independent of the input size image. So let’s consider, for example, that we have <span class="math inline">\(10\)</span> filters that are of size <span class="math inline">\(3\times 3\times 3\)</span> in one layer of a neural network. Each filter has <span class="math inline">\(27\)</span> <span class="math inline">\((=3\times 3\times 3) + 1\)</span> bias <span class="math inline">\(=&gt; 28\)</span> parameters. Therefore, the total amount of parameters in the layer is 280 (<span class="math inline">\(10\times 28\)</span>).</p>
<p>This means that all the neurons in the first hidden layer detect exactly the same feature, just at different locations in the input image. To see why this makes sense, suppose the weights and bias are such that the hidden neuron can pick out, say, a vertical edge in a particular local receptive field. That ability is also likely to be useful at other places in the image. And so it is useful to apply the same feature detector everywhere in the image. To put it in slightly more abstract terms, convolutional networks are well adapted to the translation invariance of images: move a picture of a cat (say) a little ways, and it’s still an image of a cat.</p>
<p>For this reason, we sometimes call the map from the input layer to the hidden layer a feature map. We call the weights defining the feature map the <strong>shared weights</strong>. And we call the bias defining the feature map in this way the <strong>shared bias</strong>. The shared weights and bias are often said to define a kernel or filter (Figures 11 and 12, also link to https://pathmind.com/wiki/convolutional-network).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/img_x1.PNG" class="img-fluid figure-img" data-fig.pos="h"></p>
<figcaption>Convolution process</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/img_x2.PNG" class="img-fluid figure-img" data-fig.pos="h"></p>
<figcaption>Convolution process</figcaption>
</figure>
</div>
</section>
<section id="deep-convolutional-network" class="level3">
<h3 class="anchored" data-anchor-id="deep-convolutional-network">Deep Convolutional Network</h3>
<p>We are now ready to build a complete deep convolutional neural network. The following architecture depicts a simple example of that (Figure 13)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/img_6.PNG" class="img-fluid figure-img" data-fig.pos="h"></p>
<figcaption>Stacked convolution layers</figcaption>
</figure>
</div>
</section>
</section>
<section id="pooling-layers" class="level2">
<h2 class="anchored" data-anchor-id="pooling-layers">Pooling Layers</h2>
<p>There are two types of pooling layers: max and average pooling. Max pooling We define a spatial neighborhood (a filter), and as we slide it through the input, we take the largest element within the region covered by the filter.</p>
<p>We can think of max-pooling as a way for the network to ask whether a given feature is found anywhere in a region of the image. It then throws away the exact positional information. The intuition is that once a feature has been found, its exact location isn’t as important as its rough location relative to other features. A big benefit is that there are many fewer pooled features, and so this helps reduce the number of parameters needed in later layers (Figure 14).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/img_7.PNG" class="img-fluid figure-img" data-fig.pos="h"></p>
<figcaption>Max-pooling</figcaption>
</figure>
</div>
<section id="average-pooling" class="level3">
<h3 class="anchored" data-anchor-id="average-pooling">Average pooling</h3>
<p>As the name suggests, it retains the average of the values encountered within the filter. One thing worth noting is the fact that a pooling layer does not have any parameters to learn. Of course, we have hyper-parameters to select, the filter size and the stride (it’s common not to use any padding).</p>
</section>
</section>
<section id="fully-connected-layer" class="level2">
<h2 class="anchored" data-anchor-id="fully-connected-layer">Fully Connected Layer</h2>
<p>A fully connected layer acts like a “standard” single neural network layer, where you have a weight matrix W and bias b. We can see its application in the following example of a Convolutional Neural Network. This network is inspired by the LeNet-5 network (Figure 15).</p>
<p>It’s common that, as we go deeper into the network, the sizes (nh, nw) decrease, while the number of channels (nc) increases.</p>
<p>Another common pattern you can see in neural networks is to have CONV layers, one or more, followed by a POOL layer, and then again one or more CONV layers followed by a POOL layer and, at the end, a few FC layers followed by a Softmax.</p>
<p>When choosing the right hyper-parameters (f, s, p, ..), look at the literature and choose an architecture that was successfully used and that can apply to your application. There are several “classic” networks, such as LeNet, AlexNet, VGG,</p>
<p>These networks are normally used in transfer learning, where we can use the weights coming from the existing trained network and then replace the output unit, since training such a big network from scratch would require a long time otherwise.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/img_8.PNG" class="img-fluid figure-img" data-fig.pos="h"></p>
<figcaption>Convolution network scheme</figcaption>
</figure>
</div>
</section>
<section id="cnn-definition-in-keras" class="level2">
<h2 class="anchored" data-anchor-id="cnn-definition-in-keras">CNN definition in Keras</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> <span class="dv">6</span>, <span class="at">kernel_size =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">5</span>), <span class="at">strides =</span> <span class="dv">1</span>, <span class="at">padding =</span> <span class="st">"valid"</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">activation =</span> <span class="st">"relu"</span>,<span class="at">input_shape =</span> <span class="fu">c</span>(<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_max_pooling_2d</span>(<span class="at">pool_size =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> <span class="dv">16</span>, <span class="at">kernel_size =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">5</span>), <span class="at">strides =</span> <span class="dv">1</span>, <span class="at">padding =</span> <span class="st">"valid"</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>                <span class="at">activation =</span> <span class="st">"relu"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_max_pooling_2d</span>(<span class="at">pool_size =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_flatten</span>() <span class="sc">%&gt;%</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">120</span>, <span class="at">activation =</span> <span class="st">"relu"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">84</span>, <span class="at">activation =</span> <span class="st">"relu"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">"softmax"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>