---
title: "Decision Trees Lab 1"
authors:
- Adapted by EVL, FRC and ASP
date: "`r Sys.Date()`"
format:
    html: 
      toc: true
      toc-depth: 3
      code-fold: show
      fig-width: 8
      fig-height: 6
    pdf: default
knit:
  quarto:
    chunk_options:
      echo: true
      cache: false
      prompt: false
      tidy: true
      comment: NA
      message: false
      warning: false
    knit_options:
      width: 75
reference-location: margin
execute:
    echo: true
    message: false
    warning: false
bibliography: "../../StatisticalLearning.bib"
editor_options: 
  chunk_output_type: console
---

```{r packages, include=FALSE}
# If the package is not installed then it will be installed

if(!require("ISLR")) install.packages("ISLR")
if(!require("rsample")) install.packages("rsample")
if(!require("rpart.plot")) install.packages("rpart.plot")
if(!require("skimr")) install.packages("skimr")
if(!require("kableExtra")) install.packages("kableExtra")

```

# Predicting car sales with *classification* trees

## Introduction

This example has been adapted from the book "Introduction to Statistical Learning with R", [lab 8.3](https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch8-baggboost-lab.Rmd). 

The authors have decided to use the R `tree` package, which is not the most powerful R package for trees, but offers a good compromise between power and flexibility.

The lab relies on the `Carseats` dataset, a simulated dataset, that is included with the book's package,  containing several variables about sales of child car seats at different stores.

```{r}
require(ISLR2)
data("Carseats")
help("Carseats")
```

A data frame with 400 observations on the following 11 variables.

- `Sales`: Unit sales (in thousands) at each location
- `CompPrice`: Price charged by competitor at each location
- `Income`: Community income level (in thousands of dollars)
- `Advertising`: Local advertising budget for company at each location (in thousands of dollars)
- `Population`: Population size in region (in thousands)
- `Price`: Price company charges for car seats at each site
- `ShelveLoc`: A factor with levels Bad, Good and Medium indicating the quality of the shelving location for the car seats at each site
- `Age`: Average age of the local population
- `Education`: Education level at each location
- `Urban`: A factor with levels No and Yes to indicate whether the store is in an urban or rural location
- `US`: A factor with levels No and Yes to indicate whether the store is in the US or not

The first part of the lab will aim at predicting the variable `sales`. 

In order to apply classification trees first, we start by categorizing the `sales`variable. *This is not usually seen as a good strategy, so take it only for didactical purpose*.


## Data description

We use a generic name for the dataset, in order to facilitate code reuse.

```{r assignments}
myDescription <- "The data are a simulated data set containing sales of child car seats at different stores [@james2013introduction]"
mydataset <- Carseats
```

```{r, dataDescription}
n <- nrow(mydataset)
p <- ncol(mydataset)
```

There are ```r n``` rows and  ```r p``` columns.

The variable `Sales` is categorized  creating a new variable, `High`, which takes on a value of `Yes` if the `Sales` variable exceeds 8, and a value of `No` otherwise.

```{r}
# as.factor() changes the type of variable to factor
mydataset$High=as.factor(ifelse(mydataset$Sales<=8,"No","Yes"))
```

The number of observations for each class is:

```{r}
kable(table(mydataset$High), caption= "Number of observations for each class", col.names = c('High','Freq'))
```

The aim is of this study is to predict the categorical values of sales (`High`) using all variables but `Sales`.

It is a classification problem and we will build a *classification tree model*.

### Data summarization

This is a short data set summary

```{r}
summary(mydataset)
```

An improved description:

```{r}
skimr::skim(mydataset)
```

## Preprocess

It is very common that the data need to be preprocessed before training the model*

In this case, there seem to be no missing values, no outliers and most variables are decently symmetrical, so no cleaning or preprocessing are required.

## Train/Test partition of data

In order to properly evaluate the performance of a model, we must estimate the error rather than simply computing the training error.

With thhis aim in mind we proceed as follows: 

1.  split the observations into a training set and a test set,
2.  build the model using the training set, and
3.  evaluate its performance on the test data.

```{r, dataPartition}
set.seed(2)
pt <- 1/2
train <- sample(1:nrow(mydataset),pt*nrow(mydataset))
mydataset.test <- mydataset[-train,]
High.test <-  mydataset[-train,"High"]
```

The train and tets set have `r length(train)` `r nrow(mydataset) - length(train)` observations respectively.

In train data, the number of observations for each class is:

```{r showPartition}
kableExtra::kable(table(mydataset[train,"High"]), caption= "Train data: number of observations for each class", col.names = c('High','Freq'))
```

## Train model

We now use the `tree()` function to fit a classification tree in order to predict `High` using all variables but `Sales` using only de train set.

```{r modelTreeTrain}
library(tree)
tree.mydataset=tree(High~.-Sales, mydataset,
                    subset=train, 
                    split="deviance")
```

<!-- We might have also used `rpart` to build the tree -->

<!-- ```{r} -->
<!-- tree.mydataset2=rpart::rpart(High~.-Sales, mydataset, subset=train) -->
<!-- ``` -->

The `summary()` function lists the variables that are used as internal nodes in the tree, the number of terminal nodes, and the **training** error rate

```{r summarizeTreeTrain}
summary(tree.mydataset)
# summary(tree.mydataset2)
```

For classification trees the deviance of a tree (roughly equivalent to the concept of impurity) is defined  as the sum over all terminal leaves of:
$$
-2 \sum_m \sum_k n_{mk} log(\hat{p}_{mk}),
$$

where $n_{mk}$ is the number of observations in the `m`th terminal node that belong to the `k`th class. 

The *residual mean deviance* reported is simply the deviance divided by $n - |T_0|$ where $T_0$ is the number of terminal nodes.

## Plot the Tree

The next step is display the tree graphically. We use the `plot()` function to display the tree structure, and the `text()`function to display the node labels.

```{r plotTree1, fig.cap="Classification tree", fig.height=10, fig.width=12}
plot(tree.mydataset)
text(tree.mydataset,pretty=0, cex=0.6)
```


It is also possible to show a `R` print output corresponding to each branch of the tree.

```{r}
tree.mydataset
```

## Prediction

In order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error. 

We have split the observations into a training set and a test set, and the tree has been built using the training set.

After this, the tree performance  *is evaluated on the test data*. 
The `predict()` function can be used for this purpose.

```{r TreePerformance}
tree.pred=predict(tree.mydataset,mydataset.test,type="class")
res <- table(tree.pred,High.test)
res
accrcy <- sum(diag(res)/sum(res))
```

The accuracy is **`r accrcy`** or misclassification error rate is **`r 1-accrcy`**, which are respectively smaller and biiger than those computed from the tree built on the train data.

## Pruning the tree (Tunning model)

We know there is a chance that fitting the tree produces some overfitting so we can consider whether *pruning the tree* could lead to improved results.

The function `cv.tree()` performs cross-validation in order to determine the optimal level of tree complexity. 
- Cost complexity pruning is used in order to select a sequence of trees for consideration. 
- We use the argument `FUN = prune.misclass` in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the `cv.tree()` function, which is deviance. 

The `cv.tree()` function reports the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used 

```{r prepareForPrunning}
set.seed(123987)
cv.mydataset=cv.tree(tree.mydataset,FUN=prune.misclass)
names(cv.mydataset)
cv.mydataset
```

Note that, despite the name, `dev` corresponds to the cross-validation error rate in this instance.

The output shows how, as the size of the tree increases, so does the deviance. 

This can be better visualized by plotting the error rate as a function of  `size`and `k`.

```{r errorRatePlot}
par(mfrow=c(1,2))
plot(cv.mydataset$size,cv.mydataset$dev,type="b")
plot(cv.mydataset$k,cv.mydataset$dev,type="b")
par(mfrow=c(1,1))
```

These plots can be used to suggest the best tree, but it can also be chosen automatically by taking the minimal value $k$ from the output of the `cv.tree` function.

```{r bestSize}
myBest <- cv.mydataset$size[which.min(cv.mydataset$dev)]
```

Now, the `prune.misclass()` function can be used to prune the tree and obtain a "best tree". If *we decide to call the best tree the one that has reached the smallest deviance* we can proceed as follows:

```{r pruneTheTree}
prune.mydataset=prune.misclass(tree.mydataset,best=myBest)
```


```{r plotPrunedTree, fig.cap="The best classification pruned tree", fig.height=10, fig.width=12}
plot(prune.mydataset)
text(prune.mydataset,pretty=0)
```

The tree is clearly smaller than the original one, but how well does this pruned tree perform on the test data set?

```{r testPrunedTree}
prunedTree.pred=predict(prune.mydataset,mydataset.test,type="class")
prunedRes <- table(prunedTree.pred,High.test)
prunedRes
prunedAccrcy <- sum(diag(prunedRes)/sum(prunedRes))
```

The accuracy is **`r prunedAccrcy`**.

If we increase the value of `best`, for example `r  cv.mydataset$size[1]` terminal nodes, we obtain a larger pruned tree with lower classification accuracy:

```{r prunedTree2, fig.cap="Other classification pruned tree", fig.height=10, fig.width=12}
prune.mydataset=prune.misclass(tree.mydataset, 
                               best = cv.mydataset$size[1])
plot(prune.mydataset)
text(prune.mydataset, pretty=0)
```

```{r predictPrunedTree2}
ptree.pred=predict(prune.mydataset, mydataset.test, type="class")
pres <- table(ptree.pred, High.test)
pres
paccrcy <- sum(diag(pres)/sum(pres))
```

The accuracy is **`r paccrcy`**.

**In conclusion** It can be seen that the difference in accuracy between the pruned tree and the original one is small. Indeed, changing the seed for splitting can lead to both smaller or bigger accuracy in the pruned tree than in the original one.

Obviously, the pruned tree is smaller so even if the original tree is slightly more accurate than the pruned one we might prefer the second one, because it is relying on less data to produce almost the same accuracy, whic is *something that most users usually prefer*.

# Predicting car sales with *regression* trees

A reasonable question is how would the accuracy of the trees be affected if, instead of categorizing sales we had used it "as.is", building a regression tree instead.

The answer can be found directly by computing and optimizing a regression tree.

If we wish to copare the perfomance of both approaches we need a common measure of accuracy. For regression trees the Mean Square Error is generally used, but we can introduce a relative error measure that makes it more comparable (although this has to be discussed further).

```{r carSalesRegTree}

# Split the data into training and test sets
set.seed(2)
pt <- 1/2
train <- sample(1:nrow(mydataset), pt * nrow(mydataset))
mydataset.test <- mydataset[-train,]
sales.test <- mydataset$Sales[-train]

# Fit the regression tree using the Sales variable
tree.mydataset <- tree(Sales ~ . - High, mydataset,
                       subset = train)

# Summary of the fitted regression tree
summary(tree.mydataset)

# Plot the regression tree
plot(tree.mydataset)
text(tree.mydataset, pretty = 0, cex = 0.6)

# Predict using the test data
tree.pred <- predict(tree.mydataset, mydataset.test)

# Calculate accuracy (not applicable for regression)
# Evaluate the model using metrics such as mean squared error or R-squared
# For regression, you may use mean squared error
mse <- mean((tree.pred - sales.test)^2)
mse

# Prune the regression tree
set.seed(123987)
cv.mydataset <- cv.tree(tree.mydataset, FUN = prune.tree)
names(cv.mydataset)
cv.mydataset

# Plot the cross-validation error
par(mfrow = c(1, 2))
plot(cv.mydataset$size, cv.mydataset$dev, type = "b")
plot(cv.mydataset$k, cv.mydataset$dev, type = "b")
par(mfrow = c(1, 1))

# Choose the best tree size
myBest <- cv.mydataset$size[which.min(cv.mydataset$dev)]

# Prune the tree with the best size
prune.mydataset <- prune.tree(tree.mydataset, 
                              best = myBest)

# Plot the pruned regression tree
plot(prune.mydataset)
text(prune.mydataset, pretty = 0)

# Predict using the pruned tree
prunedTree.pred <- predict(prune.mydataset, mydataset.test)

# Calculate mean squared error for pruned tree
prunedMSE <- mean((prunedTree.pred - sales.test)^2)
prunedMSE
```

In this case, pruning does not improve the tree and the best tree is the one returned by the initial tun of the algorithm.

If however, we look for a compromise between the tree size and the deviance we can choose, based on the cv plots, a size of 6 or even 3:

```{r pruneto5}
# Prune the tree with the best size
pruneto5.mydataset <- prune.tree(tree.mydataset, 
                              best = 6)

# Plot the pruned regression tree
plot(pruneto5.mydataset)
text(pruneto5.mydataset, pretty = 0)

# Predict using the pruned tree
prunedTree5.pred <- predict(pruneto5.mydataset, mydataset.test)

# Calculate mean squared error for pruned tree
prunedMSE5 <- mean((prunedTree5.pred - sales.test)^2)
prunedMSE5
```

```{r pruneto3}
# Prune the tree with the best size
pruneto3.mydataset <- prune.tree(tree.mydataset, 
                              best = 3)

# Plot the pruned regression tree
plot(pruneto3.mydataset)
text(pruneto3.mydataset, pretty = 0)

# Predict using the pruned tree
prunedTree3.pred <- predict(pruneto3.mydataset, mydataset.test)

# Calculate mean squared error for pruned tree
prunedMSE3 <- mean((prunedTree3.pred - sales.test)^2)
prunedMSE3
```

Clearly, the best compromise seems to prune with a size of 5, which hardly increases the MSE while providinga good simplification of the tree

# Predicting House prices pith regression trees

This example is borrowed from [@amat2017].

The Boston dataset available in the MASS package contains housing prices for the city of Boston, as well as socioeconomic information for the neighborhood in which they are located.

```{r}
library(MASS)
data("Boston")
datos <- Boston
head(datos, 3)
```

Our goal is to fit a regression model that allows predicting the average price of a home (medv) based on the available variables.

```{r, fig.align='center'}
color <- adjustcolor("forestgreen", alpha.f = 0.5)
ps <- function(x, y, ...) {  # custom panel function
  panel.smooth(x, y, col = color, col.smooth = "black", 
               cex = 0.7, lwd = 2)
}
pairs(datos[,c(1:6,14)], cex = 0.7, upper.panel = ps, col = color)
pairs(datos[,c(7:14)], cex = 0.7, upper.panel = ps, col = color)

```

## Model fitting

Create a train and test sets

```{r}
set.seed(123)
train <- sample(1:nrow(datos), size = nrow(datos)/2)
datos_train <- datos[train,]
datos_test  <- datos[-train,]
```

We use the `tree` function of the `tree` package to build the model. This function grows the tree until it meets a stop condition. By default, these conditions are:

-   `mincut`: minimum number of observations that at least one of the child nodes must have for the division to occur.
-   `minsize`: minimum number of observations a node must have in order for it to be split.

```{r}
set.seed(123)
arbol_regresion <- tree::tree(
                    formula = medv ~ .,
                    data    = datos_train,
                    split   = "deviance",
                    mincut  = 20,
                    minsize = 50
                  )
summary(arbol_regresion)
```

The `summary` shows that the trained tree has a total of 6 terminal nodes and that the variables `rm, lstat, dis` and `tax` have been used as predictors.

In the context of regression trees, the `Residual mean deviance` term is the residual sum of squares divided by (number of observations - number of terminal nodes). The smaller the deviation, the better the fit of the tree to the training observations.

The tree can be visualized:

```{r}
par(mar = c(1,1,1,1))
plot(x = arbol_regresion, type = "proportional")
text(x = arbol_regresion, splits = TRUE, pretty = 0, cex = 0.8, col = "firebrick")
```

## Prunning the tree

We use the `cv.tree` function that uses cross validation to identify the optimal penalty value. By default, this function relies on the *deviance* to guide the pruning process.

We **grow the tree again** so we have a big tree to prune:

```{r}
arbol_regresion <- tree::tree(
                    formula = medv ~ .,
                    data    = datos_train,
                    split   = "deviance",
                    mincut  = 1,
                    minsize = 2,
                    mindev  = 0
                  )

# Optimization
set.seed(123)
cv_arbol <- tree::cv.tree(arbol_regresion, K = 5)
```

The function returns an object `cv_arbol` containing:

-   `size`: The size (number of terminal nodes) of each tree.
-   `dev`: The cross-validation test error estimate for each tree size.
-   `k`: The range of penalty values $\alpha$ evaluated.
-   `method`: The criteria used to select the best tree.

These can be used to visualize and understand the optimization performed.

```{r}
size_optimo <- rev(cv_arbol$size)[which.min(rev(cv_arbol$dev))]
paste("Optimal size obtained is:", size_optimo)
```

```{r}
library(ggplot2)
library(ggpubr)


resultados_cv <- data.frame(
                   n_nodes  = cv_arbol$size,
                   deviance = cv_arbol$dev,
                   alpha    = cv_arbol$k
                 )

p1 <- ggplot(data = resultados_cv, aes(x = n_nodes, y = deviance)) +
      geom_line() + 
      geom_point() +
      geom_vline(xintercept = size_optimo, color = "red") +
      labs(title = "Error vs tree size") +
      theme_bw() 
  
p2 <- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +
      geom_line() + 
      geom_point() +
      labs(title = "Error vs penalization (alpha)") +
      theme_bw() 

ggarrange(p1, p2)
```

Once the optimal value identified, the final pruning is applied with the `prune.tree` function. This function also accepts the optimal value of $\alpha$ instead of size.

```{r}
arbol_final <- tree::prune.tree(
                  tree = arbol_regresion,
                  best = size_optimo
               )

par(mar = c(1,1,1,1))
plot(x = arbol_final, type = "proportional")
text(x = arbol_final, splits = TRUE, pretty = 0, cex = 0.8, col = "firebrick")
```

## Predicting with the model

We can use both, original and pruned trees to predict the data for the test set.

The quality of the prediction is based in the Root Mean Square.

For the original tree one has:

```{r}
predicciones <- predict(arbol_regresion, newdata = datos_test)
test_rmse    <- sqrt(mean((predicciones - datos_test$medv)^2))
paste("Error de test (rmse) del árbol inicial:", round(test_rmse,2))
```

And for the final tree:

```{r}
predicciones_finales <- predict(arbol_final, newdata = datos_test)
test_rmse    <- sqrt(mean((predicciones - datos_test$medv)^2))
paste("Error de test (rmse) del árbol final:", round(test_rmse,2))
```

# Alternative packages for CART

## Comparison between `caret` and `rpart`

Two popular frames for working with trees are the `cart`and the `rpart` packages. The former is focused on decision trees, while the firs is a generic framework to perform any type of classification and prediction tasks, including trees.

The tables below shows a comparison between the main functions for working with trees with `rpart` and `caret`.

| **Task**                            | **`caret`** Package                                                  | **`rpart`** Package                                |
|:------------------------------------|:---------------------------------------------------------------------|:---------------------------------------------------|
| **Building a Decision Tree**        | **`train()`** function with method = "rpart"                         | **`rpart()`** function                             |
| **Plotting a Decision Tree**        | **`plot()`** function with type = "text"                             | **`plot()`** function                              |
| **Pruning a Decision Tree**         | **`train()`** function with method = "rpart" and tuneLength \> 1     | **`prune()`** function                             |
| **Evaluating Model Performance**    | **`train()`** function with method = "rpart" and metric = "Accuracy" | **`predict()`** function                           |
| **Handling Missing Values**         | **`preProcess()`** function with method = "medianImpute"             | **`na.action`** argument in **`rpart()`** function |
| **Tuning Hyperparameters**          | **`train()`** function with method = "rpart" and tuneGrid argument   | **`rpart.control()`** function                     |
| **Visualizing Variable Importance** | **`varImp()`** function                                              | **`importance()`** function                        |

here's an extended comparison table that includes examples of each task using **`caret`** and **`rpart`**:

| **Task**                            | **`caret`** Package                                                                                       | **`rpart`** Package                                                                                 |
|:------------------------------------|:----------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------|
| **Building a Decision Tree**        | **`train(Species ~ ., method = "rpart", data = iris)`**                                                   | **`rpart(Species ~ ., data = iris)`**                                                               |
| **Plotting a Decision Tree**        | **`plot(fit, type = "text")`**\<br\> **`fit <- rpart(Species ~ ., data = iris)`**                         | **`plot(fit)`**\<br\> **`fit <- rpart(Species ~ ., data = iris)`**                                  |
| **Pruning a Decision Tree**         | **`train(Species ~ ., method = "rpart", data = iris, tuneLength = 5)`**                                   | **`fit <- rpart(Species ~ ., data = iris)`**\<br\> **`prune(fit, cp = 0.02)`**                      |
| **Evaluating Model Performance**    | **`train(Species ~ ., method = "rpart", data = iris, metric = "Accuracy")`**                              | **`fit <- rpart(Species ~ ., data = iris)`**\<br\> **`pred <- predict(fit, iris, type = "class")`** |
| **Handling Missing Values**         | **`preProcess(iris, method = "medianImpute")`**                                                           | **`rpart(Species ~ ., data = na.omit(iris), na.action = na.rpart)`**                                |
| **Tuning Hyperparameters**          | **`train(Species ~ ., method = "rpart", data = iris, tuneGrid = expand.grid(cp = c(0.001, 0.01, 0.1)))`** | **`rpart(Species ~ ., data = iris, control = rpart.control(cp = c(0.001, 0.01, 0.1)))`**            |
| **Visualizing Variable Importance** | **`varImp(fit)`**\<br\> **`fit <- rpart(Species ~ ., data = iris)`**                                      | **`importance(fit)`**\<br\> **`fit <- rpart(Species ~ ., data = iris)`**                            |

In the examples above, we tried to make the parameters as comparable as possible but, given that there are many other parameters, it is unlikely that identical results are obtained in most cases.


# A case for improvement

This example relies on the `AmesHousing` dataset on house prices in Ames, IA.

```{r}
if(!require(AmesHousing))
  install.packages("AmesHousing", dep=TRUE)
ames <- AmesHousing::make_ames()
```

```{r}
if(!require(rsample))
  install.packages("rsample", dep=TRUE)
# Stratified sampling with the rsample package
set.seed(123)
split <- rsample::initial_split(ames, prop = 0.7, 
                       strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```

We use the `rpart`package to fit a regression tree to predict the `Sales_price` variable.

```{r}
require(rpart)
ames_dt1 <- rpart(
  formula = Sale_Price ~ .,
  data    = ames_train
  # method  = "anova"
)
```

We can visualize the tree with `rpart.plot()`

```{r}
require(rpart.plot)
rpart.plot(ames_dt1, cex=0.5)
```

`rpart()` automatically applies a range of cost complexity ($\alpha$ values to prune the tree).

To compare the error for each $\alpha$ value, `rpart()` performs a 10-fold CV (by default).

```{r}
printcp(ames_dt1)
```

```{r}
plotcp(ames_dt1)
```

A good choice of `cp` for pruning is often the leftmost value for which the mean lies below the horizontal line

## Feature importance

The `vip` function from the `vip` package allows computing variable importance from a built predictor object.

```{r}
if(!require(vip))
  install.packages("vip", dep=TRUE)
require(vip)
## ? vip
vip(ames_dt1, num_features = 40, bar = FALSE)
```

If we use `caret`instead of `rpart` the total reduction in the loss function across all splits is standardized so that the most important feature has a value of 100 and the remaining features are scored based on their relative importance.


# Exercises

This problem involves the $O J$ data set which is part of the ISLR2 package.

(a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.
(b) Fit a tree to the training data, with Purchase as the response and the other variables as predictors. Use the summary () function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?
(c) Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.
(d) Create a plot of the tree, and interpret the results.
(e) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?
(f) Apply the cv.tree() function to the training set in order to determine the optimal tree size.
(g) Produce a plot with tree size on the $x$-axis and cross-validated classification error rate on the $y$-axis.
(h) Which tree size corresponds to the lowest cross-validated classification error rate?
(i) Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.
(j) Compare the training error rates between the pruned and unpruned trees. Which is higher?
(k) Compare the test error rates between the pruned and unpruned trees. Which is higher?

Once you have solved the exercise, try to repeat it using another R package, either `rpàrt` or `caret`. Compare the results obtained and comment about the differences observed.

# References
