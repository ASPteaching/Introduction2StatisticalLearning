---
title: "Decision Trees Lab 2: Ensembles"
authors:
- Adapted by EVL, FRC and ASP
date: "`r Sys.Date()`"
format:
    html: 
      toc: true
      toc-depth: 3
      code-fold: false
      fig-width: 8
      fig-height: 6
    pdf: default
knit:
  quarto:
    chunk_options:
      echo: true
      cache: false
      prompt: false
      tidy: true
      comment: NA
      message: false
      warning: false
    knit_options:
      width: 75
reference-location: margin
execute:
    echo: true
    message: false
    warning: false
    cache: true
bibliography: "StatisticalLearning.bib"
editor_options: 
  chunk_output_type: console
---

```{r packages, include=FALSE}
# If the package is not installed then it will be installed

if(!require("knitr")) install.packages("knitr")
if(!require("tree")) install.packages("tree")
if(!require("ISLR")) install.packages("ISLR")
if(!require("rpart.plot")) install.packages("rpart.plot")
```

```{r message=FALSE}
# Helper packages
library(dplyr)       # for data wrangling
library(ggplot2)     # for awesome plotting
library(doParallel)  # for parallel backend to foreach
library(foreach)     # for parallel processing with for loops

# Modeling packages
library(caret)       # for general model fitting
library(rpart)       # for fitting decision trees
library(ipred)       # for fitting bagged decision trees

```

# A case for improvement

This example relies on the `AmesHousing` dataset on house prices in Ames, IA.
We start building a regression tree. The fitted tree cannot be improved by pruning, so it will opt to try ensemblemethods on it.

```{r}
if(!require(AmesHousing))
  install.packages("AmesHousing", dep=TRUE)
ames <- AmesHousing::make_ames()
```

We split the data in separate test / training sets and do it in such a way that samplig is balanced for the response variable, `Sale_Price`.

```{r}
if(!require(rsample))
  install.packages("rsample", dep=TRUE)
# Stratified sampling with the rsample package
set.seed(123)
split <- rsample::initial_split(ames, prop = 0.7, 
                       strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```

We use the `tree` package to fit a regression tree to predict the `Sales_price` variable.

```{r}
require(rpart)
 ames_dt1 <- rpart(
   formula = Sale_Price ~ .,
   data    = ames_train
   # method  = "anova"
 )
```

We can visualize the tree with `rpart.plot()`

```{r}
require(rpart.plot)
rpart.plot(ames_dt1, cex=0.5)
```

`rpart()` automatically applies a range of cost complexity ($\alpha$ values to prune the tree).

To compare the error for each $\alpha$ value, `rpart()` performs a 10-fold CV (by default).

```{r}
printcp(ames_dt1)
```

```{r}
plotcp(ames_dt1)
```

A good choice of `cp` for pruning is often the leftmost value for which the mean lies below the horizontal line

## Feature importance

The `vip` function from the `vip` package allows computing variable importance from a built predictor object.

```{r}
if(!require(vip))
  install.packages("vip", dep=TRUE)
require(vip)
## ? vip
vip(ames_dt1, num_features = 40, bar = FALSE)
```


# Bagging trees

The first example is adapted from [@Boehmke2020], also [available online](https://bradleyboehmke.github.io/HOML/).

This example relies on the `AmesHousing` dataset on house prices in Ames, IA.

```{r}
if(!require(AmesHousing))
  install.packages("AmesHousing", dep=TRUE)
ames <- AmesHousing::make_ames()
write.csv(ames, "amesData.csv")
ames2 <- read.csv("amesData.csv", row.names = 1)
```

```{r}
set.seed(123)
train <- sample(1:nrow(ames), nrow(ames)/2)
# split <- rsample::initial_split(ames, prop = 0.7, 
#                        strata = "Sale_Price")
ames_train  <- ames[train,]
ames_test   <- ames[-train,]
```

Building a decision trees to predict the sales price for the Ames housing data yields a poor performance classifier/predictor that is beaten by alternatives such as MARS or KNN (check it!)

In this example, rather than use a single pruned decision tree, we can use, say, 100 bagged unpruned trees (by not pruning the trees we're keeping bias low and variance high which is when bagging will have the biggest effect).

Bagging is equivalent to RandomForest if we use all the trees so the library `randomForest` is used.


```{r}
# make bootstrapping reproducible
set.seed(123)

library(randomForest)
bag.Ames <- randomForest(Sale_Price ~ ., 
                         data = ames_train, 
                         mtry = ncol(ames_train-1), 
                         ntree = 100,
                         importance = TRUE)

```

Bagging, as most ensemble procedures, can be time consuming.
See [@Boehmke2020](https://bradleyboehmke.github.io/HOML/bagging.html#easily-parallelize) for an example on how to easily parallelize code, and save time.

```{r}
show(bag.Ames)
```

Bagging tends to improve quickly as the number of resampled trees increases, and then it reaches a platform.

The figure below has been produced iterated the computation above over `nbagg` values of 1â€“200 and applied the `bagging()` function.

```{r, echo=FALSE, fig.align='center', out.width="100%", fig.cap="Error curve for bagging 1-200 deep, unpruned decision trees. The benefit of bagging is optimized at 187 trees although the majority of error reduction occurred within the first 100 trees"}
knitr::include_graphics("images/baggingRSME.png")
```

## Variable importance

Due to the bagging process, models that are normally perceived as interpretable are no longer so. 

However, we can still make inferences about how features are influencing our model using *feature importance* measures based on the sum of the reduction in the loss function (e.g., SSE) attributed to each variable at each split in a given tree.

```{r}
require(dplyr)
VIP <- importance(bag.Ames) 
VIP <- VIP[order(VIP[,1], decreasing = TRUE),]
head(VIP, n=30)
```


```{r}
invVIP <-VIP[order(VIP[,1], decreasing = FALSE),1] 
tVIP<- tail(invVIP, n=15)
barplot(tVIP, 
        names.arg=row.names(tVIP), cex.names=0.5,las=2,
        horiz = TRUE)
```


Alternatively if the tree is built with `caret` the `vip` function from package `vip` can be used.

```{r eval=FALSE}
  system.time(
  ames_bag2 <- train(
    Sale_Price ~ .,
    data = ames_train,
    method = "treebag",
    trControl = trainControl(method = "oob"),
    nbagg = 100,  
    keepX=TRUE,
    control = rpart.control(minsplit = 2, cp = 0)
    )
  )

vip::vip(ames_bag2, num_features = 40)
```


```{r, out.width="100%"}
knitr::include_graphics("images/ames2VIP.png")
```

## Random forests for gene expression data

Random forest have been particularly successful in Bioinformatics where high dimensional data are common.

One common application has been the use of RF to derive cancer-related classifiers based on gene expression data.

Gene expression data are high dimensional tabular datasets where for each inividual the expression of a high number of genes has been measured

The example uses "RMA-preprocessed gene expression data" obtained by [@Chiaretti2004]. Briefly they consist of:

- 12625 genes (hgu95av2 Affymetrix GeneChip)
- 128 samples (arrays)
- phenotypic data on all 128 patients, including:
- 95 B-cell cancer
- 33 T-cell cancer

A standard bioinformatic preprocessing has been applied.

The code below is shown for consistency, but unless you are interested/familiar with Bioconductor and microarray data storage and preprocessing it can be skipped.

This code generates a simplified dataset that is saved into a binary file `data/smallALL.Rda'. This file can be directly loaded for the example.

```{r eval=FALSE}
if(!require(affy)) BiocManager::install("affy")
if(!require(genefilter)) BiocManager::install("genefilter")
if(!require(ALL)) BiocManager::install("ALL")
library(affy)
library(ALL)
data(ALL)
```

Preprocessing is applied to obtain relevant subset of data
Also, keep 30 arrays here JUST for computational convenience #  

```{r eval=FALSE}
library(genefilter); 
e.mat <- 2^(exprs(ALL)[,c(81:110)]) 
ffun <- filterfun(pOverA(0.20,100)) 
t.fil <- genefilter(e.mat,ffun) 
smallData <- log2(e.mat[t.fil,]) 
group <- c(rep('B',15),rep('T',15)) 
dim(smallData) 
colnames(smallData)
infoData <- cbind(pData(ALL)[81:110,1:5], group) # column "BT" defines groups
save (smallData, infoData, file="data/smallALL.Rda")
```

```{r loadOmics}
load (file = "data/smallALL.Rda")
```

We use the `randomForest` library to build an "out-of-the box" classifier.

```{r}
if (!require(randomForest)) install.packages("randomForest", dep=TRUE)
library(randomForest) 
set.seed(1234) 
system.time(
rf <- randomForest(x=t(smallData),
                   y=as.factor(infoData$group),
                   ntree=10000) 
)
```

Inspect the results

```{r}
rf
```

Now look at variable importance:

```{r}
imp.temp <- abs(rf$importance[,]) 
t <- order(imp.temp,decreasing=TRUE)
plot(c(1:nrow(smallData)),imp.temp[t],log='x',cex.main=1.5,    xlab='gene rank',ylab='variable importance',cex.lab=1.5,    pch=16,main='ALL subset results')  
```

Or, a better plot:

```{r}
varImpPlot(rf, n.var=25, main='ALL Subset Results') 
```


We can focus on the 25 most important genes

```{r}
gn.imp <- names(imp.temp)[t] 
gn.25 <- gn.imp[1:25] 
# vector of top 25 genes, in order
```

We can use the Bioinformatics Bioconductor libraries to find out more about these these genes. Information on how to do it can be found at [https://aspteaching.github.io/An-Introduction-to-Pathway-Analysis-with-R-and-Bioconductor/](https://aspteaching.github.io/An-Introduction-to-Pathway-Analysis-with-R-and-Bioconductor/).

Again, the code is shown but it has been run aside this lab and the result saved as a binary file.

```{r eval=FALSE}
if(!require(hgu95av2.db)) BiocManager::install("hgu95av2.db")
if(!require(AnnotationDbi)) BiocManager::install("AnnotationDbi")
library(hgu95av2.db)
geneAnots <- AnnotationDbi::select(hgu95av2.db, gn.25,
                      c("SYMBOL", "GENENAME"))
head(geneAnots, n=25)
```

```{r}
knitr::include_graphics("images/geneNames.png")
```


To end the exploration we plot  heatmap that shows how the two groups differ in gene expression.

```{r eval=FALSE}
sigGenes<- smallData[gn.25,]
t <- is.element(rownames(small.eset),gn.25) 
sig.eset <- small.eset[t,]    
# matrix of expression values, not necessarily in order  
library(RColorBrewer) 
hmcol <- colorRampPalette(brewer.pal(11,"PuOr"))(256) 
colnames(sig.eset) <- group 
# This will label the heatmap columns 
csc <- rep(hmcol[50],30) 
csc[group=='T'] <- hmcol[200]    
# column side color will be purple for T and orange for B 
heatmap(sigGenes,scale="row", col=hmcol,ColSideColors=csc) 
```

```{r}
knitr::include_graphics("images/allHeatMap.png")
```

# Random Forests with Python

The following link points to a good brief tutorial on how to train and evaluate Random Forests using Python

[DataCamp: Random Forest Classification with Scikit-Learn](https://www.datacamp.com/tutorial/random-forests-classifier-python)

# References
