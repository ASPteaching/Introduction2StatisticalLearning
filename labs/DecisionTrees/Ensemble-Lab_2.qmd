---
title: "Decision Trees Lab 2: Ensembles"
authors:
- Adapted by EVL, FRC and ASP
date: "`r Sys.Date()`"
format:
    html: 
      toc: true
      toc-depth: 3
      code-fold: false
      fig-width: 8
      fig-height: 6
    pdf: default
knit:
  quarto:
    chunk_options:
      echo: true
      cache: false
      prompt: false
      tidy: true
      comment: NA
      message: false
      warning: false
    knit_options:
      width: 75
reference-location: margin
execute:
    echo: true
    message: false
    warning: false
    cache: true
bibliography: "StatisticalLearning.bib"
editor_options: 
  chunk_output_type: console
---

```{r packages, include=FALSE}
# If the package is not installed then it will be installed

if(!require("knitr")) install.packages("knitr")
if(!require("tree")) install.packages("tree")
if(!require("ISLR")) install.packages("ISLR")
if(!require("rpart.plot")) install.packages("rpart.plot")
```

```{r message=FALSE}
# Helper packages
library(dplyr)       # for data wrangling
library(ggplot2)     # for awesome plotting
library(doParallel)  # for parallel backend to foreach
library(foreach)     # for parallel processing with for loops

# Modeling packages
library(caret)       # for general model fitting
library(rpart)       # for fitting decision trees
library(ipred)       # for fitting bagged decision trees

```

# Introduction

This lab presents some examples on building ensemble predictors with a variety of methods.

In order to facilitate the comparison between methods and tools the same prediction problem will be solved with distinct methods and distinct parameter settings.

The example problem is the prediction of house prices in the `AmesHousing` dataset, a dataset with multiple variables about cost of housing in Ames, IA.

## The dataset

Packge `AmesHousing` contains the data jointly with some instructions to create the required dataset.

We will use, however data from the `modeldata` package where some preprocessing of the data has already been performed (see: [https://www.tmwr.org/ames](https://www.tmwr.org/ames))

```{r}
if(!require(modeldata))
  install.packages("modeldata", dep=TRUE)
data(ames, package = "modeldata")
```

The dataset has 74 variables so a descriptive analysis is not provided.

```{r}
dim(ames)
boxplot(ames)
```

<!-- Although not strictly necessary, given that the variable that has the widest range of variation is the variable to predict we make a log transform for it. Eventually we can use either eof these to build and predict and compare how the models perform. -->

<!-- ```{r} -->
<!-- require(dplyr) -->
<!-- ames <- ames %>% mutate(logSale_Price = log10(Sale_Price)) -->
<!-- ``` -->


## Spliting the data into test/train

We split the data in separate test / training sets and do it in such a way that samplig is balanced for the response variable, `Sale_Price`.

```{r}
if(!require(rsample))
  install.packages("rsample", dep=TRUE)
# Stratified sampling with the rsample package
set.seed(123)
split <- rsample::initial_split(ames, prop = 0.7, 
                       strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```


# A simple regression tree

As a first attempt to predict Sales_Price we build a unique regression tree, which as has been discussed is a weak learner. The `tree` package will be used to fit and optimize the tree.

We build a tree using non-restrictive parameters. This will allow space for pruning it better.

We use the `tree.control` function to set the values for the `control` parameter in the `tree` function.

Let's start with a tree with the default values.

```{r}
require(tree)

ctlPars <-tree.control(nobs=nrow(ames_train), mincut = 5, minsize = 10, mindev = 0.01)

ames_rt1 <-  tree::tree(
                    formula = Sale_Price ~ .,
                    data    = ames_train,
                    split   = "deviance",
                    control = ctlPars)
summary(ames_rt1)
```

This gives a small tree with only 11 terminal nodes.

We can visualize the tree 

```{r plotAmes1}
plot(x = ames_rt1, type = "proportional")
text(x = ames_rt1, splits = TRUE, pretty = 0, cex = 0.6, col = "firebrick")
```

In order to optimize the tree we compute the best cost complexity value

```{r pruneAmes1}
set.seed(123)
cv_ames_rt1 <- tree::cv.tree(ames_rt1, K = 5)

optSize <- rev(cv_ames_rt1$size)[which.min(rev(cv_ames_rt1$dev))]
paste("Optimal size obtained is:", optSize)
```

The best value of alpha is obtained with the same tree, which suggests that there will be no advantage in pruning.
This is confirmed by plotting tree size vs deviance which shows that the tree with the samllest error is the biggest one that can be obtained.

```{r plotPruneAmes}
library(ggplot2)
library(ggpubr)

resultados_cv <- data.frame(
                   n_nodes  = cv_ames_rt1$size,
                   deviance = cv_ames_rt1$dev,
                   alpha    = cv_ames_rt1$k
                 )

p1 <- ggplot(data = resultados_cv, aes(x = n_nodes, y = deviance)) +
      geom_line() + 
      geom_point() +
      geom_vline(xintercept = optSize, color = "red") +
      labs(title = "Error vs tree size") +
      theme_bw() 
  
p2 <- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +
      geom_line() + 
      geom_point() +
      labs(title = "Error vs penalization (alpha)") +
      theme_bw() 

ggarrange(p1, p2)
```

We could have tried to obtain a bigger tree with the hope that pruning might find a better tree.  This can be done setting the tree parameters to minimal values.


```{r fitAmesRT2}
ctlPars2 <-tree.control(nobs=nrow(ames_train), mincut = 1, minsize = 2, mindev = 0)
ames_rt2 <-  tree::tree(
                    formula = Sale_Price ~ .,
                    data    = ames_train,
                    split   = "deviance",
                    control = ctlPars2)
summary(ames_rt2)
```

This bigger tree has indeed a smaller deviance but pruning provides no benefit:

```{r pruneAmes2}
set.seed(123)
cv_ames_rt2 <- tree::cv.tree(ames_rt2, K = 5)

optSize2 <- rev(cv_ames_rt2$size)[which.min(rev(cv_ames_rt2$dev))]
paste("Optimal size obtained is:", optSize2)
```

```{r}
prunedTree2 <- tree::prune.tree(
                  tree = ames_rt2,
                  best = optSize2
               )
summary(prunedTree2)
```


```{r}
res_cv2 <- data.frame(
                   n_nodes  = cv_ames_rt2$size,
                   deviance = cv_ames_rt2$dev,
                   alpha    = cv_ames_rt2$k
                 )

p1 <- ggplot(data = res_cv2, aes(x = n_nodes, y = deviance)) +
      geom_line() + 
      geom_point() +
      geom_vline(xintercept = optSize2, color = "red") +
      labs(title = "Error vs tree size") +
      theme_bw() 
  
p2 <- ggplot(data = res_cv2, aes(x = alpha, y = deviance)) +
      geom_line() + 
      geom_point() +
      labs(title = "Error vs penalization (alpha)") +
      theme_bw() 

ggarrange(p1, p2)
```

The performance of the trees is hardly different between small or big tree in pruned or non-pruned version.

```{r ames_rt_pred1}
ames_rt_pred1 <- predict(ames_rt1, newdata = ames_test)
test_rmse1    <- sqrt(mean((ames_rt_pred1 - ames_test$Sale_Price)^2))
paste("Error test (rmse) for initial tree:", round(test_rmse1,2))
```


```{r ames_rt_pred2}
ames_rt_pred2 <- predict(ames_rt2, newdata = ames_test)
test_rmse2    <- sqrt(mean((ames_rt_pred2 - ames_test$Sale_Price)^2))
paste("Error test (rmse) for big tree:", round(test_rmse2,2))
```

```{r ames_pruned_pred}
ames_pruned_pred <- predict(prunedTree2, newdata = ames_test)
test_rmse3    <- sqrt(mean((ames_pruned_pred - ames_test$Sale_Price)^2))
paste("Error test (rmse) for pruned tree:", round(test_rmse3,2))
improvement <- (test_rmse3-test_rmse2)/test_rmse2*100
```

The MSE for each model will be saved to facilitate comparison with other models

```{r}
errTable <- data.frame(Model=character(),  RMSE=double())
errTable[1, ] <-  c("Default Regression Tree", round(test_rmse1,2))
errTable[2, ] <-  c("Big Regression Tree", round(test_rmse2,2))
errTable[3, ] <-  c("Optimally pruned Regression Tree", round(test_rmse3,2))
```

In summary, what is illustrated by this example is that, for some datasets, it is very hard to obtain an optimal tree because there seems to be a minimum complexity which is very hard to decrease. 

Building a saturated tree only provides a slight improvement of less than 5% in RMSE at the cost of having to use 5 times more variables in a tree withh more than 1000 nodes.

This is a good point to consider using an ensemble instead of single trees.


# Bagging trees

The first attempt to build an ensemble may be to apply `bagging` that is building multiple trees from a set of resamples and averaging the predictions of each tree.

In this example, rather than use a single pruned decision tree, we can use, say, 100 bagged unpruned trees (by not pruning the trees we're keeping bias low and variance high which is when bagging will have the biggest effect).

Bagging is equivalent to RandomForest if we use all the trees so the library `randomForest` is used.

```{r fitbaggedTrees1}
# make bootstrapping reproducible
set.seed(123)

library(randomForest)
bag.Ames <- randomForest(Sale_Price ~ ., 
                         data = ames_train, 
                         mtry = ncol(ames_train-1), 
                         # ntree = 100,
                         importance = TRUE)
```

Bagging, as most ensemble procedures, can be time consuming.
See [@Boehmke2020](https://bradleyboehmke.github.io/HOML/bagging.html#easily-parallelize) for an example on how to easily parallelize code, and save time.

```{r}
show(bag.Ames)
```

```{r predictBag}
yhat.bag <- predict(bag.Ames, newdata = ames_test)
plot(yhat.bag, ames_test$Sale_Price)
abline(0, 1)

test_rmse_bag  <- sqrt(mean(mean((yhat.bag - ames_test$Sale_Price)^2)))
paste("Error test (rmse) for bagged tree:", round(test_rmse_bag,2))
errTable[4, ] <-  c("Bagged Tree (ntree=100)", round(test_rmse_bag,2))
```

Bagging tends to improve quickly as the number of resampled trees increases, and then it reaches a platform.

The figure below has been produced iterated the computation above over `nbagg` values of 1â€“200 and applied the `bagging()` function.

```{r, echo=FALSE, fig.align='center', out.width="100%", fig.cap="Error curve for bagging 1-200 deep, unpruned decision trees. The benefit of bagging is optimized at 187 trees although the majority of error reduction occurred within the first 100 trees"}
knitr::include_graphics("images/baggingRSME.png")
```

## Variable importance

Due to the bagging process, models that are normally perceived as interpretable are no longer so. 

However, we can still make inferences about how features are influencing our model using *feature importance* measures based on the sum of the reduction in the loss function (e.g., SSE) attributed to each variable at each split in a given tree.

```{r}
require(dplyr)
VIP <- importance(bag.Ames) 
VIP <- VIP[order(VIP[,1], decreasing = TRUE),]
head(VIP, n=30)
```

Importance values can be plotted directly:

```{r}
invVIP <-VIP[order(VIP[,1], decreasing = FALSE),1] 
tVIP<- tail(invVIP, n=15)
barplot(tVIP, horiz = TRUE, cex.names=0.5)
```

Alternatively one can use the `vip`function from the `vip` package


```{r}
if (!require(vip)) install.packages("vip", dep=TRUE)
vip(bag.Ames, num_features = 40, bar = FALSE)
```

# A random forest to improve bagging

Bagging can be improved if, instead of using all variables to build each tree we rely on subsets of variables, which are chosen at each split in order to decrease correlation between trees.

Random Forests are considered to produce good predictors with default values sop no parameter is set in a first iteration.

```{r fitRF}
# make bootstrapping reproducible
set.seed(123)

require(randomForest)
RF.Ames <- randomForest(Sale_Price ~ ., 
                         data = ames_train, 
                         importance = TRUE)
```


```{r}
show(RF.Ames)
```

```{r predictRF}
yhat.rf <- predict(RF.Ames, newdata = ames_test)
plot(yhat.rf, ames_test$Sale_Price)
abline(0, 1)

test_rmse_rf  <- sqrt(mean(mean((yhat.rf - ames_test$Sale_Price)^2)))
paste("Error test (rmse) for Random Forest:", round(test_rmse_rf,2))
errTable[5, ] <-  c("Random Forest (defaults)", round(test_rmse_rf,2))
```

There is some improvement on bagging but it is clearly small.

Notice however that the percentage of explained variance is bigger for RF than for Bag


# Random forests for gene expression data

Random forest have been particularly successful in Bioinformatics where high dimensional data are common.

One common application has been the use of RF to derive cancer-related classifiers based on gene expression data.

Gene expression data are high dimensional tabular datasets where for each inividual the expression of a high number of genes has been measured

The example uses "RMA-preprocessed gene expression data" obtained by [@Chiaretti2004]. Briefly they consist of:

- 12625 genes (hgu95av2 Affymetrix GeneChip)
- 128 samples (arrays)
- phenotypic data on all 128 patients, including:
- 95 B-cell cancer
- 33 T-cell cancer

A standard bioinformatic preprocessing has been applied.

The code below is shown for consistency, but unless you are interested/familiar with Bioconductor and microarray data storage and preprocessing it can be skipped.

This code generates a simplified dataset that is saved into a binary file `data/smallALL.Rda'. This file can be directly loaded for the example.

```{r eval=FALSE}
if(!require(affy)) BiocManager::install("affy")
if(!require(genefilter)) BiocManager::install("genefilter")
if(!require(ALL)) BiocManager::install("ALL")
library(affy)
library(ALL)
data(ALL)
```

Preprocessing is applied to obtain relevant subset of data
Also, keep 30 arrays here JUST for computational convenience #  

```{r eval=FALSE}
library(genefilter); 
e.mat <- 2^(exprs(ALL)[,c(81:110)]) 
ffun <- filterfun(pOverA(0.20,100)) 
t.fil <- genefilter(e.mat,ffun) 
smallData <- log2(e.mat[t.fil,]) 
group <- c(rep('B',15),rep('T',15)) 
dim(smallData) 
colnames(smallData)
infoData <- cbind(pData(ALL)[81:110,1:5], group) # column "BT" defines groups
save (smallData, infoData, file="data/smallALL.Rda")
```

```{r loadOmics}
load (file = "data/smallALL.Rda")
```

We use the `randomForest` library to build an "out-of-the box" classifier.

```{r}
if (!require(randomForest)) install.packages("randomForest", dep=TRUE)
library(randomForest) 
set.seed(1234) 
system.time(
rf <- randomForest(x=t(smallData),
                   y=as.factor(infoData$group),
                   ntree=10000) 
)
```

Inspect the results

```{r}
rf
```

Now look at variable importance:

```{r}
imp.temp <- abs(rf$importance[,]) 
t <- order(imp.temp,decreasing=TRUE)
plot(c(1:nrow(smallData)),imp.temp[t],log='x',cex.main=1.5,    xlab='gene rank',ylab='variable importance',cex.lab=1.5,    pch=16,main='ALL subset results')  
```

Or, a better plot:

```{r}
varImpPlot(rf, n.var=25, main='ALL Subset Results') 
```


We can focus on the 25 most important genes

```{r}
gn.imp <- names(imp.temp)[t] 
gn.25 <- gn.imp[1:25] 
# vector of top 25 genes, in order
```

We can use the Bioinformatics Bioconductor libraries to find out more about these these genes. Information on how to do it can be found at [https://aspteaching.github.io/An-Introduction-to-Pathway-Analysis-with-R-and-Bioconductor/](https://aspteaching.github.io/An-Introduction-to-Pathway-Analysis-with-R-and-Bioconductor/).

Again, the code is shown but it has been run aside this lab and the result saved as a binary file.

```{r eval=FALSE}
if(!require(hgu95av2.db)) BiocManager::install("hgu95av2.db")
if(!require(AnnotationDbi)) BiocManager::install("AnnotationDbi")
library(hgu95av2.db)
geneAnots <- AnnotationDbi::select(hgu95av2.db, gn.25,
                      c("SYMBOL", "GENENAME"))
head(geneAnots, n=25)
```

```{r}
knitr::include_graphics("images/geneNames.png")
```


To end the exploration we plot  heatmap that shows how the two groups differ in gene expression.

```{r eval=FALSE}
sigGenes<- smallData[gn.25,]
t <- is.element(rownames(small.eset),gn.25) 
sig.eset <- small.eset[t,]    
# matrix of expression values, not necessarily in order  
library(RColorBrewer) 
hmcol <- colorRampPalette(brewer.pal(11,"PuOr"))(256) 
colnames(sig.eset) <- group 
# This will label the heatmap columns 
csc <- rep(hmcol[50],30) 
csc[group=='T'] <- hmcol[200]    
# column side color will be purple for T and orange for B 
heatmap(sigGenes,scale="row", col=hmcol,ColSideColors=csc) 
```

```{r}
knitr::include_graphics("images/allHeatMap.png")
```

# Random Forests with Python

The following link points to a good brief tutorial on how to train and evaluate Random Forests using Python

[DataCamp: Random Forest Classification with Scikit-Learn](https://www.datacamp.com/tutorial/random-forests-classifier-python)

# References
