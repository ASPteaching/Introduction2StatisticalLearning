---
title: "Decision Trees Lab 2: Ensembles"
authors:
- Adapted by EVL, FRC and ASP
date: "`r Sys.Date()`"
format:
    html: 
      toc: true
      toc-depth: 3
      code-fold: false
      fig-width: 8
      fig-height: 6
    pdf: default
knit:
  quarto:
    chunk_options:
      echo: true
      cache: false
      prompt: false
      tidy: true
      comment: NA
      message: false
      warning: false
    knit_options:
      width: 75
reference-location: margin
execute:
    echo: true
    message: false
    warning: false
    cache: true
bibliography: "StatisticalLearning.bib"
editor_options: 
  chunk_output_type: console
---

```{r packages, include=FALSE}
# If the package is not installed then it will be installed

if(!require("knitr")) install.packages("knitr")
if(!require("tree")) install.packages("tree")
if(!require("ISLR")) install.packages("ISLR")
if(!require("rpart.plot")) install.packages("rpart.plot")
```

```{r message=FALSE}
# Helper packages
library(dplyr)       # for data wrangling
library(ggplot2)     # for awesome plotting
library(modeldata)  # for parallel backend to foreach
library(foreach)     # for parallel processing with for loops

# Modeling packages
library(caret)       # for general model fitting
library(rpart)       # for fitting decision trees
library(ipred)       # for fitting bagged decision trees
```

# Introduction

This lab presents some examples on building ensemble predictors with a variety of methods.

In order to facilitate the comparison between methods and tools the same prediction problem will be solved with distinct methods and distinct parameter settings.

The example problem is the prediction of house prices in the `AmesHousing` dataset, a dataset with multiple variables about cost of housing in Ames, IA.

## The dataset

Packge `AmesHousing` contains the data jointly with some instructions to create the required dataset.

We will use, however data from the `modeldata` package where some preprocessing of the data has already been performed (see: [https://www.tmwr.org/ames](https://www.tmwr.org/ames))

```{r}
if(!require(modeldata))
  install.packages("modeldata", dep=TRUE)
data(ames, package = "modeldata")
```

The dataset has 74 variables so a descriptive analysis is not provided.

```{r}
dim(ames)
boxplot(ames)
```

<!-- Although not strictly necessary, given that the variable that has the widest range of variation is the variable to predict we make a log transform for it. Eventually we can use either eof these to build and predict and compare how the models perform. -->

<!-- ```{r} -->
<!-- require(dplyr) -->
<!-- ames <- ames %>% mutate(logSale_Price = log10(Sale_Price)) -->
<!-- ``` -->


## Spliting the data into test/train

We split the data in separate test / training sets and do it in such a way that samplig is balanced for the response variable, `Sale_Price`.

```{r}
if(!require(rsample))
  install.packages("rsample", dep=TRUE)
# Stratified sampling with the rsample package
set.seed(123)
split <- rsample::initial_split(ames, prop = 0.7, 
                       strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```


# A simple regression tree

As a first attempt to predict Sales_Price we build a unique regression tree, which as has been discussed is a weak learner. The `tree` package will be used to fit and optimize the tree.

We build a tree using non-restrictive parameters. This will allow space for pruning it better.

We use the `tree.control` function to set the values for the `control` parameter in the `tree` function.

Let's start with a tree with the default values.

```{r}
require(tree)

ctlPars <-tree.control(nobs=nrow(ames_train), mincut = 5, minsize = 10, mindev = 0.01)

ames_rt1 <-  tree::tree(
                    formula = Sale_Price ~ .,
                    data    = ames_train,
                    split   = "deviance",
                    control = ctlPars)
summary(ames_rt1)
```

This gives a small tree with only 11 terminal nodes.

We can visualize the tree 

```{r plotAmes1}
plot(x = ames_rt1, type = "proportional")
text(x = ames_rt1, splits = TRUE, pretty = 0, cex = 0.6, col = "firebrick")
```

In order to optimize the tree we compute the best cost complexity value

```{r pruneAmes1}
set.seed(123)
cv_ames_rt1 <- tree::cv.tree(ames_rt1, K = 5)

optSize <- rev(cv_ames_rt1$size)[which.min(rev(cv_ames_rt1$dev))]
paste("Optimal size obtained is:", optSize)
```

The best value of alpha is obtained with the same tree, which suggests that there will be no advantage in pruning.
This is confirmed by plotting tree size vs deviance which shows that the tree with the samllest error is the biggest one that can be obtained.

```{r plotPruneAmes}
library(ggplot2)
library(ggpubr)

resultados_cv <- data.frame(
                   n_nodes  = cv_ames_rt1$size,
                   deviance = cv_ames_rt1$dev,
                   alpha    = cv_ames_rt1$k
                 )

p1 <- ggplot(data = resultados_cv, aes(x = n_nodes, y = deviance)) +
      geom_line() + 
      geom_point() +
      geom_vline(xintercept = optSize, color = "red") +
      labs(title = "Error vs tree size") +
      theme_bw() 
  
p2 <- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +
      geom_line() + 
      geom_point() +
      labs(title = "Error vs penalization (alpha)") +
      theme_bw() 

ggarrange(p1, p2)
```

We could have tried to obtain a bigger tree with the hope that pruning might find a better tree.  This can be done setting the tree parameters to minimal values.


```{r fitAmesRT2}
ctlPars2 <-tree.control(nobs=nrow(ames_train), mincut = 1, minsize = 2, mindev = 0)
ames_rt2 <-  tree::tree(
                    formula = Sale_Price ~ .,
                    data    = ames_train,
                    split   = "deviance",
                    control = ctlPars2)
summary(ames_rt2)
```

This bigger tree has indeed a smaller deviance but pruning provides no benefit:

```{r pruneAmes2}
set.seed(123)
cv_ames_rt2 <- tree::cv.tree(ames_rt2, K = 5)

optSize2 <- rev(cv_ames_rt2$size)[which.min(rev(cv_ames_rt2$dev))]
paste("Optimal size obtained is:", optSize2)
```

```{r}
prunedTree2 <- tree::prune.tree(
                  tree = ames_rt2,
                  best = optSize2
               )
summary(prunedTree2)
```


```{r}
res_cv2 <- data.frame(
                   n_nodes  = cv_ames_rt2$size,
                   deviance = cv_ames_rt2$dev,
                   alpha    = cv_ames_rt2$k
                 )

p1 <- ggplot(data = res_cv2, aes(x = n_nodes, y = deviance)) +
      geom_line() + 
      geom_point() +
      geom_vline(xintercept = optSize2, color = "red") +
      labs(title = "Error vs tree size") +
      theme_bw() 
  
p2 <- ggplot(data = res_cv2, aes(x = alpha, y = deviance)) +
      geom_line() + 
      geom_point() +
      labs(title = "Error vs penalization (alpha)") +
      theme_bw() 

ggarrange(p1, p2)
```

The performance of the trees is hardly different between small or big tree in pruned or non-pruned version.

```{r ames_rt_pred1}
ames_rt_pred1 <- predict(ames_rt1, newdata = ames_test)
test_rmse1    <- sqrt(mean((ames_rt_pred1 - ames_test$Sale_Price)^2))
paste("Error test (rmse) for initial tree:", round(test_rmse1,2))
```


```{r ames_rt_pred2}
ames_rt_pred2 <- predict(ames_rt2, newdata = ames_test)
test_rmse2    <- sqrt(mean((ames_rt_pred2 - ames_test$Sale_Price)^2))
paste("Error test (rmse) for big tree:", round(test_rmse2,2))
```

```{r ames_pruned_pred}
ames_pruned_pred <- predict(prunedTree2, newdata = ames_test)
test_rmse3    <- sqrt(mean((ames_pruned_pred - ames_test$Sale_Price)^2))
paste("Error test (rmse) for pruned tree:", round(test_rmse3,2))
improvement <- (test_rmse3-test_rmse2)/test_rmse2*100
```

The MSE for each model will be saved to facilitate comparison with other models

```{r}
errTable <- data.frame(Model=character(),  RMSE=double())
errTable[1, ] <-  c("Default Regression Tree", round(test_rmse1,2))
errTable[2, ] <-  c("Big Regression Tree", round(test_rmse2,2))
errTable[3, ] <-  c("Optimally pruned Regression Tree", round(test_rmse3,2))
```

In summary, what is illustrated by this example is that, for some datasets, it is very hard to obtain an optimal tree because there seems to be a minimum complexity which is very hard to decrease. 

Building a saturated tree only provides a slight improvement of less than 5% in RMSE at the cost of having to use 5 times more variables in a tree withh more than 1000 nodes.

This is a good point to consider using an ensemble instead of single trees.


# Bagging trees

The first attempt to build an ensemble may be to apply `bagging` that is building multiple trees from a set of resamples and averaging the predictions of each tree.

In this example, rather than use a single pruned decision tree, we can use, say, 100 bagged unpruned trees (by not pruning the trees we're keeping bias low and variance high which is when bagging will have the biggest effect).

Bagging is equivalent to RandomForest if we use all the trees so the library `randomForest` is used.

```{r fitbaggedTrees1}
# make bootstrapping reproducible
set.seed(123)

library(randomForest)
bag.Ames <- randomForest(Sale_Price ~ ., 
                         data = ames_train, 
                         mtry = ncol(ames_train-1), 
                         # ntree = 100,
                         importance = TRUE)
```

Bagging, as most ensemble procedures, can be time consuming.
See [@Boehmke2020](https://bradleyboehmke.github.io/HOML/bagging.html#easily-parallelize) for an example on how to easily parallelize code, and save time.

```{r}
show(bag.Ames)
```

## Distinct error rates

The following chunks of code show the fit between data and predictions for the train set, the test set and the out-of bag samples

### Error estimated from train samples

```{r predictBagTrain}
yhattrain.bag <- predict(bag.Ames, newdata = ames_train)
plot(yhattrain.bag, ames_train$Sale_Price)
abline(0, 1)
train_mse_bag  <- mean(yhattrain.bag - ames_train$Sale_Price)^2
paste("Error train (rmse) for bagged tree:", round(train_mse_bag,2))
```


### Error estimated from test samples

```{r predictBagTest}
yhat.bag <- predict(bag.Ames, newdata = ames_test)
plot(yhat.bag, ames_test$Sale_Price)
abline(0, 1)
test_mse_bag  <- mean(yhat.bag - ames_test$Sale_Price)^2
paste("Error test (rmse) for bagged tree:", round(test_mse_bag,2))
```

### Error estimated from out-of-bag samples

Bagging allows computing an out-of bag error estimate. 

Out of bag error rate is reported in the output and can be computed from the `predicted` (*"the predicted values of the input data based on out-of-bag samples"*) 


```{r predictBagOOB}
oob_err<- mean((bag.Ames$predicted-ames_train$Sale_Price)^2)
plot(bag.Ames$predicted, ames_train$Sale_Price)
abline(0, 1)
paste("Out of bag error for bagged tree:", round(oob_err,2))
```

Interestingly this may be not only bigger than the error estimated on the train set but also bigger than the error estimated on the test set.


## Bagging parameter tuning

Bagging tends to improve quickly as the number of resampled trees increases, and then it reaches a platform.

The figure below has been produced iterated the computation above over `nbagg` values of 1–200 and applied the `bagging()` function.

```{r, echo=FALSE, fig.align='center', out.width="100%", fig.cap="Error curve for bagging 1-200 deep, unpruned decision trees. The benefit of bagging is optimized at 187 trees although the majority of error reduction occurred within the first 100 trees"}
knitr::include_graphics("images/baggingRSME.png")
```

## Variable importance

Due to the bagging process, models that are normally perceived as interpretable are no longer so. 

However, we can still make inferences about how features are influencing our model using *feature importance* measures based on the sum of the reduction in the loss function (e.g., SSE) attributed to each variable at each split in a given tree.

```{r}
require(dplyr)
VIP <- importance(bag.Ames) 
VIP <- VIP[order(VIP[,1], decreasing = TRUE),]
head(VIP, n=30)
```

Importance values can be plotted directly:

```{r}
invVIP <-VIP[order(VIP[,1], decreasing = FALSE),1] 
tVIP<- tail(invVIP, n=15)
barplot(tVIP, horiz = TRUE, cex.names=0.5)
```

Alternatively one can use the `vip`function from the `vip` package


```{r}
if (!require(vip)) install.packages("vip", dep=TRUE)
vip(bag.Ames, num_features = 40, bar = FALSE)
```

# A random forest to improve bagging

Bagging can be improved if, instead of using all variables to build each tree we rely on subsets of variables, which are chosen at each split in order to decrease correlation between trees.

Random Forests are considered to produce good predictors with default values sop no parameter is set in a first iteration.

```{r fitRF}
# make bootstrapping reproducible
set.seed(123)

require(randomForest)
RF.Ames <- randomForest(Sale_Price ~ ., 
                         data = ames_train, 
                         importance = TRUE)
```


```{r}
show(RF.Ames)
```

```{r predictRF}
yhat.rf <- predict(RF.Ames, newdata = ames_test)
plot(yhat.rf, ames_test$Sale_Price)
abline(0, 1)

test_rmse_rf  <- sqrt(mean((yhat.rf - ames_test$Sale_Price)^2))
paste("Error test (rmse) for Random Forest:", round(test_rmse_rf,2))
errTable[5, ] <-  c("Random Forest (defaults)", round(test_rmse_rf,2))
```

There is some improvement on bagging but it is clearly small.

Notice however that the percentage of explained variance is bigger for RF than for Bag

## Parameter optimization for RF

Several parameters can be changed to optimize a random forest predictor, but, usually, the most important one is the *number of variables* to be randomly selected at each split $m_{try}$, followed by the number of tree, which tends to stabilize after a certain value.

A common strategy to find the optimum combination of parameters is  to perform a *grid search* through a combination of parameter values. Obviously it can be time consuming so a small grid is run in the example below to illustrate how to do it.


```{r}
num_trees_range <- seq(100, 400, 100)

num_vars_range <- floor(ncol(ames_train)/(seq(2,4,1)))

RFerrTable <- data.frame(Model=character(), 
                       NumTree=integer(), NumVar=integer(), 
                       RMSE=double())
errValue <- 1
system.time(
for (i in seq_along(num_trees_range)){
  for (j in seq_along(num_vars_range)) {  
    numTrees <- num_trees_range[i]
    numVars <- num_vars_range [j] # floor(ncol(ames_train)/3) # default
    RF.Ames.n <- randomForest(Sale_Price ~ ., 
                         data = ames_train, 
                         mtry = numVars,
                         ntree= numTrees,
                         importance = TRUE)
    yhat.rf <- predict(RF.Ames.n, newdata = ames_test)
    oob.rf <- RF.Ames.n$predicted
    
    test_rmse_rf  <- sqrt(mean((yhat.rf - ames_test$Sale_Price)^2))

  
    RFerrTable[errValue, ] <-  c("Random Forest", 
                            NumTree = numTrees, NumVar = numVars, 
                            RMSE = round(test_rmse_rf,2)) 
    errValue <- errValue+1
  }
}
)
```
  
```{r}
RFerrTable %>% kableExtra::kable()
```

The minimum RMSE is attained at.

```{r}
RFerrTable[RFerrTable$RMSE==min(RFerrTable$RMSE),]
```


# Random Forests with Python

The following link points to a good brief tutorial on how to train and evaluate Random Forests using Python

[DataCamp: Random Forest Classification with Scikit-Learn](https://www.datacamp.com/tutorial/random-forests-classifier-python)

# References
