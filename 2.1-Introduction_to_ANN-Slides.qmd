---
title:  "Introduction to Artificial Neural Networks"
author: "A. Sanchez, F. Reverter and E. Vegas"
format:
  revealjs: 
    incremental: false  
    transition: slide
    background-transition: fade
    transition-speed: slow
    scrollable: true
    menu:
      side: left
      width: half
      numbers: true
    slide-number: c/t
    show-slide-number: all
    progress: true
    css: "css4CU.css"
    theme: sky
knit:
  quarto:
    chunk_options:
      echo: true
      cache: false
      prompt: false
      tidy: true
      comment: NA
      message: false
      warning: false
    knit_options:
      width: 75
bibliography: "DeepLearning.bib"
editor_options: 
  chunk_output_type: console
---

# From Artificial Neural Networks to ARtifial Intelligence

## Historical Background (1)

-   In the post-pandemic world, a lightning rise of AI, with a mess of realities and promises is impacting society.

-   Since ChatGPT entered the scene everybody has an experience, an opinion, or a fear on the topic.

![](https://bernardmarr.com/wp-content/uploads/2022/04/The-Dangers-Of-Not-Aligning-Artificial-Intelligence-With-Human-Values.jpg){fig-align="center" width="100%"}

## Is it just machine learning?

- Most tasks performed by AI can be described as Classification or  Prediction used in applications as:
   
   - Recommendation systems,
   - Image recognition, Image generation
   - Natural language processing

-   AI relies on machine learning algorithms, to make predictions based on large amounts of data.

-   AI has far-reaching implications beyond its predictive capabilities, including ethical, social or technological.

## AI, ANNs and Deep learning

- In many contexts, talking about AI means talking about 
*Deep Learning (DL)*.

- DL is a successful AI model which has powered many application such as *self-driving cars, voice assistants, and medical diagnosis systems*.

- DL originates in the field of *Artificial Neural Networks*

- But DL extends the basic principles of ANNs by:

    -   Adding complex architectures and algorithms and
    -   At the same time becoming more automatic


## The early history of AI (1)


```{r, out.width="90%", fig.cap= "[A Quick History of AI, ML and DL](https://nerdyelectronics.com/a-quick-history-of-ai-ml-and-dl/)"}
knitr::include_graphics("images/AIHistory1.jpg")
```


## Milestones in the history of DL

We can see several hints worth to account for:

-   The **Perceptron** and the first **Artificial Neural Network** where the basic building block was introduced.

-   The **Multilayered perceptron** and back-propagation where complex architectures were suggested to improve the capabilities.

-   **Deep Neural Networks**, with many hidden layers, and auto-tunability capabilities.

## From ANN to Deep learning

![Why Deep Learning Now?](images/WhyDLNow.png){fig-align="center" width="100%"} 

:::{.font90}
Source: [Alex Amini's MIT Introduction to Deep Learning' course](introtodeeplearning.com)
:::

## Success stories

Success stories such as

- the development of self-driving cars,

- the use of AI in medical diagnosis, and

- online shopping personalized recommendations

have also contributed to the widespread adoption of AI.

## Not to talk abou the fears

:::: {.columns}

::: {.column width='60%'}
- AI also comes with fears from multiple sources from science fiction to religion

  - Mass unemployment
  
  - Loss of privacity
  
  - AI bias
  
  - AI fakes
  
  - Or, simply, AI takeover

:::

::: {.column width='40%'}

<br>

![](2.1-Introduction_to_ANN-Slides_insertimage_2.png)

:::

::::


## Back to science

Where/How does it all fit?

```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/AI-ML-DL-1.jpg")
```

## AI, ML, DL ...

-   **Artificial intelligence**: Ability of a computer to perform tasks commonly associated with intelligent beings.

-   **Machine learning**: study of algorithms that learn from examples and experience instead of relying on hard-coded rules and make predictions on new data

-   **Deep learning**: sub field of ML focusing on learning data representations as successive successive layers of increasingly meaningful representations.

## How does DL improve

```{r, fig.align='center', out.width="100%", fig.cap="[ML and DL Approaches for Brain Disease Diagnosis](https://ieeexplore.ieee.org/document/9363896)"}
knitr::include_graphics("images/ML_vs_DL-2.png")
```


::: {.notes}
-   DNN: feature extraction and classification without (or with much les) human intervention.
-   DNN improves with data availability, without seemingly reaching plateaus.
:::

## Size does matter!

![An illustration of the performance comparison between deep learning (DL) and other machine learning (ML) algorithms, where DL modeling from large amounts of data can increase the performance](images/PerformanceVsAmountOfData.png){fig-align="center" width="100%"}

## The impact of Deep learning {.smaller}

-   Near-human-level image classification

-   Near-human-level speech transcription

-   Near-human-level handwriting transcription

-   Dramatically improved machine translation

-   Dramatically improved text-to-speech conversion

-   Digital assistants such as Google Assistant and Amazon Alexa

-   Near-human-level autonomous driving

-   Improved ad targeting, as used by Google, Baidu, or Bing

-   Improved search results on the web

-   Ability to answer natural language questions

-   Superhuman Go playing

## Not all that glitters is gold ...

-   According to F. Chollet, the developer of Keras,

    -   "*we shouldn't believe the short-term hype, but should believe in the long-term vision*.
    -   *It may take a while for AI to be deployed to its true potential---a potential the full extent of which no one has yet dared to dream*
    -   *but AI is coming, and it will transform our world in a fantastic way*".

# The Artificial Neurone (AN)


## Emulating biological neurons


:::: {.columns}

::: {.column width='50%'}

```{r, fig.align='center', out.width="100%", fig.cap="[A biological Neuron]()"}
knitr::include_graphics("images/NaturalNeuron.png")
```

:::

::: {.column width='50%'}

```{r, fig.align='center', out.width="100%", fig.cap="[MuCulloch & Pitts proposal]()"}
knitr::include_graphics("images/MacCulloghPitts-Neuron.png")
```

:::

::::

- The first model of an artifial neurone was proposed by Mc Cullough & Pitts in 1943


## Mc Cullough's neuron

-   It may be divided into 2 parts.
    -   The first part, $g$,takes an input (as the dendrites of a neuron would do),
    -   It performs an aggregation and
    -   based on the aggregated value the second part, $f$, makes a decision.

See [the source of this picture](https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1) for an illustration on how this can be used to emulate logical operations such as AND, OR or NOT, but not XOR.

## Limitations

This first attempt to emulate neurons succeeded but with limitations:

-   What about non-Boolean (say, real) inputs?

-   What if all inputs are not equal?

-   What if we want to assign more importance to some inputs?

-   What about functions which are not linearly separable? Say XOR function

## Overcoming the limitations

-   To overcome these limitations Rosenblatt, proposed the perceptron model, or  *artificial neuron*, in 1958.

-   Generalizes McCullough-Pitts neuron in that *weights and thresholds can be learnt over time*.

    -   It takes a weighted sum of the inputs and
    -   It sets the output to iff the sum is more than an arbitrary threshold (**$\theta$**).

## Rosenblatt's perceptron

[![](images/RosenblattPerceptron1.png){width="100%"}](https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d)

## Rosenblatt's perceptron

:::{.font80}
-   Instead of hand coding the thresholding parameter $\theta$,
-   It is added as one of the inputs, with the weight $w_0=-\theta$.
:::

[![](images/RosenblattPerceptron2.png){width="100%"}](https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d)

## Comparison between the two

[![](images/McCullaughVSRosenblattPerceptron.png){width="100%"}](https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d)

## Comparison between the two

-   This is an improvement because

    -   both, weights and threshold, can be learned and
    -   the inputs can be real values

-   But there is still a drawback in that a single perceptron can only be used to implement linearly separable functions.

-   Artificial Neural Networks improve on this by introducing *Activation Functions*

## Activation in biological neurons

-   Biological neurons are specialized cells that transmit signals to communicate with each other.
- Neuron's activation is based on releasing *neurotransmitters*, chemicals that transmit signals between nerve cells.
    -   When the signal reaching the neuron exceeds a certain threshold, it releases neurotransmitters to continue the communication process.

## Activation functions in AN

-   Analogously, *activation functions* in AN are functions to decide if the AN it is activated or not.
-   AN's activation function is a mathematical function applied to the neuron's input to produce an output.
    -   In practice it extends to complicated functions that can learn complex patterns in the data.
    -   Activation functions can incorporate non-linearity, improving over linear classifiers.

## Activation function

```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/ActivationFunction0.png")
```

## Artificial Neuron

With all these ideas in mind we can now define an Artificial Neuron as a *computational unit* that :

-   takes as input $x=(x_0,x_1,x_2,x_3),\ (x_0 = +1 \equiv bias)$,

-   outputs $h_{\theta}(x) = f(\theta^\intercal x) = f(\sum_i \theta_ix_i)$,

-   where $f:\mathbb{R}\mapsto \mathbb{R}$ is called the **activation function**.

## Activation functions

:::{.font90}

-   Goal of activation function is to provide the neuron with *the capability of producing the required outputs*.

-   Flexible enough to produce

    -   Either linear or non-linear transformations.
    -   Output in the desired range (\[0,1\], {-1,1}, $\mathbb{R}^+$...)

-   Usually chosen from a (small) set of possibilities.

    -   Sigmoid function
    -   Hyperbolic tangent, or `tanh`, function
    -   ReLU

:::

## The sigmoid function {.smaller}

::: columns
::: {.column width="50%"}

$$
f(z)=\frac{1}{1+e^{-z}}
$$

-   Output real values $\in (0,1)$.

-   Natural interpretations as *probability*


```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/sigmoidFunction.png")
```

:::

::: {.column width="50%"}

```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/sigmoidFunctionDerivative.png")
```


:::
:::

## the hyperbolic tangent {.smaller}

::: columns
::: {.column width="50%"}
Also called `tanh`, function:

$$
f(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}
$$

-   outputs are zero-centered and bounded in −1,1

-   scaled and shifted Sigmoid

-   stronger gradient but still has vanishing gradient problem

-   Its derivative is $f'(z)=1-(f(z))^2$.
:::

::: {.column width="50%"}
```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/TanhFunction.png")
```
:::
:::

## The ReLU {.smaller}

::: columns
::: {.column width="50%"}
-   *rectified linear unit*: $f(z)=\max\{0,z\}$.

-   Close to a linear: piece-wise *linear* function with two linear pieces.

-   Outputs are in %(0,\infty)\$ , thus not bounded

-   Half rectified: activation threshold at 0

-   No vanishing gradient problem
:::

::: {.column width="50%"}
```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/ReLUFunction.png")
```
:::
:::

## More activation functions

![](images/ActivationFunctions.png){width="100%"}.

## Putting it all together

```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/ArtificialNeuron.png")
```

## In words

- An ANN a vector of input values $x_{1}, \ldots, x_{d}$ and combines it with some weights that are local to the neuron $\left(w_{0}, w_{1}, . ., w_{d}\right)$ to compute a net input $w_{0}+\sum_{i=1}^{d} w_{i} * x_{i}$. 

- To compute its output, it then passes the net input through a possibly non-linear univariate activation function $g(\cdot)$, usually vchosen from a set of options such as *Sigmoid*, *Tanh* or *ReLU* functions

- To dela with the  *bias*, we create an extra input variable $x_{0}$ with value always equal to 1 , and so the function computed by a single artificial neuron (parameterized by its weights $\mathbf{w}$ ) is:

$$
y(\mathbf{x})=g\left(w_{0}+\sum_{i=1}^{d} w_{i} x_{i}\right)=g\left(\sum_{i=0}^{d} w_{i} x_{i}\right)=g\left(\mathbf{w}^{\mathbf{T}} \mathbf{x}\right)
$$

# From neurons to neural networks

## The basic neural network {.smaller}

Following with the brain analogy one can combine (artificial) neurons to create better learners.

A simple artificial neural network is usually created by combining two types of modifications to the basic perceptron (AN).

- Stacking several neurons insteads of just one.
- Adding an additional layer of neurons, which is call a *hidden* layer, 

This yields a system where the output of a \emph{neuron} can be the input
of another in many different ways.


## An Artificial Neural network

```{r, fig.align='center', out.width="90%"}
knitr::include_graphics("images/nn.jpg")
```

## The architecture of ANN {.smaller}

In this figure, we have used circles to also denote the inputs to the
network. 

- Circles labeled +1 are *bias units*, and correspond to the intercept term. 

- The leftmost layer of the network is called the *input layer*.

- The rightmost layer of the network is called the output layer.

  - In this example, the output layer has only one node. 

- The middle layer of nodes is called the *hidden layer*, because its values are not observed in the training set.


Bias nodes are not counted when stating the neuron size. In the example above we also say that our example neural network has:

 - 3 input units (not counting the bias unit), 
 - 3 hidden units, and 
 - 1 output unit.

## How an ANN works {.smaller}

An ANN is a predictive model (a *learner) whose properties and behaviour  can be well characterized.

In practice this means:

- The ANN operates through a process known as *forward propagation*, which encompasses the entire journey of information flow from the input layer to the output layer.

- Forward propagation is performed by composing a series of linear and non-linear (activation) functions.

- These are characterized (parametrized) by their *weights* and *biases*, that need to be *learnt*. This is done by *training the ANN*.

## Training the ANN {.smaller}

- In order for the ANN to perform well, the training process aims at finding the best possible  parameter values for the learning task defined by the fnctions. This is done by

  - Selecting an appropriate (convex) loss function,
  - Finding those weights that minimize a the total *cost* function (avg. loss).

- This is usually done using some iterative optimization procedure such as *gradient descent*.
  - This requires evaluating derivatives in a huge number of points.
  - Such high number may be reduced by *Stochastic Gradient Descent*.
  - The evaluation of derivatives is simplified thanks to *Backpropagation*.


## Forward propagation {.smaller}

As described above the process that encompasses the computations required to go from the input values to the final output is known as *forward propagation*.

The  weights are combined with the input to produce the final output.

Each node, $a_i^{(2)}$ of the hidden layer opperates on all nodes of the input values

\begin{eqnarray}
a_1^{(2)}&=&f(\theta_{10}^{(1)}+\theta_{11}^{(1)}x_1+\theta_{12}^{(1)}x_2+\theta_{13}^{(1)}x_3)\\
a_2^{(2)}&=&f(\theta_{20}^{(1)}+\theta_{21}^{(1)}x_1+\theta_{22}^{(1)}x_2+\theta_{23}^{(1)}x_3)\\
a_3^{(2)}&=&f(\theta_{30}^{(1)}+\theta_{31}^{(1)}x_1+\theta_{32}^{(1)}x_2+\theta_{33}^{(1)}x_3))
\end{eqnarray}

The output of the hidden layer is transformed through the activation function:

$$
h_{\Theta}(x)=a_1^{(3)}=f(\theta_{10}^{(2)}+\theta_{11}^{(2)}a_1^{(2)}+\theta_{12}^{(2)}a_2^{(2)}+\theta_{13}^{(2)}a_3^{(2)}
$$

## A compact notation (1) {.smaller}

Let  $z_i^{(l)}$ denote the total weighted sum of inputs to unit $i$ in layer $l$:

$$
z_i^{(2)}=\theta_{i0}^{(1)}+\theta_{i1}^{(1)}x_1+\theta_{i2}^{(1)}x_2+\theta_{i3}^{(1)}x_3,
$$ 
the output becomes: $a_i^{(l)}=f(z_i^{(l)})$.


Extending the activation function $f(\cdot)$ to apply elementwise to vectors: 

$$
    f([z_1,z_2,z_3]) = [f(z_1), f(z_2),f(z_3)],
$$
we can write the previous equations more compactly as:

```{=tex}
\begin{eqnarray}
z^{(2)}&=&\Theta^{(1)}x\nonumber\\
a^{(2)}&=&f(z^{(2)})\nonumber\\
z^{(3)}&=&\Theta^{(2)}a^{(2)}\nonumber\\
h_{\Theta}(x)&=&a^{(3)}=f(z^{(3)})\nonumber
\end{eqnarray}
```

## A compact notation (2) {.smaller}

More generally, recalling that we also use $a^{(1)}=x$ to also  denote the values from the input layer,

Given layer $l$'s activations $a^{(l)}$, we can compute layer   $l+1$'s activations $a^{(l+1)}$ as:


\begin{equation}
z^{(l+1)}=\Theta^{(l)}a^{(l)}
\label{eqforZs}
\end{equation}

\begin{equation}
a^{(l+1)}=f(z^{(l+1)})
\label{eqforAs}
\end{equation}

## A compact notation (3) {.smaller}


This can be used to provide a matrix representation for the weighted sum
of inputs of all neurons:

$$
z^{(l+1)}=
\begin{bmatrix}
z_1^{(l+1)}\\
z_2^{(l+1)}\\
\vdots\\
z_{s_{l+1}}^{(l)}
\end{bmatrix}=
\begin{bmatrix}
\theta_{10}^{(l)}& \theta_{11}^{(l)}&\theta_{12}^{(l)}&...&\theta_{1s_{l}}^{(l)}&\\
\theta_{20}^{(l)}& \theta_{21}^{(l)}&\theta_{22}^{(l)}&...&\theta_{2s_{l}}^{(l)}&\\
\vdots & \vdots& \vdots & \vdots & \vdots\\
\theta_{s_{l+1}0}^{(l)}& \theta_{s_{l+1}1}^{(l)}&\theta_{s_{l+1}2}^{(l)}&...&\theta_{s_{l+1}s_{l}}^{(l)}&\\
\end{bmatrix}
\cdot\begin{bmatrix}
1\\
a_1^{(l)}\\
a_2^{(l)}\\
\vdots\\
a_{s_l}^{(l)}
\end{bmatrix}

$$

## A compact notation (4) {.smaller}

So that, the activation is then:

$$
a^{(l+1)}=
\begin{bmatrix}
a_1^{(l+1)}\\
a_2^{(l+1)}\\
\vdots\\
a_{s_{l+1}}^{(l)}
\end{bmatrix}=f(z^{(l+1)})=\begin{bmatrix}
f(z_1^{(l+1)})\\
f(z_2^{(l+1)})\\
\vdots\\
f(z_{s_{l+1}}^{(l)})
\end{bmatrix}
$$

## Multiple architectures for ANN

-   We have so far focused on a single hidden layer neural network of the example. 

- One can. however build neural networks with many distinct   architectures (meaning patterns of connectivity between neurons),
    including ones with multiple hidden layers.

-   See [here the Neural Network
    Zoo](https://www.asimovinstitute.org/neural-network-zoo/).


## Training an ANN {.smaller}

- An ANN is a predictive model whose properties and behaviour  can be mathematically characterized.

- In practice this means:
  - The ANN acts by composing a series of linear and non-linear (activation) functions.
  - These are characterized by their *weights* and *biases*, that need to be *learnt*
  .
- *Training* the network is done by 
  - Selecting an appropriate (convex) loss function,
  - Finding those weights that minimize a the total *cost* function (avg loss).

## The tools for training

- Training an ANN is usually done using some iterative optimization procedure such as *Gradient Descent*.

- This requires evaluating derivatives in a huge number of points.
    - Such high number may be reduced by *Stochastic Gradient Descent*.
    - The evaluation of derivatives is simplified thanks to *Backpropagation*.

## A guiding example

Consider a toy ANN to illustrate the concepts

::: columns
::: {.column width="50%"}
- Input layer with 3 input units (plus bias unit),
- 1 hidden layer with 3 hidden units,
- Output layer with 1 output unit.
:::

::: {.column width="50%"}

```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/nn.jpg")
```
:::
:::

## A logistic regression ANN

- From input layer to layer 2: A non-linear transformation yields new set of complex features.

- From layer 2 to output layer: New features are transformed, using a sigmoid activation function.

$$
\mbox{The final output is: }h_{\theta}(x)=\frac{1}{1+e^{-\theta^\intercal x}}
$$

## Logistic regression (1)

Recall that, the logistic regression model is:

$$
\log\frac{p(Y=1|x)}{1-p(Y=1|x)}=\theta^\intercal x
$$

Isolating $p(Y=1|x)$ and taking logs in both sides:

$$
\frac{p(Y=1|x)}{1-p(Y=1|x)}=e^{\theta^\intercal x}
$$

## Logistic regression (2)

$$
p(Y=1|x)=\frac{e^{\theta^\intercal x}}{1+e^{\theta^\intercal x}}=\frac{1}{1+e^{-\theta^\intercal x}}
$$

-   That is: *when the activation function of the output node is the sigmoid activation function, the output coincides with a logistic regression on complex features*

-   The output of the ANN. $h_{\theta}(x)$, estimates $p(Y=1|x)$.

## ANN have weights aka *parameters*

- Let $n_l$ denote the number of layers in our network, thus $n_l=3$ in our example.
- Layer $L_1$ is input layer, and layer $L_{n_l}=L_3$ the output layer.
- Our ANN has parameters $\Theta=(\Theta^{(1)},\Theta^{(2)})$, where:
  - $\theta^{(l)}_{ij}$ denotes weights associated with
    - the connection between unit $j$ in layer $l$ and
    - unit $i$ in layer $l+1$.

## Back to the example:

-   Thus, in our example, we have:

    -   $\Theta^{(1)}\in\mathbb{R}^{3\times 4}$, and
    -   $\Theta^{(2)}\in\mathbb{R}^{1\times 4}$.

Note that bias units don't have inputs or connections going into them, since they always output the value +1.

## The ANN is defined by its weights

- Let $s_l$ denote the number of nodes in layer $l$ (not counting the bias unit).

- Now, write $a^{(l)}_i$ to denote the activation (meaning output value) of unit $i$ in layer $l$.

  - For $l=1$, $a^{(1)}_i=x_i$ denotes $i$-th input.

- Given a fixed setting of the parameters $\Theta$, this ANN defines a model $h_{\Theta}(x)$ that outputs a real number.

## Combining everything to compute {.smaller}

-   We can now see *how these weights are used to produce the output*: \begin{eqnarray}
    a_1^{(2)}&=&f(\theta_{10}^{(1)}+\theta_{11}^{(1)}x_1+\theta_{12}^{(1)}x_2+\theta_{13}^{(1)}x_3)\\
    a_2^{(2)}&=&f(\theta_{20}^{(1)}+\theta_{21}^{(1)}x_1+\theta_{22}^{(1)}x_2+\theta_{23}^{(1)}x_3)\\
    a_3^{(2)}&=&f(\theta_{30}^{(1)}+\theta_{31}^{(1)}x_1+\theta_{32}^{(1)}x_2+\theta_{33}^{(1)}x_3)\\
    h_{\Theta}(x)&=&a_1^{(3)}=f(\theta_{10}^{(2)}+\theta_{11}^{(2)}a_1^{(2)}+\theta_{12}^{(2)}a_2^{(2)}+\theta_{13}^{(2)}a_3^{(2)})
    \end{eqnarray}

-   Now, letting $z_i^{(l)}$ denote the total weighted sum of inputs to unit $i$ in layer $l$, including the bias term $$z_i^{(2)}=\theta_{i0}^{(1)}+\theta_{i1}^{(1)}x_1+\theta_{i2}^{(1)}x_2+\theta_{i3}^{(1)}x_3,
    $$ the output becomes: $a_i^{(l)}=f(z_i^{(l)})$.

## Compacting notation {.smaller}

- Note that this easily lends itself to a more compact notation.

- Activation functions $f(\cdot)$ apply to individual neurons, so it is admisible to write:
$$
    f([z_1,z_2,z_3]) = [f(z_1), f(z_2),f(z_3)],
$$

- A more compact notation for previous equations:

$$
\begin{eqnarray*}
z^{(2)}&=&\Theta^{(1)}x\nonumber\\
a^{(2)}&=&f(z^{(2)})\nonumber\\
z^{(3)}&=&\Theta^{(2)}a^{(2)}\nonumber\\
h_{\Theta}(x)&=&a^{(3)}=f(z^{(3)})\nonumber
\end{eqnarray*}
$$

## Compacting notation (II)

-   More generally, recalling that we also use $a^{(1)}=x$ to also denote the values from the input layer,

-   then given layer $l$'s activations $a^{(l)}$, we can compute layer $l+1$'s activations $a^{(l+1)}$ as:

```{=tex}
\begin{eqnarray}
z^{(l+1)}&=&\Theta^{(l)}a^{(l)}\\
a^{(l+1)}&=&f(z^{(l+1)})
\end{eqnarray}
```

## Matricial representation (I) {.smaller}

- The output in layer $l+1$, from layer $l$ can be written as a *matrix product*

$$
z^{(l+1)}=
\begin{bmatrix}
z_1^{(l+1)}\\
z_2^{(l+1)}\\
\vdots\\
z_{s_{l+1}}^{(l)}
\end{bmatrix}=
\begin{bmatrix}
\theta_{10}^{(l)}& \theta_{11}^{(l)}&\theta_{12}^{(l)}&...&\theta_{1s_{l}}^{(l)}&\\
\theta_{20}^{(l)}& \theta_{21}^{(l)}&\theta_{22}^{(l)}&...&\theta_{2s_{l}}^{(l)}&\\
\vdots & \vdots& \vdots & \vdots & \vdots\\
\theta_{s_{l+1}0}^{(l)}& \theta_{s_{l+1}1}^{(l)}&\theta_{s_{l+1}2}^{(l)}&...&\theta_{s_{l+1}s_{l}}^{(l)}&\\
\end{bmatrix}
\cdot\begin{bmatrix}
1\\
a_1^{(l)}\\
a_2^{(l)}\\
\vdots\\
a_{s_l}^{(l)}
\end{bmatrix}
$$

## Matricial representation (II)

The activation is then:

$$
a^{(l+1)}=
\begin{bmatrix}
a_1^{(l+1)}\\
a_2^{(l+1)}\\
\vdots\\
a_{s_{l+1}}^{(l)}
\end{bmatrix}=f(z^{(l+1)})=\begin{bmatrix}
f(z_1^{(l+1)})\\
f(z_2^{(l+1)})\\
\vdots\\
f(z_{s_{l+1}}^{(l)})
\end{bmatrix}
$$


## Eficient Forward propagation

- The way input data is transformed, through a series of weightings and transformations, until the ouput layer is called   *forward propagation*.

- By organizing parameters in matrices, and using matrix-vector operations, fast linear algebra routines can be used to perform the required calculations in a fast efficent way.


## Multiple architectures for ANN {.smaller}

:::: {.columns}

::: {.column width='50%'}

- We have so far focused on a single hidden layer neural network of the example

- One can build neural networks with many distinct architectures (meaning patterns of connectivity between neurons), including ones with multiple hidden layers.


:::

::: {.column width='50%'}

![](2.1-Introduction_to_ANN-Slides_insertimage_3.png)
[The Neural Network Zoo](https://www.asimovinstitute.org/neural-network-zoo/)

:::

::::


## Multiple layer dense Networks

-   Most common choice is a $n_l$-layered network:
    -   layer 1 is the input layer,
    -   layer $n_l$ is the output layer,
    -   and each layer $l$ is densely connected to layer $l+1$.
-   In this setting, to compute the output of the network, we can compute all the activations in layer $L_2$, then layer $L_3$, and so on, up to layer $L_{nl}$, using equations seen previously.

## Feed Forward NNs

-   The type of NN described is called feed-forward *neural network (FFNN)*, since
    -   All computations are done by Forward propagation
    -   The connectivity graph does not have any directed loops or cycles.

# Optimizing the ANN

# Optimizitation for ANN

- The equations described above show how to use forward propagation to yield a prediction, given an input and a set of parameters (weights and biases).

- As with any learning tasks what we want to do is finding a set of parameter's best values that, in some sense provide the best possible prediction.

- For this we need two things

  - A loss function to quantify the difference between reali world and predictions.
  - Some optimization procedure(s) to iteratively tune the parameter' values in order to minimize the loss.

## A loss function for optimization

A first idea may be to use *squared error loss*:

$$
  l(h_\theta(x),y)=\left (y-\frac{1}{1+e^{-\theta^\intercal x}}\right )^2
$$

However it happens to be that [*this is is not a convex   problem*](https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c)  which means that MSE is not appropriate.

## Non convexity illustration {.smaller}

```{r, eval=FALSE}
library(plot3D)

# Define the squared error loss function
squared_error <- function(y, y_hat) {
  return(0.5 * (y - y_hat)^2)
}

# Define the logistic activation function
logistic <- function(z) {
  return(1 / (1 + exp(-z)))
}

# Generate data
x <- seq(-10, 10, length.out = 200)
y <- seq(-10, 10, length.out = 200)
z <- outer(x, y, FUN = function(x, y) squared_error(1, logistic(x + y)))

# Scale the z values to exaggerate the differences
z <- z * 500

# Plot the exaggerated loss surface
persp3D(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "lightblue", ticktype = "detailed", xlab = "Weight 1", ylab = "Weight 2", zlab = "Loss")

```

## Non convexity illustration {.smaller}

```{r echo=FALSE}
library(plot3D)

# Define the squared error loss function
squared_error <- function(y, y_hat) {
  return(0.5 * (y - y_hat)^2)
}

# Define the logistic activation function
logistic <- function(z) {
  return(1 / (1 + exp(-z)))
}

# Generate data
x <- seq(-10, 10, length.out = 200)
y <- seq(-10, 10, length.out = 200)
z <- outer(x, y, FUN = function(x, y) squared_error(1, logistic(x + y)))

# Scale the z values to exaggerate the differences
z <- z * 500

# Plot the exaggerated loss surface
persp3D(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "lightblue", ticktype = "detailed", xlab = "Weight 1", ylab = "Weight 2", zlab = "Loss")

```


## **Binary cross-entropy loss function** 

$$
    l(h_\theta(x),y)=\big{\{}\begin{array}{ll}
    -\log h_\theta(x) & \textrm{if }y=1\\
    -\log(1-h_\theta(x))& \textrm{if }y=0
    \end{array}
$$

This function can also be written as:

$$
l(h_\theta(x),y)=-y\log h_\theta(x) - (1-y)\log(1-h_\theta(x))
$$

Using cross-entropy loss, the cost function is of the form:

\begin{eqnarray*}
    J(\theta)=-\frac{1}{n}\big[\sum_{i=1}^n&&(y^{(i)}\log h_\theta(x^{(i)})+\\ &&(1-y^{(i)})\log(1-h_\theta(x^{(i)}))\big]
\end{eqnarray*}

Now, this is a convex optimization problem.

## Regularized cross entropy

In practice we often work with a *regularized version* of the cost function (we don't regularize the bias units)

```{=tex}
\begin{eqnarray*}
J(\Theta)&=&-\frac{1}{n}\big[\sum_{i=1}^n \sum_{k=1}^K y_k^{(i)}\log( h_\theta(x^{(i)}))_k\\
&+&(1-y_k^{(i)})\log(1-(h_\theta(x^{(i)}))_k)\big]\\
&+&\lambda\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}
(\theta_{ji}^{(l)})^2
\end{eqnarray*}
```


## Gradient Descent {.smaller}

- We saw in the previous section that training a network corresponds to choosing the parameters, that is, the weights and biases, that minimize
the cost function. 

- The weights and biases take the form of
matrices and vectors, but at this stage it is convenient to imagine them
stored as a single vector that we call $\theta$. 

- Generally, we will suppose $\theta\in\mathbb{R}^p$, and write the cost function as $J(\theta)$ to emphasize its dependence on the parameters. So Cost
$J: \mathbb{R}^p\rightarrow \mathbb{R}$.

## Gradient Descent {.smaller}

![Error hyper-surface](images/errorsurface.jpg){width="60%"}

## Gradient Descent {.smaller}

- *Gradient Descent* is a classical method in optimization that is often
referred to as *steepest descent*.

Given a function $J(\theta)$ to be minimized, the method proceeds iteratively, computing a sequence of vectors $\theta^1, \theta^2, ..., \theta^n$ in $\mathbb{R}^p$ with the
aim of converging to a vector that minimizes the cost function. 

Suppose that our current vector is $\theta$. 
*How should we choose a perturbation, $\Delta\theta$, so that the next vector, $\theta+\Delta\theta$, represents an improvement, that is: $J(\theta +\Delta\theta) < J(\theta)$?*


## Gradient Descent {.smaller}

As is often the case, we will proceed by linearization of the cost function using a Taylor series expansion.

If $\Delta\theta$ is small, then ignoring terms of order $||\Delta\theta||^2$ or higher, a Taylor
series expansion gives:

$$
J(\theta+\Delta\theta)\approx J(\theta)+\sum_{i=1}^p\frac{\partial J(\theta)}{\partial\theta_i}\Delta\theta_i
$$ 
where ${\displaystyle \frac{\partial J(\theta)}{\partial\theta_i}}$ denotes the
partial derivative of the cost function with respect to the $i$-th weight. 


## Gradient Descent {.smaller}

For convenience, we will let $\nabla J(\theta)\in\mathbb{R}^p$
denote the vector of partial derivatives, known as the gradient, so that

\begin{equation}\label{g1}
\nabla J(\theta)=\left(\frac{\partial J(\theta)}{\partial\theta_1},...,\frac{\partial J(\theta)}{\partial\theta_p}\right)^\intercal
\end{equation} 

Now the approximation can be written as:

\begin{equation}\label{g2}
J(\theta+\Delta\theta)\approx J(\theta)+\nabla J(\theta)^\intercal\Delta\theta
\end{equation}

## Gradient Descent {.smaller}

Recalling that our aim is to reduce the value of the cost function, the Taylor approximation above motivates the idea of choosing $\Delta\theta$ to *make $\nabla J(\theta)^\intercal\Delta\theta$ negative*, because this will make the value of $J(\theta+\Delta\theta)$ smaller. 

The bigger in absolute value we can make this negative expression, the smaller will be the value of the cost function.

Now, how much is it possible to make  $\nabla J(\theta)^\intercal\Delta\theta$ decrease?

The Cauchy-Schwarz inequality, which states
that for any $f,g\in\mathbb{R}^p$, we have:
$$
|f^\intercal g|\leq ||f||\cdot ||g||.
$$ 
Moreover, the two sides are equal if and only if $f$ and $g$ are linearly dependent (meaning they
are parallel).

So the most negative that $f^\intercal g$ can be is $-||f||\cdot||g||$,
which happens when $f=-g$. Hence we should choose $\Delta\theta$ to lie in the direction of $-\nabla J(\theta)$. 


## Gradient Descent {.smaller}

Keeping in mind that the Taylor linearization of $J(\theta)$ is an approximation that is relevant only for small $\Delta\theta$, we
will limit ourselves to a small step in that direction. This leads to the update:

\begin{equation}\label{g3}
\theta \rightarrow \theta-\eta\nabla J(\theta)
\end{equation}

\underline{This equation defines the steepest descent method}. 

Here $\eta$ is small step size that, in this context, is known as the *learning rate*. 


## Gradient Descent {.smaller}


In summary, givent a cost function $J(\theta)$ to be optimized the gradient descent optimization proceeds as follows:


1. **Initialize** $\theta_0$ randomly or with some predetermined values
2. **Repeat until convergence:**
    $$
    \theta_{t+1} = \theta_{t} - \alpha \nabla J(\theta_{t})
    $$
3. **Stop when:** $|J(\theta_{t+1}) - J(\theta_{t})| < \epsilon$


Where:

- $\theta_0$ is the initial parameter vector,
- $\theta_t$ is the parameter vector at iteration $t$,
- $\alpha$ is the learning rate,
- $\nabla J(\theta_{t})$ is the gradient of the loss function with respect to $\theta$ at iteration $t$,
- $\epsilon$ is a small positive value indicating the desired level of convergence.


## Computing derivatives

In order to use gradient descent, we need to compute $J(\theta)$ and the
partial derivative terms
$$
\frac{\partial}{\partial\theta_j}J(\theta)
$$

## Initialization {.smaller}

The input data have to be normalized to have approximately the same range. 

The biases can be initialized to 0. They also cannot be initialized with the same values, otherwise, all the neurons of a hidden
layer would have the same behavior. 

Perhaps the only property known with
complete certainty is that the initial parameters need to break symmetry
between different units. 

We generally initialize the weights at random:
the values $\theta_{ij}^{(l)}$ are i.i.d. Uniform on $[-c,c]$ with possibly $c= 1/\sqrt{N_l}$ where $N_l$ is the size of the hidden layer
$l$. 

We also sometimes initialize the weights with a normal distribution
$N(0,0.01)$.


# An example using R

## A predictive ANN

We use the `neuralnet` package to build a simple neural network to predict if a type of stock pays dividends or not.

```{r echo=TRUE}
if (!require(neuralnet)) 
  install.packages("neuralnet", dep=TRUE)
```

## Data for the example

And use the `dividendinfo.csv` dataset from <https://github.com/MGCodesandStats/datasets>

```{r echo=TRUE}
mydata <- read.csv("https://raw.githubusercontent.com/MGCodesandStats/datasets/master/dividendinfo.csv")
str(mydata)
```

## Data pre-processing

```{r echo=TRUE}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
normData <- as.data.frame(lapply(mydata, normalize))
```

## Test and training sets

Finally we break our data in a test and a training set:

```{r echo=TRUE}
perc2Train <- 2/3
ssize <- nrow(normData)
set.seed(12345)
data_rows <- floor(perc2Train *ssize)
train_indices <- sample(c(1:ssize), data_rows)
trainset <- normData[train_indices,]
testset <- normData[-train_indices,]
```

## Training a neural network

We train a simple NN with two hidden layers, with 4 and 2 neurons respectively.

```{r echo=TRUE}
#Neural Network
library(neuralnet)
nn <- neuralnet(dividend ~ fcfps + earnings_growth + de + mcap + current_ratio, 
                data=trainset, 
                hidden=c(2,1), 
                linear.output=FALSE, 
                threshold=0.01)
```

## Network plot

The output of the procedure is a neural network with estimated weights

```{r echo=TRUE}
plot(nn, rep = "best")
```

## Predictions

```{r echo=TRUE}
temp_test <- subset(testset, select =
                      c("fcfps","earnings_growth", 
                        "de", "mcap", "current_ratio"))
nn.results <- compute(nn, temp_test)
results <- data.frame(actual = 
                  testset$dividend, 
                  prediction = nn.results$net.result)
head(results)
```

## Model evaluation

```{r echo=TRUE}
roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)
attach(roundedresultsdf)
table(actual,prediction)
```


# Some Extensions
## Gradient descent drawbacks

-   When we have a large number of parameters and a large number of training points, computing the gradient vector (\ref{g1}) at every iteration of the steepest descent method can be time consuming.
-   It is mainly due to that we have *to sum across all training points*.
-   This becomes prohibitively expensive when we have Big Data.

## Stochastic Gradient {.smaller}

-   A much cheaper alternative is to replace the mean of the individual gradients over all training points

-   by the gradient at a single, randomly chosen, training point.

-   This leads to the simplest form of the *stochastic gradient method*.

-   Choose an integer $i$ uniformly at random from $\{1,...,n\}$ and update \begin{equation}\label{g4}
    \theta_j=\theta_j-\eta\frac{\partial}{\partial\theta_j}J(\theta;x^{(i)})
    \end{equation}

-   Notice we have included $x^{(i)}$ in the notation of $J(\theta;x^{(i)})$ to remark the dependence.

## Rationale for SGD

-At each step, the stochastic gradient method uses one randomly chosen training point to represent the full training set.

-   As the iteration proceeds, the method sees more training points.

-   So *there is some hope* that this dramatic reduction in cost-per-iteration will be worthwhile overall.

-   Note that, even for very small $\eta$, the update (\ref{g4}) is not guaranteed to reduce the overall cost function we have traded the mean for a single sample.

-   Hence, although the phrase stochastic gradient descent is widely used, we prefer to use **stochastic gradient**.

## SGD variants

-   The version of the stochastic gradient method that we introduced in (\ref{g4}) is the simplest from a large range of possibilities.
-   In particular, the index $i$ in (\ref{g4}) was chosen by sampling with replacement after using a training point, it is returned to the training set and is just as likely as any other point to be chosen at the next step.
-   An alternative is to sample without replacement; that is, to cycle through each of the $n$ training points in a random order.
-   Performing $n$ steps in this manner, referred to as completing an epoch, may be summarized as follows:

```{=tex}
\begin{itemize}
\item Shuffle the integers $\{1,...,n\}$ into a new order $\{k_1,...,k_n\}$
\item for $i$ in  $1:n$ update $\theta_j=\theta_j-\eta\frac{\partial}{\partial\theta_j}J(\theta;x^{(k_i)})$
\end{itemize}
```
## SGD variants (3)

-   If we regard the stochastic gradient method as approximating the mean over all training points by a single sample, then
-   It is natural to consider a compromise where we use a small sample average. For some $m<<n$ we could take steps of the following form.

```{=tex}
\begin{itemize}
\item Choose $m$ integers $\{k_1,...,k_m\}$ uniformly at random from $\{1,...,n\}$ 
\item update $\theta_j=\theta_j-\eta\frac{1}{m}\sum_{i=1}^m\frac{\partial}{\partial\theta_j}J(\theta;x^{(k_i)})$
\end{itemize}
```
-   In this iteration, the set $\{x^{(k_i)}\}_{i=1}^m$ is known as a mini-batch.

## Improving SGD

-   Because the stochastic gradient method is usually implemented within the context of a very large scale computation, algorithmic choices such as mini-batch size and the form of randomization are often driven by the requirements of high performance computing architectures.
-   Also, it is, of course, possible to vary these choices, along with others, such as the learning rate, dynamically as the training progresses in an attempt to accelerate convergence.

## Back propagation

-   Back-propagation is the algorithm used to compute the gradients of the network.
-   This procedure was developed by several authors in the decade of the 60's but is Paul J. Werbos, (1974) in his thesis when demonstrates the use of this algorithm for ANN. - Years later, (David, E.

1986) presents the modern way to apply this technique to ANN, and sets the basis of the algorithm in use today.

## The delta rule

-   In this paper, the authors presents a new method capable to change the predictions towards a desired output, they called it the delta rule.

-   This rule consist in compute the total error for the network and check how the error changes when certain elements from the network changes its value.

-   These changes are computed by differentiating the cost function with regard to each element in the network\

-   Which would give us a measure of how much each element is contributing to the total error of the network,

-   This is, computing the gradient of the cost function we can know how the total error changes with regard to each element, and therefore apply the delta rule.

## Applying the chain rule

-   The cost function is an intricate composed function which contains the weights of all layers,
-   the problem now is that the computations of this gradients are not straightforward as in a simple function,
-   A node from a layer is the result of the composition of all the nodes from previous layers.
-   To overcome it, Back-propagation uses the chain rule of differential calculus to compute the gradients of each element in the neural network,

-It contains two main phases referred to as the forward phase and backward phase:

## Back-propagation: **Forward Phase**:

-   The inputs for a data example are fed into the FNN.
-   These inputs will be propagated through the network for all the neurons in all the layers using the current set of weights, to finally compute the final output.

## Back-propagation: **Backward Phase**:

-   Once the final output and the cost function are computed, we need to compute the gradients for all the weights in all the layers in the FNN to update them in order to reach the minimum.
-   To compute the gradients the chain rule is used, is a backwards a process from output to input,
-   Therefore we will start from the output node, compute all the gradients of the previous layer, and so on until we reach the input layer.


# ANN example using Python


## Iris data classification

- The example below has been borrowed from D. Wishart's cours on "Machine Learning" from Bioinformatics Canada courses in Bioinformatics.

- The example trains a simple ANN to classify Iris species using the well-known "iris" dataset.

- The code and data is available in the lab folder as a python notebook `irisANN.ipynb`


## The general workflow {.smaller}

:::: {.columns}

::: {.column width='50%'}

**PREPARE**

- Read data
- Check data
- Create training/testing data sets
- Create a one hot encoding function
- Create a normalization function
- Normalize the data
- Perform label encoding and one hot encoding
- Define activation function


:::

::: {.column width='50%'}

**RUN**

- Initialize weights and biases
- *Determine number of batches*
- *Perform forward propagation*
- *Calculate the errors*
- *Perform back propagation*
- *Update weights and biases*

:::

::::





## Load required libraries {.smaller}


:::: {.columns}

::: {.column width='60%'}


```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/ANN-IrisExample1.png")
```

:::

::: {.column width='40%'}

- `numpy` allows for mathematical operations and array
handling
- `pandas` is used to read data and for providing
dataframe capabilities
- `seaborn` and `matplotlib` are used for data visualization

:::
::::

## Load data

```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/ANN-IrisExample2.png")
```

## Check data {.smaller}

- To make sure that (i) no data is missing and (ii) check the class labels a function `verify_dataset` is defined.


```{r, fig.align='center', out.width="100%", fig.cap=""}
knitr::include_graphics("images/ANN-IrisExample3.png")
```

## Create test/training sets {.smaller}

- The data set of 150 flowers is divided into 2/3 for
training and 1/3 for testing.


```{r, fig.align='center', out.width="100%", fig.cap=""}
knitr::include_graphics("images/ANN-IrisExample4.png")
```

## Transform non numerical data {.smaller}

- ANNs require categorical input data to be binary encoded, while numerical input data can be used directly.

- One-hot encoding recodes *categorical* or *nominal* variables to 1s and 0s.


```{r, fig.align='center', out.width="100%", fig.cap=""}
knitr::include_graphics("images/ANN-IrisExample5a.png")
```

## one-hot encoding function {.smaller}


```{r, fig.align='center', out.width="100%", fig.cap=""}
knitr::include_graphics("images/ANN-IrisExample5b.png")
```

## Data normalization {.smaller}

- The numerical input data (flower measurements) exist in different
ranges
- If this input data is not scaled, the model will assume that features
with higher values are more important when predicting flower
classes, which may not be the case

- A key step in preparing the data is normalization, also known as
feature scaling 

  – Here, L2 normalization will be used so that for each
row in the data set, the sum of squares adds up to one


## A normalization function {.smaller}


```{r, fig.align='center', out.width="100%", fig.cap=""}
knitr::include_graphics("images/ANN-IrisExample6.png")
```


## Separate I/O and normalize {.smaller}

- Input data "x" is extracted from the original
dataset, and normalized by calling the `normalize`
function

```{r, fig.align='center', out.width="100%", fig.cap=""}
knitr::include_graphics("images/ANN-IrisExample7.png")
```

## Encode categorical vars (1) {.smaller}

- Before one hot encoding the output (categorical)
variables, the labels are first replaced by 0,1,2 for
*setosa*, *virginica*, and *versicolor* respectively

- This is known as *label encoding*

```{r, fig.align='center', out.width="100%", fig.cap=""}
knitr::include_graphics("images/ANN-IrisExample8.png")
```

## Encode categorical vars (2) {.smaller}

- Next, the y values (flower species) are extracted from
the iris dataset and then one-hot encoded by calling
the `to_one_hot` function

```{r, fig.align='center', out.width="100%", fig.cap=""}
knitr::include_graphics("images/ANN-IrisExample9.png")
```


## Define Activation function  {.smaller}

- The sigmoid function is used for the activation function, as it
outputs values between 0 and 1

- This is useful because this model predicts probabilities as an output

- Additionally, this function is differentiable which allows for gradient descent optimization.


```{r, fig.align='center', out.width="100%", fig.cap=""}
knitr::include_graphics("images/ANN-IrisExample11.png")
```


## Weights and biases inizialization

- Before training the model, the weights and biases are initialized
as random values between 0 and 1

  - w0 is the set of weights for connections between the input and
hidden layer
  - w1 is the set of weights for connections between the hidden and
output layer

- The biases bh and bo are for the hidden and output layers
respectively

## Initialize weight and biases  {.smaller}

```{r, fig.align='center', out.width="100%", fig.cap=""}
knitr::include_graphics("images/initializeweights.png")
```


# To be continued ...


# References and Resources
