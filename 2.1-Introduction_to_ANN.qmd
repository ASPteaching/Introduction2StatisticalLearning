---
title: "Introduction to Artificial Neural Networks"
authors:
- Esteban Vegas
- Ferran Reverter
- Alex Sanchez
date: "`r Sys.Date()`"
format:
    
    html: 
      toc: true
      toc-depth: 3
      code-fold: show
      fig-width: 8
      fig-height: 6
      embed-resources: true
    pdf:
      toc: true
      toc-depth: 3
      code-fold: false
      fig-width: 8
      fig-height: 6
      geometry: 
      - top=30mm
      - left=30mm
knit:
  quarto:
    chunk_options:
      echo: true
      cache: false
      prompt: false
      tidy: true
      comment: NA
      message: false
      warning: false
    knit_options:
      width: 75
reference-location: margin
execute:
    echo: true
    message: false
    warning: false
    cache: true
bibliography: "DeepLearning.bib"
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

```{r echo=FALSE}
options(width=100) 
if(!require("knitr")) install.packages("knitr")
library("knitr")
#getOption("width")
knitr::opts_chunk$set(comment=NA,echo = TRUE, cache=TRUE)
```

# From Artificial Neurons to Artificial Intelligence

In the post-pandemic world a big phenomenon is impacting society: a lightning rise of Artificial Intelligence with a mess of realities and promises.

Although it has been growing steadily, it has probably been since the appearence of ChatGPT, followed by other LLMs, that everybody has an experience, an opinion, or a fear on the topic.

![](https://bernardmarr.com/wp-content/uploads/2022/04/The-Dangers-Of-Not-Aligning-Artificial-Intelligence-With-Human-Values.jpg "The 5 Biggest Artificial Intelligence (AI) Trends In 2023, Bernard Marr"){fig-align="center"}

Most tasks performed by AI can be described as Classification or  Prediction used in a wide range of applications ranging from from recommender systems, natural language processing, image recognition or image generation.

AI engines use statistical learning methods, such as machine learning
algorithms, to make predictions based on large amounts of data.

However, it is important to keep in mind that AI has far-reaching
implications beyond its predictive capabilities, including ethical,
social, and technological considerations that must be taken into account when developing and deploying AI systems.

## The rise of Deep learning

Today, when technically trained people think about AI, one of the first concepts that comes into mind is Deep Learning.

Deep learning is a highly successful model in the field of AI, which has powered numerous applications in various domains. It has shown
remarkable performance in tasks such as image recognition, natural
language processing, and speech recognition.

Deep Learning, indeed, has been developed in the field of ARtificial Neural Networks (ANNs), but it extends the basic principles of ANNs
by introducing more complex architectures and algorithms and, at the
same time, by enabling machines to learn from large datasets by
automatically identifying relevant patterns and features without
explicit programming.

One key advantage of deep learning over traditional machine learning
algorithms is its ability to handle high-dimensional and unstructured
data such as images, videos, and audio.

## The early history of artificial \[neural networks\]/intelligence

![A Brief History of AI from 1940s till Today](images/AIHistory1.jpg)

[![The origins of Deep learning and modern Artificial Intelligence can
be traced back to the per4ceptron. Source: "A Quick History of AI, ML
and
DL"](images/AIHistory2.jpg)](https://nerdyelectronics.com/a-quick-history-of-ai-ml-and-dl/)

The origins of AI, and as such of DL can be traced almost one century
backwards. While it is an interesting, or even fascinating, history wee
don't go into it (see a summary in [A Quick History of AI, ML and
DL](https://nerdyelectronics.com/a-quick-history-of-ai-ml-and-dl/){#AIHistory}

We can see there however, several hints worth to account for because we will go through them to understand how a deep neural network works. These are:

-   The **Perceptron** and the first **Artificial Neural Network** where  the basic building block was introduced.

-   The **Multilayered perceptron** and back-propagation where complex     architectures were suggested to improve the capabilities.

-   **Deep Neural Networks**, with many hidden layers, and
    auto-tunability capabilities.

In short, there has been an mathematical and a technological evolution
that at some point has allowed to meet with

-   The required theoretical background (DNN)

-   The required computational capabilities (GPU, HPC)

-   The required quantity of data (Big Data, Images, Social Networks)

This has resulted in making artificial intelligence widely accessible to
businesses, researchers, and the general public.

![Why Deep Learning Now?](images/WhyDLNow.png){fig-align="center"
width="100%"} Source: [Alex Amini's 'MIT Introduction to Deep Learning'
course](introtodeeplearning.com)

Success stories such as

-   the development of self-driving cars,

-   the use of AI in medical diagnosis, and

-   the creation of personalized recommendations in online shopping

have also contributed to the widespread adoption of AI.


AI comes, however, also with fears from multiple sources from science fiction to religion

- Mass unemployment
  
- Loss of privacity
  
- AI bias
  
- AI fakes
  
- Or, simply, AI takeover
  
  ![](2.1-Introduction_to_ANN-Slides_insertimage_2.png)


## Comparison with Traditional Machine Learning

A reasonable question is: "*How are Artificial Intelligence, Machine
Learning and Deep learning related*"?

A standard answer can be found in the image below that has a myriad
variations:

```{r, fig.align='center', out.width="100%", echo=FALSE}
knitr::include_graphics("images/AI-ML-DL-1.jpg")
```

We can keep, for instance, the followin three definitions, which also
have many variants:

-   Artificial intelligence is the ability of a computer to perform
    tasks commonly associated with intelligent beings.

-   Machine learning is the study of algorithms that learn from examples
    and experience instead of relying on hard-coded rules and make
    predictions on new data

-   Deep learning is a sub-field of machine learning focusing on
    learning data representations as successive successive layers of
    increasingly meaningful representations.

```{r, fig.align='center', out.width="100%", echo=FALSE}
knitr::include_graphics("images/ML_vs_DL-2.png")
```

We will be coming back to the difference between ML and DL, but we can at least highlight two strengths of DL that differentiate it from ML:

- DNN combine feature extraction and classification in a way that does
    not require (or dramatically decreases) human intervention.
- The power of DNN requires in its ability to improve with data
  availability, without seemingly reaching plateaus as ML.

![An illustration of the performance comparison between deep learning
(DL) and other machine learning (ML) algorithms, where DL modeling from
large amounts of data can increase the
performance](images/PerformanceVsAmountOfData.png){fig-align="center"}

**Deep learning is having a strong impact** in many day-to-day activities, which explains the growing interest in the topic at all levels, not only technical or scientific one.

-   Near-human-level image classification

-   Near-human-level speech transcription

-   Near-human-level handwriting transcription

-   Dramatically improved machine translation

-   Dramatically improved text-to-speech conversion

-   Digital assistants such as Google Assistant and Amazon Alexa

-   Near-human-level autonomous driving

-   Improved ad targeting, as used by Google, Baidu, or Bing

-   Improved search results on the web

-   Ability to answer natural language questions

-   Superhuman Go playing

According to [@Allaire2022] ... "*we shouldn't believe the short-term hype, but should believe in the long-term vision. It may take a while for AI to be deployed to its true potential---a potential the full
extent of which no one has yet dared to dream---but AI is coming, and it
will transform our world in a fantastic way*".

Once the introduction is ready we con move onto the building blocks of
neural networks, perceptrons.

# The Artificial Neuron (AN)

## Mc Cullogh's neuron and the perceptron


The first computational model of a neuron was proposed by Warren McCullough (neuroscientist) and Walter Pitts (logician) in 1943.

THe ide was to build as a mathematical model that might emulate a neuron, in the sense that *given some inputs, and an  appropriate set of examples, learn to produce the desired output*?

[![](images/MacCulloghPitts-Neuron.png){fig-align="center"}](https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1)

It may be divided into 2 parts. The first part, $g$,takes an input (ahem
dendrite ahem), performs an aggregation and based on the aggregated
value the second part, $f$, makes a decision. See [the source of this
picture](https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1)
for an illustration on how this can be used to emulate logical
operations such as AND, OR or NOT, but not XOR.

This first attempt to emulate neurons succeeded but with limitations:

-   What about non-Boolean (say, real) inputs?

-   What if all inputs are not equal?

-   What if we want to assign more importance to some inputs?

-   What about functions which are not linearly separable? Say XOR
    function

To overcome these limitations Frank Rosenblatt, an American
psychologist, proposed the classical **perceptron model**, the *artificial neuron*, in 1958. 

It is a more generalized computational model than the
McCullough-Pitts neuron where weights and thresholds can be learnt over time.

The perceptron proposed by Rosenblatt this is very similar to an M-P neuron but it takes a weighted sum of the inputs and sets the output to one iff the sum is more than an arbitrary threshold (**$\theta$**).

[![](images/RosenblattPerceptron1.png)](https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d)

Additionally, instead of hand coding the thresholding parameter
$\theta$, we add it as one of the inputs, with the weight $w_0=-\theta$
like shown below, which makes it learnable.

[![](images/RosenblattPerceptron2.png)](https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d)

[![](images/McCullaughVSRosenblattPerceptron.png)](https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d)

Now, while this is an improvement (because both the weights and the
threshold can be learned and the inputs can be real values) there is
still a drawback in that a single perceptron can only be used to
implement linearly separable functions.

Artificial Neural Networks improve on this by introducing *Activation
Functions* which, eventually, can be non-linear.

## Neurons and Activation Functions

How come biological and artificial neurons come to compare?

- Biological neurons are specialized cells in the central nervous system
that transmit electrical and chemical signals to communicate with each
other and the rest of the body.

- On the other hand, artificial neurons are mathematical models used in
neural networks to process information.

Neurons do not need to respond to all stimuli. When the stimulus is big enough to produce a response we say that the neurom has been activated. By extension one can consider that activation is controlled (or decided) by an *activation function*.


In both biological and artificial neurons, the **activation function**
is what is responsible for *deciding whether the neuron activates or not  based on the input it receives*.

-   In the case of a biological neuron, the activation function is based
    on the release of neurotransmitters, which are chemical substances
    that transmit signals between nerve cells. When the electrical
    signal reaching the neuron exceeds a certain threshold, the neuron
    releases neurotransmitters, which are received by other neurons or
    cells in the body to continue the communication process.
-   On the other hand, in an artificial neuron, the *activation function*
    is a mathematical function applied to the neuron's input to produce
    an output. Like in the biological neuron, this activation function
    decides whether the neuron activates or not based on the input it
    receives.
    
*An activation function is a function that is added into an artificial neuron in order to help it learn complex patterns in the data.*

```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/ActivationFunction0.png")
```

[Read more here about activation
functions.](https://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253)

With all these inputs in mind we can now define an Artificial Neuron as
a *computational unit* that 

- takes as input $x=(x_0,x_1,x_2,x_3)$ ($x_0$ = +1, called bias), and 

- outputs $h_{\theta}(x) = f(\theta^\intercal x) = f(\sum_i \theta_ix_i)$, 
- where  $f:\mathbb{R}\mapsto \mathbb{R}$ is called the **activation function**.

The goal of the activation function is to provide the Neuron with *the
capability of producing the required outputs*.

For instance, if the output has to be a probability, the activation
function will only produce values between 0 and 1.

With this idea in mind activation functions are chosen from a set of
pre-defined functions:

-   the sigmoid function:

$$
f(z)=\frac{1}{1+e^{-z}}
$$

-   the hyperbolic tangent, or `tanh`, function:

$$
f(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}
$$

The `tanh(z)` function is a re-scaled version of the sigmoid, and its
output range is $[-1,1]$ instead of $[0,1]$.

Two useful properties to recall are that: 

- *If* $f(z)=1/(1+e^z)$ is the
sigmoid function, then its derivative is given by $f'(z)=f(z)(1-f(z))$.

```{r, fig.align='center', out.width="100%", echo=FALSE}
knitr::include_graphics("images/sigmoidFunctionDerivative.png")
```

-   *Similarly, if* $f$ is the `tanh` function, then its derivative is
    given by $f'(z)=1-(f(z))^2$.

In modern neural networks, the default recommendation is to use the
    *rectified linear unit* or ReLU defined by the activation function
    $f(z)=\max\{0,z\}$.

- This function remains very close to a linear one, in the sense that is a piece-wise linear function with two linear pieces.

- Because rectified linear units are nearly linear, they preserve many of
the properties that make linear models easy to optimize with gradient
based methods.

- They also preserve many of the properties that make linear models
generalize well.

[![](images/ActivationFunctions.png)](https://medium.com/@shrutijadon/survey-on-activation-functions-for-deep-learning-9689331ba092).

**Putting altogether** we have the following schematic representation of
an artificial neuron where
$\Sigma=\left\langle w_{j}, x\right\rangle+ b_{j}$ and
$\left\langle w_{j}, x\right\rangle$ represents the dot product between
vectors $w$ and $x$.

![](images/ArtificialNeuron.png)

In summary:

- An artificial neuron takes a vector of input values $x_{1}, \ldots, x_{d}$ and combines it with some weights that are local to the neuron $\left(w_{0}, w_{1}, . ., w_{d}\right)$ to compute a net input $w_{0}+\sum_{i=1}^{d} w_{i} * x_{i}$. 

- To compute its output, it then passes the net input through a possibly non-linear univariate activation function $g(\cdot)$, usually vchosen from a set of options such as *Sigmoid*, *Tanh* or *ReLU* functions

- Given there is a special weight $w_{0}$, the *bias*, in order to treat all weights equally, we use the trick of creating an extra input variable $x_{0}$ with value always equal to 1 , and so the function computed by a single artificial neuron (parameterized by its weights $\mathbf{w}$ ) is:

$$
y(\mathbf{x})=g\left(w_{0}+\sum_{i=1}^{d} w_{i} x_{i}\right)=g\left(\sum_{i=0}^{d} w_{i} x_{i}\right)=g\left(\mathbf{w}^{\mathbf{T}} \mathbf{x}\right)
$$

- Single artificial neurons are used to model functions $\mathbb{R}^{d} \mapsto \mathbb{R}$. 
  - The weights $w_{0}, w_{1}, . ., w_{d}$ are the parameters of the model. Different values will compute different functions. 

  - As usual, **the learning task is to find suitable weights that allow neurons to model the data as accurately as possible**.

# From neurons to neural networks

## The basic neural networks

Following with the brain analogy one can combine (artificial) neurons to create better learners.

A simple artificial neural network is usually created by combining two types of modifications to the basic perceptron (AN).

- Stacking several neurons insteads of just one.
- Adding an additional layer of neurons, which is call a *hidden* layer, 

This yields a system where the output of a \emph{neuron} can be the input
of another in many different ways.

The image below shows a small neural network with three neurons stacked in a *hidden* layer.


![](images/nn.jpg){width="70%" align='center'}


In this figure, we have used circles to also denote the inputs to the
network. 

- The circles labeled +1 are called bias units, and correspond to the intercept term. 

- The leftmost layer of the network is called the *input layer*.

- The rightmost layer of the network is called the *output* layer.

- The middle layer of nodes is called the *hidden layer*, because its values are not observed in the training set.


Bias nodes are not counted when stating the neuron size. 

With all this in mind our example neural network has three layers with:

 - 3 input units (not counting the bias unit), 
 - 3 hidden units, and 
 - 1 output unit.



<!-- ## Multilayer perceptrons -->

<!-- A multilayer perceptron (or Artificial neural network) is a structure -->
<!-- composed by *several hidden layers of neurons* where the output of a -->
<!-- neuron of a layer becomes the input of a neuron of the next layer. -->

<!-- Moreover, the output of a neuron can also be the input of a neuron of -->
<!-- the same layer or of neuron of previous layers (this is the case for -->
<!-- recurrent neural networks). On last layer, called output layer, we may -->
<!-- apply a different activation function as for the hidden layers depending -->
<!-- on the type of problems we have at hand : regression or classification. -->

<!-- The Figure below represents a neural network with three input variables, -->
<!-- one output variable, and two hidden layers. -->

<!-- ```{r, fig.align='center', out.width="100%"} -->
<!-- knitr::include_graphics("images/MultiLayer1.png") -->
<!-- ``` -->

<!-- Multilayers perceptrons have a basic architecture since each unit (or -->
<!-- neuron) of a layer is linked to all the units of the next layer but has -->
<!-- no link with the neurons of the same layer. -->

<!-- The parameters of the architecture are: -->

<!-- -   the number of hidden layers and -->
<!-- -   the number of neurons in each layer. -->

<!-- The activation functions are also to choose by the user. For the output -->
<!-- layer, as mentioned previously, the activation function is generally -->
<!-- different from the one used on the hidden layers. For example:. -->

<!-- -   For regression, we apply no activation function on the output layer. -->
<!-- -   For binary classification, the output gives a prediction of -->
<!--     $\mathbb{P}(Y=1 / X)$ since this value is in $[0,1]$ and the sigmoid -->
<!--     activation function is generally considered. -->
<!-- -   For multi-class classification, the output layer contains one neuron -->
<!--     per class (i), giving a prediction of $\mathbb{P}(Y=i / X)$. The sum -->
<!--     of all these values has to be equal to 1. The sum of all these -->
<!--     values has to be equal to 1. -->
<!--     -   A common choice for multi-class ANN is the soft-max activation -->
<!--         function: $$ -->
<!--         \operatorname{softmax}(z)_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j} \exp \left(z_{j}\right)} -->
<!--         $$ -->



An ANN is a predictive model (a *learner*) whose properties and behaviour  can be well characterized.

<!-- In practice this means: -->

- The ANN operates through a process known as *forward propagation*, which encompasses the entire journey of information flow from the input layer to the output layer.

- Forward propagation is performed by composing a series of linear and non-linear (activation) functions.

- These are characterized (parametrized) by their *weights* and *biases*, that need to be *learnt*. This is done by *training the ANN*.

- In order for the ANN to perform well, the training process aims at finding the best possible  parameter values for the learning task defined by the fnctions. This is done by

  - Selecting an appropriate (convex) loss function,
  - Finding those weights that minimize a the total *cost* function (avg. loss).

- This is usually done using some iterative optimization procedure such as *gradient descent*.
  - This requires evaluating derivatives in a huge number of points.
  - Such high number may be reduced by *Stochastic Gradient Descent*.
  - The evaluation of derivatives is simplified thanks to *Backpropagation*.


### A logistic regression ANN

The simple ANN shown above can be casted to perform a logistic regression:

-   The transformation of the inputs through the hidden layer yields new set of *complex features*.

-   Such complex features may be transformed using a sigmoid activation function to produce the output, which takes the form:
$$
\mbox{The output is: }h_{\theta}(x)=\frac{1}{1+e^{-\theta^\intercal x}}
$$

Now, recall that, the logistic regression model is:

$$
\log\frac{p(Y=1|x)}{1-p(Y=1|x)}=\theta^\intercal x
$$

By taking logs in both sides and then isolating $p(Y=1|x)$, we have:

\begin{eqnarray}
\frac{p(Y=1|x)}{1-p(Y=1|x)}&=&e^{\theta^\intercal x}\\
p(Y=1|x)&=& e^{\theta^\intercal x}\cdot(1-p(Y=1|x))\\
p(Y=1|x)[1+e^{\theta^\intercal x}]&=& e^{\theta^\intercal x}\\
p(Y=1|x)&=&\frac{e^{\theta^\intercal x}}{1+e^{\theta^\intercal x}}=\frac{1}{1+e^{-\theta^\intercal x}}
\end{eqnarray}

That is, *when the activation function of the output node is the sigmoid activation function, the output coincides with a logistic regression on complex features*

-   And, $h_{\theta}(x)$, the output of the ANN, estimates  $p(Y=1|x)$.

## The parameters of an ANN

-   Let $n_l$ denote the number of layers in our network, thus $n_l=3$
    in our example.

-   Label layer $l$ as $L_l$, so layer $L_1$ is the input layer, and
    layer $L_{n_l}=L_3$ the output layer.

-   Our neural network has parameters:
    $\Theta=(\Theta^{(1)},\Theta^{(2)})$, where we will write
    $\theta^{(l)}_{ij}$ to denote the parameter (or weight) associated
    with the connection between unit $j$ in layer $l$, and unit $i$ in
    layer $l+1$.

-   Thus, in our example, we have:

    -   $\Theta^{(1)}\in\mathbb{R}^{3\times 4}$, and
    -   $\Theta^{(2)}\in\mathbb{R}^{1\times 4}$.

Note that bias units don't have inputs or connections going into them,
since they always output the value +1.

We also let $s_l$ denote the number of nodes in layer $l$ (not counting
the bias unit).

Now, write $a^{(l)}_i$ to denote the activation (meaning output value)
of unit $i$ in layer $l$.

For $l=1$, we also use $a^{(1)}_i=x_i$ to denote the $i$-th input.

Given a fixed setting of the parameters $\Theta$, our neural network
defines a model $h_{\Theta}(x)$ that outputs a real number.

## Forward propagation

As described above the process that encompasses the computations required to go from the input values to the final output is known as *forward propagation*.

To understand how it works let's check how the  weights are combined with the input to produce the final output.

Each node, $a_i^{(2)}$ of the hidden layer opperates on all nodes of the input values as follows:

\begin{eqnarray}
a_1^{(2)}&=&f(\theta_{10}^{(1)}+\theta_{11}^{(1)}x_1+\theta_{12}^{(1)}x_2+\theta_{13}^{(1)}x_3)\\
a_2^{(2)}&=&f(\theta_{20}^{(1)}+\theta_{21}^{(1)}x_1+\theta_{22}^{(1)}x_2+\theta_{23}^{(1)}x_3)\\
a_3^{(2)}&=&f(\theta_{30}^{(1)}+\theta_{31}^{(1)}x_1+\theta_{32}^{(1)}x_2+\theta_{33}^{(1)}x_3))
\end{eqnarray}

The output of the hidden layer is transformed through the activation function:

$$
h_{\Theta}(x)=a_1^{(3)}=f(\theta_{10}^{(2)}+\theta_{11}^{(2)}a_1^{(2)}+\theta_{12}^{(2)}a_2^{(2)}+\theta_{13}^{(2)}a_3^{(2)}
$$

### A more compact notation

If we let  $z_i^{(l)}$ denote the total weighted sum of inputs to unit $i$ in layer $l$, including the bias term:

$$
z_i^{(2)}=\theta_{i0}^{(1)}+\theta_{i1}^{(1)}x_1+\theta_{i2}^{(1)}x_2+\theta_{i3}^{(1)}x_3,
$$ 
the output becomes: $a_i^{(l)}=f(z_i^{(l)})$.


Extending the activation function $f(\cdot)$ to apply to vectors in   an element-wise fashion: 

$$
    f([z_1,z_2,z_3]) = [f(z_1), f(z_2),f(z_3)],
$$
we can write the previous equations more compactly as:

```{=tex}
\begin{eqnarray}
z^{(2)}&=&\Theta^{(1)}x\nonumber\\
a^{(2)}&=&f(z^{(2)})\nonumber\\
z^{(3)}&=&\Theta^{(2)}a^{(2)}\nonumber\\
h_{\Theta}(x)&=&a^{(3)}=f(z^{(3)})\nonumber
\end{eqnarray}
```

More generally, recalling that we also use $a^{(1)}=x$ to also  denote the values from the input layer,

Given layer $l$'s activations $a^{(l)}$, we can compute layer   $l+1$'s activations $a^{(l+1)}$ as:


\begin{equation}
z^{(l+1)}=\Theta^{(l)}a^{(l)}
\label{eqforZs}
\end{equation}

\begin{equation}
a^{(l+1)}=f(z^{(l+1)})
\label{eqforAs}
\end{equation}


This can be used to provide a matrix representation for the weighted sum
of inputs of all neurons:

$$
z^{(l+1)}=
\begin{bmatrix}
z_1^{(l+1)}\\
z_2^{(l+1)}\\
\vdots\\
z_{s_{l+1}}^{(l)}
\end{bmatrix}=
\begin{bmatrix}
\theta_{10}^{(l)}& \theta_{11}^{(l)}&\theta_{12}^{(l)}&...&\theta_{1s_{l}}^{(l)}&\\
\theta_{20}^{(l)}& \theta_{21}^{(l)}&\theta_{22}^{(l)}&...&\theta_{2s_{l}}^{(l)}&\\
\vdots & \vdots& \vdots & \vdots & \vdots\\
\theta_{s_{l+1}0}^{(l)}& \theta_{s_{l+1}1}^{(l)}&\theta_{s_{l+1}2}^{(l)}&...&\theta_{s_{l+1}s_{l}}^{(l)}&\\
\end{bmatrix}
\cdot\begin{bmatrix}
1\\
a_1^{(l)}\\
a_2^{(l)}\\
\vdots\\
a_{s_l}^{(l)}
\end{bmatrix}
$$

So that, the activation is then:

$$
a^{(l+1)}=
\begin{bmatrix}
a_1^{(l+1)}\\
a_2^{(l+1)}\\
\vdots\\
a_{s_{l+1}}^{(l)}
\end{bmatrix}=f(z^{(l+1)})=\begin{bmatrix}
f(z_1^{(l+1)})\\
f(z_2^{(l+1)})\\
\vdots\\
f(z_{s_{l+1}}^{(l)})
\end{bmatrix}
$$


## Multiple architectures for ANN

- We have so far focused on a single hidden layer neural network of
  the example.
    
- Indeed, One can build neural networks with many distinct
  architectures (meaning patterns of connectivity between neurons),
  including ones with multiple hidden layers.

-   See here [The Neural Network
    Zoo![](2.1-Introduction_to_ANN-Slides_insertimage_3.png)](https://www.asimovinstitute.org/neural-network-zoo/).

### Feed Forward Neural networks

The most common choice is a $n_l$-layered network where layer 1 is the
input layer, layer $n_l$ is the output layer, and each layer $l$ is
densely connected to layer $l+1$.

In this setting, to compute the output of the network, we can
successively compute all the activations in layer $L_2$, then layer
$L_3$, and so on, up to layer $L_{nl}$ , using the equations above. This is one example of a feed-forward *neural network (FFNN)*, since the connectivity graph does not have any directed loops or cycles.

### The number of output units

Neural networks can also have multiple output units.

For example, in the image below we can see a network with two hidden layers layers $L_2$ and $L_3$ and four output units in layer $L_4$, where bias of each layer were omitted.

![Neural network](images/nn2.jpg){width="80%"}

To train this network, we would need training examples $(x^{(i)},y^{(i)})$ where $y^{(i)}\in\mathbb{R}^4$. 

This type of network is useful if there are multiple outputs that you're interested in predicting.

- For example, in a medical diagnosis application, the vector $x$ might give the input features of a patient, and the different outputs $y_i$'s might indicate presence or absence of different diseases.

# Training Neural Networks

- An ANN is a predictive model whose properties and behaviour  can be mathematically characterized.

- In practice this means:
  - The ANN acts by composing a series of linear and non-linear (activation) functions.
  - These are characterized by their *weights* and *biases*, that need to be *learnt*.
- *Training* the network is done by 
  - Selecting an appropriate (convex) loss function,
  - Finding those weights that minimize a the total *cost* function (avg loss).

## The tools for training

- Training an ANN is usually done using some iterative optimization procedure such as *Gradient Descent*.

- This requires evaluating derivatives in a huge number of points.
    - Such high number may be reduced by *Stochastic Gradient Descent*.
    - The evaluation of derivatives is simplified thanks to *Backpropagation*.
    
- It is important to realize that backpropagation is both
  
  - The algorithm for training, i.e. for tuning the weights in order to minimize the loss function
  - A way to train efficientlt thanks to a series of tricks and techniques such as automatic differentiation.
  

## A loss function for optimization

A first idea may be to use *squared error loss*:

$$
  l(h_\theta(x),y)=\left (y-\frac{1}{1+e^{-\theta^\intercal x}}\right )^2
$$

However it happens to be that, given a sigmoid activation function, the resulting loss function [* is is not a convex   problem*](https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c)  which means that MSE is not appropriate.

Quadratic loss may be used, for instance, with ReLu activation.

To illustrate this, we can consider a simple neural network with one input, one hidden layer with a logistic activation function, and one output neuron. 

We aim to minimize the squared error loss function with respect to the weights of the network. Here's how we can visualize the loss surface using R:

```{r}
library(plot3D)

# Define the squared error loss function
squared_error <- function(y, y_hat) {
  return(0.5 * (y - y_hat)^2)
}

# Define the logistic activation function
logistic <- function(z) {
  return(1 / (1 + exp(-z)))
}

# Generate data
x <- seq(-10, 10, length.out = 200)
y <- seq(-10, 10, length.out = 200)
z <- outer(x, y, FUN = function(x, y) squared_error(1, logistic(x + y)))

# Scale the z values to exaggerate the differences
z <- z * 500

# Plot the exaggerated loss surface
persp3D(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "lightblue", ticktype = "detailed", xlab = "Weight 1", ylab = "Weight 2", zlab = "Loss")

```
  
This plot illustrates the loss surface of a neural network with two weights to optimize. As can be seen, the surface is not convex, indicating the presence of multiple local minima.

An alternative to squared error loss is the **binary cross-entropy loss function** : 

$$
    l(h_\theta(x),y)=\big{\{}\begin{array}{ll}
    -\log h_\theta(x) & \textrm{if }y=1\\
    -\log(1-h_\theta(x))& \textrm{if }y=0
    \end{array}
$$

This function can also be written as:

$$
l(h_\theta(x),y)=-y\log h_\theta(x) - (1-y)\log(1-h_\theta(x))
$$

Using cross-entropy loss, the cost function is of the form:

\begin{eqnarray*}
    J(\theta)=-\frac{1}{n}\big[\sum_{i=1}^n&&(y^{(i)}\log h_\theta(x^{(i)})+\\ &&(1-y^{(i)})\log(1-h_\theta(x^{(i)}))\big]
\end{eqnarray*}

Now, this is a convex optimization problem.

In practice we often work with a *regularized version* of the cost function (we don't regularize the bias units)

```{=tex}
\begin{eqnarray*}
J(\Theta)&=&-\frac{1}{n}\big[\sum_{i=1}^n \sum_{k=1}^K y_k^{(i)}\log( h_\theta(x^{(i)}))_k\\
&+&(1-y_k^{(i)})\log(1-(h_\theta(x^{(i)}))_k)\big]\\
&+&\lambda\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}
(\theta_{ji}^{(l)})^2
\end{eqnarray*}
```

## Parameter optimizations with *Gradient Descent*

We saw in the previous section that training a network corresponds to choosing the parameters, that is, the weights and biases, that minimize
the cost function. 

The weights and biases take the form of
matrices and vectors, but at this stage it is convenient to imagine them
stored as a single vector that we call $\theta$. 
Generally, we will suppose $\theta\in\mathbb{R}^p$, and write the cost function as $J(\theta)$ to emphasize its dependence on the parameters. So Cost
$J: \mathbb{R}^p\rightarrow \mathbb{R}$.

![Error hyper-surface](images/errorsurface.jpg){width="60%"}

*Gradient Descent* is a classical method in optimization that is often named by one of its variants
referred to as *steepest descent*.

Given a function $J(\theta)$ to be minimized, the method proceeds iteratively, computing a sequence of vectors $\theta^1, \theta^2, ..., \theta^n$ in $\mathbb{R}^p$ with the aim of converging to a vector that minimizes the cost function. 

Suppose that our current vector is $\theta$. 
*How should we choose a perturbation, $\Delta\theta$, so that the next vector, $\theta+\Delta\theta$, represents an improvement, that is: $J(\theta +\Delta\theta) < J(\theta)$?*

As is often the case, we will proceed by linearization of the cost function using a Taylor series expansion.

If $\Delta\theta$ is small, then ignoring terms of order $||\Delta\theta||^2$ or higher, a Taylor
series expansion gives:

$$
J(\theta+\Delta\theta)\approx J(\theta)+\sum_{i=1}^p\frac{\partial J(\theta)}{\partial\theta_i}\Delta\theta_i
$$ 
where ${\displaystyle \frac{\partial J(\theta)}{\partial\theta_i}}$ denotes the
partial derivative of the cost function with respect to the $i$-th weight. 

For convenience, we will let $\nabla J(\theta)\in\mathbb{R}^p$ denote the vector of partial derivatives, known as the gradient, so that

\begin{equation}\label{g1}
\nabla J(\theta)=\left(\frac{\partial J(\theta)}{\partial\theta_1},...,\frac{\partial J(\theta)}{\partial\theta_p}\right)^\intercal
\end{equation} 

Now the approximation can be written as:

\begin{equation}\label{g2}
J(\theta+\Delta\theta)\approx J(\theta)+\nabla J(\theta)^\intercal\Delta\theta
\end{equation}

Recalling that our aim is to reduce the value of the cost function, the Taylor approximation above motivates the idea of choosing $\Delta\theta$ to *make $\nabla J(\theta)^\intercal\Delta\theta$ negative*, because this will make the value of $J(\theta+\Delta\theta)$ smaller. 

The bigger in absolute value we can make this negative expression, the smaller will be the value of the cost function.

Now, how much is it possible to make  $\nabla J(\theta)^\intercal\Delta\theta$ decrease?

The answer to this question can be motivated using the [Cauchy-Schwarz inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality).

### Interlude: Cauchy-Schwarz

The Cauchy-Schwarz inequality, which states that for any $f,g\in\mathbb{R}^p$, we have:
$$
|f^\intercal g|\leq ||f||\cdot ||g||.
$$ 
Moreover, the two sides are equal if and only if $f$ and $g$ are linearly dependent (meaning they
are parallel).

- How much can $\nabla J(\theta)^\intercal\Delta\theta$ decrease?

- By Cauchy-Schwarz,biggest possible value for $\nabla J(\theta)^\intercal\Delta\theta$ is the upper bound, $||\nabla J(\theta)||\cdot ||\Delta\theta||$.
  - The equality is only reached when $||\nabla J(\theta)||= ||\Delta\theta||$
  - The highest possible negative value will  come out when $-\nabla J(\theta)=\Delta\theta$

- That is, *we should choose $\Delta\theta$ to lie in the direction of $-\nabla J(\theta)$*. 

### Going back to gradient

Keeping in mind that the Taylor linearization of $J(\theta)$ is an approximation that is relevant only for small $\Delta\theta$, we
will limit ourselves to a small step in that direction. This leads to the update:

\begin{equation}\label{g3}
\theta \rightarrow \theta-\eta\nabla J(\theta)
\end{equation}

\underline{This equation defines the steepest descent method}. 

Here $\eta$ is small step size that, in this context, is known as the *learning rate*. 

In summary, givent a cost function $J(\theta)$ to be optimized the gradient descent optimization proceeds as follows:


1. **Initialize** $\theta_0$ randomly or with some predetermined values
2. **Repeat until convergence:**
    $$
    \theta_{t+1} = \theta_{t} - \alpha \nabla J(\theta_{t})
    $$
3. **Stop when:** $|J(\theta_{t+1}) - J(\theta_{t})| < \epsilon$


Where:

- $\theta_0$ is the initial parameter vector,
- $\theta_t$ is the parameter vector at iteration $t$,
- $\alpha$ is the learning rate,
- $\nabla J(\theta_{t})$ is the gradient of the loss function with respect to $\theta$ at iteration $t$,
- $\epsilon$ is a small positive value indicating the desired level of convergence.


## The backpropagation algorithm

The gradient method provides a way to optimize weights and biases $(\theta =\{W, b\})$ by minimizing the cost function, $J(\theta)$.

This minimization requires, of course, the computation of an important number of partial derivatives
$$
\frac{\partial}{\partial\theta_j}J(\theta)
$$

The algorithm used to perform these computation is known as the *backpropagation algorithm*

The backpropagation algorithm was originally introduced in the 1970s in a MSc thesis.

In 1986 a [paper by Rumelhart, Hinton, and Williams](https://www.nature.com/articles/323533a0) describes several neural networks where backpropagation works far faster than earlier approaches to learning.

This improvement in efficiency made it possible to use neural nets to solve problems which had previously been insoluble. 

### Backpropagation intuition

The name "backpropagation" originates in the expression "Error backpropagation, which does not seem to have anything to do with derivatives.

To grasp an intuition of what we aim at it's good to recall that what the ANN does is a *Foreard calculation* also called *forward propagation*: 

- It takes the input values
- Successively appliest a series of linear (multiplies by weights and adds biases) and non-linear (activation function) transformations, through the networks layers until the predicted value is produced
- The predicted value, most likely will be different from the known output value in the train set.

::: {.r-stack}

![](images/img01.gif)

![](images/img02.gif)

![](images/img03.gif)

![](images/img04.gif)

![](images/img05.gif)

![](images/img06.gif)

![](images/img07.gif)

![](images/img08.gif)
:::

### The delta rule

- How can we use the error to update the weights using the gradient method?

- The idea behind backpropagation is to use the difference between the *real output* and the predicted output ("$\delta$") to update the weights in such a way that in the next iteration the error decrease.

  - This difference is computed first for the output layer
  - And then is backpropagated to the previous layers until all the weights are recomputed and the process can re-start.


::: {.r-stack}

![](images/img09.gif)

![](images/img10.gif)

![](images/img11.gif)

![](images/img12.gif)

![](images/img13.gif)

![](images/img14.gif)

![](images/img15.gif)

![](images/img16.gif)

![](images/img17.gif)

![](images/img18.gif)

![](images/img19.gif)

:::



## Lafunción delta y la retropropagación del error

La función que nos permite modificar el vector de pesos $w$ a partir de una instancia de entrenamiento $d \in D$ se conoce como la regla delta y se expresa de la siguiente forma:

$$
\begin{equation*}
\Delta w=\eta(c-y) d \tag{3.21}
\end{equation*}
$$

donde $c$ es el valor que indica la clase en el ejemplo de entrenamiento, $y$ el valor de la función de salida para la instancia $d$, y $\eta$ la tasa o velocidad de aprendizaje (learning rate).

El método utilizado para entrenar este tipo de redes, y basado en la regla delta, se muestra en el algoritmo 1.

Es importante destacar que para que el algoritmo converja es necesario que las clases de los datos sean linealmente separables.

**Algoritmo 1 Seudocódigo del método del descenso del gradiente**

- **Entrada**: $W$ (conjunto de vectores de pesos) y $D$ (conjunto de instancias de entrenamiento) 
  - mientras $\left(y \not \approx c_{i} \forall\left(d_{i}, c_{i}\right) \in D\right)$ hacer 
    - para todo $\left(\left(d_{i}, c_{i}\right) \in D\right)$ hacer

      - Calcular la salida $y$ de la red cuando la entrada es $d_{i}$

      - si $\left(y \not \approx c_{i}\right)$ entonces
        - Modificar el vector de pesos $w^{\prime}=w+\eta(c-y) d_{i}$

     - fin si

    - fin para

  - fin mientras

**Devolver**: El conjunto de vectores de pesos $W$


### Retropropagación del error

En las redes multicapa no podemos aplicar el algoritmo de entrenamiento visto en la sección anterior. 

El problema aparece con los nodos de las capas ocultas: no podemos saber a priori cuáles son los valores de salida correctos.

En el caso de una neurona $j$ con función sigmoide, la regla delta es:

$$
\begin{equation*}
\Delta w_{i}^{j}=\eta \sigma^{\prime}\left(z^{j}\right)\left(c^{j}-y^{j}\right) x_{i}^{j}, \tag{3.24}
\end{equation*}
$$

donde:

- $\sigma^{\prime}\left(z^{j}\right)$ indica la pendiente (derivada) de la función sigmoide, que representa el factor con que el nodo $j$ puede afectar al error. 
  - Si el valor de $\sigma^{\prime}\left(z^{j}\right)$ es pequeño, nos encontramos en los extremos de la función, donde los cambios no afectan demasiado a la salida. 
  - Por el contrario, si el valor es grande, nos encontramos en el centro de la función, donde pequeñas variaciones pueden alterar considerablemente la salida.
- $\left(c^{j}-y^{j}\right)$ representa la medida del error que se produce en la neurona $j$.
- $x_{i}^{j}$ indica la *responsabilidad* de la entrada $i$ de la neurona $j$ en el error. 
  - Cuando este valor es igual a cero, no se modifica el peso, mientras que si es superior a cero, se modifica proporcionalmente a este valor.

La delta que corresponde a la neurona $j$ puede expresarse de forma general para toda neurona, simplificando la notación anterior:

$$
\begin{gather*}
\delta^{j}=\sigma^{\prime}\left(z^{j}\right)\left(c^{j}-y^{j}\right)  \tag{3.25}\\
\Delta w_{i}^{j}=\eta \delta^{j} x_{i}^{j} \tag{3.26}
\end{gather*}
$$

A partir de aquí debemos determinar qué parte del error total se asigna a cada una de las neuronas de las capas ocultas. Es decir, debemos definir *cómo modificar los pesos y tasas de aprendizaje de las neuronas de las capas ocultas a partir del error observado en la capa de salida*.

El método de retropropagación (backpropagation) se basa en un esquema general de dos pasos:

1. Propagación hacia adelante (feedforward), que consiste en introducir una instancia de entrenamiento y obtener la salida de la red neuronal.
2. Propagación hacia atrás (backpropagation), que consiste en calcular el error cometido en la capa de salida y propagarlo hacia atrás para calcular los valores delta de las neuronas de las capas ocultas.

El algoritmo siguiente muestra el pseudocódigo del método de retropropagación. Para ver el detalle del método y su derivación matemática se puede consultar el apéndice B (Rumelhart, McClelland y col., 1986).

El entrenamiento del algoritmo se realiza ejemplo a ejemplo, es decir, una instancia de entrenamiento en cada iteración. Para cada una de estas instancias se realizan los siguientes pasos:

1. El primer paso, que es la propagación hacia adelante, consiste en aplicar el ejemplo a la red y obtener los valores de salida (línea 3 ).
2. A continuación, el método inicia la propagación hacia atrás, empezando por la capa de salida. Para cada neurona $j$ de la capa de salida:

a) Se calcula, en primera instancia, el valor $\delta^{j}$ basado en el valor de salida de la red para la neurona $j$ $\left(y^{j}\right)$, el valor de la clase de la instancia $\left(c^{j}\right)$ y la derivada de la función sigmoide $\left(\sigma^{\prime}\left(z^{j}\right)\right)$ (línea 5).

b) A continuación, se modifica el vector de pesos de la neurona de la capa de salida, a partir de la tasa de aprendizaje $(\eta)$, el valor delta de la neurona
calculado en el paso anterior $\left(\delta^{j}\right)$ y el factor $x_{i}^{j}$ que indica la responsabilidad de la entrada $i$ de la neurona $j$ en el error (línea 6 ).


3. Finalmente, la propagación hacia atrás se aplica a las capas ocultas de la red. Para cada neurona $k$ de las capas ocultas:
a) En primer lugar, se calcula el valor $\delta^{k}$ basado en la derivada de la función sigmoide $\left(\sigma^{\prime}\left(z^{k}\right)\right)$ y el sumatorio del producto de la delta calculada en el paso anterior $\left(\delta^{k}\right)$ por el valor $w_{k}^{j}$, que indica el peso de la conexión entre la neurona $k$ y la neurona $j$ (línea 9). El conjunto $S_{k}$ está formado por todos los nodos de salida a los que se encuentra conectada la neurona $k$.

b) En el último paso de la iteración, se modifica el vector de pesos de la neurona $k$, a partir de la tasa de aprendizaje $(\eta)$, el valor delta de la neurona calculado en el paso anterior $\left(\delta^{k}\right)$ y el factor $x_{i}^{k}$ que indica la responsabilidad de la entrada $k$ de la neurona $j$ en el error (línea 10).

La idea que subyace a este algoritmo es relativamente sencilla, y se basa en propagar el error de forma proporcional a la influencia que ha tenido cada nodo de las capas ocultas en el error final producido por cada una de las neuronas de la capa de salida.


### Pseudocódigo del algoritmo

*El texto siguiente está adaptado del excelente texto  publicado por la UOC [Introducción al Deep Learning](https://www.editorialuoc.com/deep-learning)*

**Algoritmo 2 Seudocódigo del método de retropropagación**

**Entrada**: $W$ (conjunto de vectores de pesos) y $D$ (conjunto de instancias de entrenamiento)

1: **mientras** (error de la red $>\varepsilon$ ) hacer

2: $\quad$ **para todo** $\left(\left(d_{i}, c_{i}\right) \in D\right)$ hacer

3: $\qquad$ Calcular el valor de salida de la red para la entrada $d_{i}$

4: $\qquad$ **para todo** (neurona $j$ en la capa de salida) hacer

5: $\qquad\quad$ Calcular el valor $\delta^{j}$ para esta neurona: $$\delta^{j}=\sigma^{\prime}\left(z^{j}\right)\left(c^{j}-y^{j}\right)$$

6: $\qquad\quad$ Modificar los pesos de la neurona siguiendo el método del gradiente:

$$
\Delta w_{i}^{j}=\eta \delta^{j} x_{i}^{j}
$$

7: $\quad$ **fin para**

8: $\quad$ **para todo** (neurona $k$ en las capas ocultas) hacer

9: $\qquad$ Calcular el valor $\delta^{k}$ para esta neurona:

$$
\delta^{k}=\sigma^{\prime}\left(z^{k}\right) \sum_{j \in S_{k}} \delta^{k} w_{k}^{j}
$$

10: $\qquad$ Modificar los pesos de la neurona siguiendo el método del gradiente:

$$
\Delta w_{i}^{k}=\eta \delta^{k} x_{i}^{k}
$$

11:$\qquad$ **fin para**

12: $\quad$ **fin para**

13: **fin mientras**

## Formalizando el algoritmo 

Supongamos que tenemos una red neuronal de $L$ capas completamente conectadas. Utilizaremos la siguiente notación:

- $n_{l}$ es el número de neuronas de la capa $l$. Por convención consideramos $n_{0}$ la dimensión de los datos de entrada.
- $X \in \mathcal{M}_{n_{0} \times m}(\mathbb{R})$ es la matriz de datos de entrada, donde cada columna representa un ejemplo y cada fila representa un atributo.
- $W^{[l]} \in \mathcal{M}_{n_{l} \times n_{l-1}}(\mathbb{R})$ denota la matriz de pesos que conecta la capa $l-1$ con la capa $l$. Más concretamente, el
elemento de $W^{[l]}$ correspondiente a la fila $j$, columna $k$, denotado $w_{j k}^{[l]}$, es un escalar que representa el peso de la conexión entre la neurona $j$ de la capa $l$ y la neurona $k$ de la capa $l-1$.
- $b^{[l]} \in \mathcal{M}_{n_{l} \times 1}(\mathbb{R})$ es un vector que denota el bias de la capa $l$. El bias correspondiente a la neurona $j$ de la capa $l$ lo denotamos $b_{j}^{[l]}$.
- $z^{[l]} \in \mathcal{M}_{n_{l} \times 1}(\mathbb{R})$ denota la combinación lineal de la entrada a la capa $l$ con los parámetros $W^{[l]}$ y $b^{[l]}$.
- $g: \mathbb{R} \rightarrow \mathbb{R}$ es una función no lineal (como relu o sigmoid). Si $M \in \mathcal{M}_{c \times d}(\mathbb{R})$ es una matriz (o un vector), denotaremos $g(M) \in \mathcal{M}_{c \times d}(\mathbb{R})$ la matriz (o el vector) que se obtiene aplicando la función $g$ a cada coordenada de $M$.
- $a^{[l]} \in \mathcal{M}_{n_{l} \times 1}(\mathbb{R})$ denota el vector salida de la capa $l$ de la red neuronal. En particular $a^{[L]}$ denota la salida de la red neuronal. Por convención, denotaremos $a^{[0]}$ el vector de atributos de entrada a la red neuronal.
- Denotaremos el producto de matrices con el símbolo *, el producto de escalares con el símbolo $\cdot$ y el producto componente a componente con el símbolo $\odot$.

### Caso particular con un único ejemplo

Para explicar con detalle la retropropagación empezamos con una red trivial que recibe un único valor (vectorial) como entrada.

#### Propagación hacia delante

Si tenemos un único ejemplo entonces $m=1$ y $X$ es un vector columna de longitud $n_{0}$, el número de atributos, que coincide con la dimensión de la entrada a la red neuronal.

Entonces, según la convención que estamos utilizando, $a^{[0]}=X$.

En este caso, las ecuaciones para la propagación hacia delante son:

$$
\begin{aligned}
z_{j}^{[l]} & =\sum_{k=1}^{n_{l-1}} w_{j k}^{[l]} \cdot a_{k}^{[l-1]}+b_{j}^{[l]} \\
a_{j}^{[l]} & =g\left(z_{j}^{[l]}\right)
\end{aligned}
$$

Para poder calcular todas las componentes a la vez es posible escribir las fórmulas anteriores en versión matricial de la siguiente forma:

$$
\begin{aligned}
& z^{[l]}=W^{[l]} * a^{[l-1]}+b^{[l]} \\
& a^{[l]}=g\left(z^{[l]}\right)
\end{aligned}
$$

Una vez hemos aplicado las fórmulas anteriores a todas las capas obtenemos la salida de la red, $a^{[L]}$. Para saber si la salida de la red es adecuada podemos definir una función que mida qué error comete la red neuronal en la salida, $\mathcal{L}\left(y, a^{[L]}\right)$, donde $y$ es la etiqueta correcta asociada al ejemplo con el que trabajamos. Si la etiqueta es binaria, es decir siempre vale 0 o 1, entonces la salida de la red neuronal está formada por un único valor (esto es, $n_{L}=1$ ) y una posible función de error es el log-loss:

$$
\mathcal{L}\left(y, a^{[L]}\right)=-\left(y \cdot \log \left(a^{[L]}\right)+(1-y) \cdot \log \left(1-a^{[L]}\right)\right)
$$

Es importante notar que hemos multiplicado por -1 la expresión dentro de los paréntesis para que el error sea positivo y minimizar el error se corresponda con minimizar la función de coste.


#### Propagación hacia atrás

Observemos que en el cálculo de $a^{[L]}$ intervienen todos los parámetros $W^{[l]}, b^{[l]}$, con $l=1, \ldots, L$. Por lo tanto, la función de coste $\mathcal{L}\left(y, a^{[L]}\right)$ también depende de los parámetros $W^{[l]}, b^{[l]}$, para todo $l=1, \ldots, L$.

Dado que la función de coste mide la distancia entre la salida de la red y la etiqueta correcta podemos intentar minimizar esta función para acercar lo máximo posible los valores predichos a los valores correctos. Para minimizar la función de coste debemos modificar los parámetros de la red de forma adecuada, y para ello podemos calcular el gradiente de la función de coste respecto a los parámetros de la red y utilizarlo para actualizar los valores de los parámetros.

Para calcular el gradiente de la función de coste respecto a los parámetros de la red utilizaremos el algoritmo de la propagación hacia atrás. Este algoritmo se basa en aplicar repetidamente la regla de la cadena para calcular las derivadas parciales de la función de coste respecto cualquier parámetro de la red neuronal.

### Recordatorio (2) La regla de la cadena

Supongamos que tenemos dos funciones de una variable:

$$
\begin{aligned}
& f: \mathbb{R} \rightarrow \mathbb{R}, \quad g: \mathbb{R} \rightarrow \mathbb{R}, \\
& x \mapsto f(x) \\
& y \mapsto g(y)
\end{aligned}
$$

Entonces, por la regla de la cadena, la derivada de la composición $g(f(x))$ respecto $x$ viene dada por el producto:

$$
\frac{\partial(g \circ f)}{\partial x}=\frac{\partial g}{\partial f} \cdot \frac{\partial f}{\partial x}
$$

Ahora supongamos que tenemos dos funciones en varias variables como las siguientes:

$$
\begin{aligned}
& f: \mathbb{R} \rightarrow \quad \mathbb{R}^{d}, \\
& x \mapsto\left(f_{1}(x), \ldots, f_{d}(x)\right) . \\
& g: \mathbb{R}^{d} \quad \rightarrow \quad \mathbb{R}, \\
& \left(y_{1}, \ldots, y_{d}\right) \mapsto g\left(y_{1}, \ldots, y_{d}\right) .
\end{aligned}
$$

Entonces, por la regla de la cadena en varias variables, la derivada de la composición $g(f(x))$ respecto $x$ viene dada por la fórmula:

$$
\frac{\partial(g \circ f)}{\partial x}=\sum_{c=1}^{d} \frac{\partial g}{\partial f_{c}} \cdot \frac{\partial f_{c}}{\partial x}
$$

Utilizando la regla de la cadena podemos calcular entonces la derivada parcial de la función de coste con respecto a cualquier parámetro de la red neuronal.

El primer paso es calcular el gradiente respecto a la salida de la red neuronal. Esto dependerá de la función de coste $\mathcal{L}$ que se utilice, pero se puede calcular de forma analítica. Por ejemplo, en el caso de la función de coste log-loss, tenemos:

$$
\frac{\partial \mathcal{L}}{\partial a^{[L]}}=-\left(\frac{y}{a^{[L]}}-\frac{1-y}{1-a^{[L]}}\right)
$$

En general, asumiremos que hemos calculado $\frac{\partial L}{\partial a^{[L]}}$ y que lo podemos utilizar en la regla de la cadena. Supongamos que queremos calcular la derivada de la función de coste respecto a un peso concreto de la red neuronal $w_{j k}^{[l]}$. Para ello, asumimos que todos los demás parámetros son constantes y tenemos entonces la siguiente composición de funciones:

$$
\begin{aligned}
& \mathbb{R} \rightarrow \mathbb{R} \rightarrow \mathbb{R} \rightarrow \mathbb{R}, \\
& w_{j k}^{[l]} \mapsto z_{j}^{[l]} \mapsto a_{j}^{[l]} \mapsto \mathcal{L}
\end{aligned}
$$

Por lo que, aplicando repetidamente la regla de la cadena en una variable, obtenemos:

$$
\frac{\partial \mathcal{L}}{\partial w_{j k}^{[l]}}=\frac{\partial \mathcal{L}}{\partial a_{j}^{[l]}} \cdot \frac{\partial a_{j}^{[l]}}{\partial z_{j}^{[l]}} \cdot \frac{\partial z_{j}^{[l]}}{\partial w_{j k}^{[l]}}
$$

Si estamos trabajando con la última capa, entonces $l=L$ $\mathrm{y}$, por lo tanto, ya tenemos calculado el valor de $\frac{\partial \mathcal{L}}{\partial a_{j}^{(\bar{l}}}$. Asumamos por ahora que, aunque $l$ sea menor que $L$ ya tenemos calculado el valor de $\frac{\partial \mathcal{L}}{\partial a_{j}^{(I)}}$, posteriormente veremos como se calcula. Entonces, utilizando las ecuaciones 2.1 y 2.2 tenemos:

$$
\begin{gathered}
\frac{\partial a_{j}^{[l]}}{\partial z_{j}^{[l]}}=g^{\prime}\left(z_{j}^{[l]}\right) \\
\frac{\partial z_{j}^{[l]}}{\partial w_{j k}^{[l]}}=a_{k}^{[l-1]}
\end{gathered}
$$

Supongamos ahora que queremos calcular la derivada $\frac{\partial \mathcal{L}}{\partial b_{j}^{[\tau]}}$. El procedimiento es análogo a lo que hemos hecho hasta ahora, tenemos en este caso la siguiente descomposición:

$$
\begin{aligned}
& \mathbb{R} \rightarrow \mathbb{R} \rightarrow \mathbb{R} \rightarrow \mathbb{R}, \\
& b_{j}^{[l]} \mapsto z_{j}^{[l]} \mapsto a_{j}^{[l]} \mapsto \mathcal{L} .
\end{aligned}
$$

Por lo que, de nuevo aplicando la regla de la cadena en una variable, obtenemos:

$$
\frac{\partial \mathcal{L}}{\partial b_{j}^{[l]}}=\frac{\partial \mathcal{L}}{\partial a_{j}^{[l]}} \cdot \frac{\partial a_{j}^{[l]}}{\partial z_{j}^{[l]}} \cdot \frac{\partial z_{j}^{[l]}}{\partial b_{j}^{[l]}}
$$

Volviendo a asumir que tenemos calculada la derivada $\frac{\partial \mathcal{L}}{\partial a_{j}^{(I)}}$, podemos calcular $\frac{\partial \mathcal{L}}{\partial b_{j}^{(I)}}$ a partir de:

$$
\begin{aligned}
& \frac{\partial a_{j}^{[l]}}{\partial z_{j}^{[l]}}=g^{\prime}\left(z_{j}^{[l]}\right) \\
& \frac{\partial z_{j}^{[l]}}{\partial b_{j}^{[l]}}=1
\end{aligned}
$$

Si queremos calcular las derivadas de la función de coste respecto a todos las componentes de una matriz o un vector a la vez, podemos utilizar la siguiente notación para la matriz de pesos:

$$
\frac{\partial \mathcal{L}}{\partial W^{[l]}}=\left(\frac{\partial \mathcal{L}}{\partial a^{[l]}} \odot g^{\prime}\left(z^{[l]}\right)\right) *\left(a^{[l-1]}\right)^{T}
$$

Donde

$$
\frac{\partial \mathcal{L}}{\partial a^{[l]}}, g^{\prime}\left(z^{[l]}\right) \in \mathcal{M}_{n_{l} \times 1}(\mathbb{R})
$$

$\mathrm{y}$

$$
\left(a^{[l-1]}\right)^{T} \in \mathcal{M}_{1 \times n_{l-1}}(\mathbb{R})
$$

por lo que

$$
\frac{\partial \mathcal{L}}{\partial W^{[]}} \in \mathcal{M}_{n_{l} \times n_{l-1}}(\mathbb{R})
$$

Y la siguiente notación para el vector de bias:

$$
\frac{\partial \mathcal{L}}{\partial b^{[l]}}=\frac{\partial \mathcal{L}}{\partial a^{[l]}} \odot g^{\prime}\left(z^{[l]}\right)=\frac{\partial \mathcal{L}}{\partial z^{[l]}} .
$$

Donde, en este caso, $\frac{\partial \mathcal{L}}{\partial b[\overline{ }} \in \mathcal{M}_{n_{l} \times 1}(\mathbb{R})$.

Por último, necesitamos poder calcular el valor de $\frac{\partial \mathcal{L}}{\partial a_{j}^{[\overline{ }}}$ para $l<L$. En este caso, la activación de la neurona $j$ en la capa
$l$ afecta a todas las neuronas de la capa $l+1$, por lo que la descomposición en funciones que tenemos es la siguiente:

$$
\begin{array}{cccccc}
\mathbb{R} & \rightarrow \mathbb{R}^{n_{l+1}} & \rightarrow & \mathbb{R}^{n_{l+1}} & \rightarrow \mathbb{R} \\
a_{j}^{[l]} & \mapsto\left(z_{1}^{[l+1]}, \ldots, z_{n_{l+1}}^{[l+1]}\right) & \mapsto\left(a_{1}^{[l+1]}, \ldots, a_{n_{l+1}}^{[l+1]}\right) & \mapsto \mathcal{L}
\end{array}
$$

Y si aplicamos la regla de la cadena en varias variables obtenemos la siguiente fórmula:

$$
\frac{\partial \mathcal{L}}{\partial a_{j}^{[l]}}=\sum_{c=1}^{n_{l+1}} \frac{\partial \mathcal{L}}{\partial a_{c}^{[l+1]}} \cdot \frac{\partial a_{c}^{[l+1]}}{\partial z_{c}^{[l+1]}} \cdot \frac{\partial z_{c}^{[l+1]}}{\partial a_{j}^{[l]}}
$$

Ahora sí, por recursividad, podemos suponer que tenemos calculada la derivada $\frac{\partial \mathcal{L}}{\partial a_{c}^{L+1]}}$ para todo $c=1, \ldots, n_{l+1}$. Por lo tanto, podemos calcular completamente la derivada $\frac{\partial \mathcal{L}}{\partial a_{j}^{([)}}$ utilizando:

$$
\begin{aligned}
& \frac{\partial a_{c}^{[l+1]}}{\partial z_{c}^{[l+1]}}=g^{\prime}\left(z_{c}^{[l+1]}\right) \\
& \frac{\partial z_{c}^{[l+1]}}{\partial a_{j}^{[l]}}=w_{c j}^{[l+1]}
\end{aligned}
$$

El cálculo de las derivadas respecto a las activaciones de las neuronas también se puede hacer para todas las componentes a la vez utilizando la siguiente notación matricial:

$$
\frac{\partial \mathcal{L}}{\partial a^{[l]}}=\left(W^{[l+1]}\right)^{T} *\left(\frac{\partial \mathcal{L}}{\partial a^{[l+1]}} \odot g^{\prime}\left(z^{[l+1]}\right)\right)
$$

Donde

$$
\left(W^{[l+1]}\right)^{T} \in \mathcal{M}_{n_{l} \times n_{l+1}}(\mathbb{R}) \text { y } \frac{\partial \mathcal{L}}{\partial a^{l++1]}}, g^{\prime}\left(z^{[l+1]}\right) \in \mathcal{M}_{n_{l+1} \times 1}(\mathbb{R})
$$

por lo que $\frac{\partial \mathcal{L}}{\partial a^{l l}} \in \mathcal{M}_{n_{l} \times 1}(\mathbb{R})$.

Observemos que para hacer los cálculos que hemos especificado necesitamos saber los valores de $z^{[l]}$ y $a^{[l]}$ para todo $l=1, \ldots, L$, que se han calculado anteriormente durante la propagación hacia delante.

A continuación resumimos los pasos para calcular el gradiente con un único ejemplo en el conjunto de datos.

### Algoritmo de propagación hacia atrás con un ejemplo

1. Calcular $\frac{\partial \mathcal{L}}{\partial a[L]}$.
2. Desde $l=L$ hasta 1 , repetir:

a) Calcular $\frac{\partial \mathcal{L}}{\partial W^{[l]}}=\left(\frac{\partial \mathcal{L}}{\partial a^{l l}} \odot g^{\prime}\left(z^{[l]}\right)\right) *\left(a^{[l-1]}\right)^{T}$.

b) Calcular $\frac{\partial \mathcal{L}}{\partial b^{[l]}}=\frac{\partial \mathcal{L}}{\partial a^{[l]}} \odot g^{\prime}\left(z^{[l]}\right)$.

c) Calcular $\frac{\partial \mathcal{L}}{\partial a^{l l-1]}}=\left(W^{[l]}\right)^{T} *\left(\frac{\partial \mathcal{L}}{\partial a^{[l]}} \odot g^{\prime}\left(z^{[l]}\right)\right)$.

3. Devolver $\frac{\partial \mathcal{L}}{\partial W^{l l}}$ y $\frac{\partial \mathcal{L}}{\partial b^{l l}}$ para todo $l=1, \ldots, L$.

En la descripción del algoritmo se puede ver por qué se llama «de propagación hacia atrás». En efecto, el algoritmo se basa en calcular las derivadas $\frac{\partial \mathcal{L}}{\partial a^{[l}}$ en cada capa y propagar su valor hacia atrás para permitir el cálculo de las derivadas $\frac{\partial \mathcal{L}}{\partial W^{[l]}}$ y $\frac{\partial \mathcal{L}}{\partial b[\text { I] }}$, que son los parámetros de la red neuronal que se pueden modificar.

### Caso general con varios ejemplos

#### Propagación hacia delante

Asumamos ahora que tenemos varios ejemplos en la matriz de datos $X$, por lo que $m>1$. Si nos fijamos en los diferentes
valores que consideramos en el apartado de notación, veremos que los únicos que dependen de los datos (a parte de $X$ ), son $z^{[l]}$ y $a^{[l]}$, que son vectores columna. Podemos considerar entonces formar matrices colocando los vectores columna correspondientes a varios ejemplos uno al lado del otro. De esta forma, obtenemos:

$$
\begin{aligned}
Z^{[l]} & =\left(\begin{array}{llll}
z^{[l](1)} & z^{[l](2)} & \cdots & z^{[l](m)}
\end{array}\right) \\
A^{[l]} & =\left(\begin{array}{llll}
a^{[l](1)} & a^{[l](2)} & \cdots & a^{[l](m)}
\end{array}\right)
\end{aligned}
$$

Donde $z^{[l](i)}$ y $a^{[l](i)}$ denotan los vectores $z^{[l]}$ y $a^{[l]}$ que corresponden al ejemplo $i$-ésimo, respectivamente, y hemos denotado con $A$ y $Z$ mayúsculas las matrices resultantes.

Utilizando la misma convención que anteriormente tenemos que $A^{[0]}=X$ y las ecuaciones matriciales de la propagación hacia delante se pueden escribir como:

$$
\begin{aligned}
Z^{[l]} & =W^{[l]} * A^{[l-1]}+B^{[l]} \\
A^{[l]} & =g\left(Z^{[l]}\right)
\end{aligned}
$$

Donde $B^{[l]}$ es una matriz formada por el vector columna $b^{[l]}$ repetido $m$ veces. De esta forma, $A^{[L]}$ denota la salida de la red, donde cada columna corresponde a la salida para cada ejemplo.

$\mathrm{Al}$ tener varios ejemplos la función de coste global se define como la media de la función de coste para cada error, es decir:

$$
J=\frac{1}{m} \sum_{i=1}^{m} \mathcal{L}\left(y^{(i)}, a^{[L](i)}\right)
$$

En este caso entonces nos interesa minimizar la función $J$ para conseguir que la salida de la red se aproxime a las etiquetas correctas de cada ejemplo.

#### Propagación hacia atrás

Dado que la acción de derivar es una transformación lineal, para calcular el gradiente de la función $J$ podemos calcular el gradiente de $\mathcal{L}$ para cada ejemplo por separado como hemos hecho en la sección anterior y posteriormente hacer la media.

Con esta información ya podríamos implementar el algoritmo de propagación hacia atrás completo con varios ejemplos. Sin embargo, dado que los procesadores actuales están diseñados para realizar cálculos en paralelo, es mucho más eficiente calcular el gradiente para todos los ejemplos a la vez utilizando matrices.

Para ello, podemos utilizar las fórmulas 2.15, 2.16 y $2.20 \mathrm{y}$ adecuarlas a las substituciones $a^{[l]} \rightarrow A^{[l]} \mathrm{y} z^{[l]} \rightarrow Z^{[l]}$.

Concretamente, para la derivada $\frac{\partial J}{\partial W^{[J}}$ obtenemos:

$$
\frac{\partial J}{\partial W^{[l]}}=\frac{1}{m}\left(\frac{\partial \mathcal{L}}{\partial A^{[l]}} \odot g^{\prime}\left(Z^{[l]}\right)\right) *\left(A^{[l-1]}\right)^{T}
$$

Donde

$$
\frac{\partial \mathcal{L}}{\partial A^{[]}}, g^{\prime}\left(Z^{[l]}\right) \in \mathcal{M}_{n_{l} \times m}(\mathbb{R}) \text { y }\left(A^{[l-1]}\right)^{T} \in \mathcal{M}_{m \times n_{l-1}}(\mathbb{R})
$$

por lo que $\frac{\partial J}{\partial W^{[l}} \in \mathcal{M}_{n_{l} \times n_{l-1}}(\mathbb{R})$. Observemos que el producto de matrices provoca que en cada componente se están sumando los valores correspondientes de todos los ejemplos, por lo que simplemente dividiendo por $m$ obtenemos la media que necesitamos.

Para la derivada $\frac{\partial J}{\partial b \text { bil }}$ podemos hacer:

$$
\frac{\partial J}{\partial b^{[l]}}=\frac{1}{m} \sum_{i=1}^{m} \frac{\partial \mathcal{L}}{\partial A^{[l]}(i)} \odot g^{\prime}\left(Z^{[l](i)}\right)=\frac{1}{m} \sum_{i=1}^{m} \frac{\partial \mathcal{L}}{\partial Z^{[l](i)}}
$$

Donde, en este caso, estamos obteniendo los valores de cada ejemplo separados en columnas, por lo que debemos hacer la media de las columnas para obtener el valor de $\frac{\partial J}{\partial b[l]} \in \mathcal{M}_{n_{l} \times 1}(\mathbb{R})$.

Por último, en las fórmulas anteriores se puede ver que no es necesario calcular $\frac{\partial J}{\partial A^{[l]}}$ para obtener $\frac{\partial J}{\partial W^{[l]}}$ y $\frac{\partial J}{\partial b b^{[J}}$, pero sí necesitamos las derivadas $\frac{\partial \mathcal{L}}{\partial A^{[]}}$. Para conseguirlas podemos adaptar directamente la fórmula $2.20 \mathrm{y}$ considerar:

$$
\frac{\partial \mathcal{L}}{\partial A^{[l]}}=\left(W^{[l+1]}\right)^{T} *\left(\frac{\partial \mathcal{L}}{\partial A^{[l+1]}} \odot g^{\prime}\left(Z^{[l+1]}\right)\right)
$$

Donde los valores correspondientes a cada ejemplo se guardan, también en este caso, separados por columnas, que es exactamente lo que nos interesa.

Finalmente, podemos resumir todo el algoritmo de propagación hacia atrás general, con cualquier número de ejemplos y en formato matricial, con el siguiente procedimiento.

### Algoritmo de propagación hacia atrás con varios ejemplos

1. Calcular $\frac{\partial L}{\partial a^{\lfloor L}}$.
2. Desde $l=L$ hasta 1 , repetir:

a) Calcular $\frac{\partial J}{\partial W^{[l]}}=\frac{1}{m}\left(\frac{\partial \mathcal{L}}{\partial A^{[l]}} \odot g^{\prime}\left(Z^{[l]}\right)\right) *\left(A^{[l-1]}\right)^{T}$.

b) Calcular $\frac{\partial J}{\partial b[]}=\frac{1}{m} \sum_{i=1}^{m} \frac{\partial L}{\partial A[][(i)} \odot g^{\prime}\left(Z^{[l]}(i)\right)$.

c) Calcular $\frac{\partial \mathcal{L}}{\partial A^{[l]}}=\left(W^{[l+1]}\right)^{T} *\left(\frac{\partial \mathcal{L}}{\partial A^{[l+1]}} \odot g^{\prime}\left(Z^{[l+1]}\right)\right)$.

3. Devolver $\frac{\partial J}{\partial W^{[l}} \mathrm{y} \frac{J}{\partial b b^{T J}}$ para todo $l=1, \ldots, L$.


# An example

In this example we train and use a "shallow neural network", called this way in contrast with "deep neural networks".

We will use the `neuralnet` R package, which is not intended to work
with deep neural networks, to build a simple neural network to predict
if a type of stock pays dividends or not.

```{r}
if (!require(neuralnet)) 
  install.packages("neuralnet", dep=TRUE)
if (!require(caret)) 
  install.packages("caret", dep=TRUE)
```

The data for the example are the `dividendinfo.csv` dataset, available
from: <https://github.com/MGCodesandStats/datasets>

```{r}
mydata <- read.csv("https://raw.githubusercontent.com/MGCodesandStats/datasets/master/dividendinfo.csv")
str(mydata)
```

## Data pre-processing

One of the most important procedures when forming a neural network is
data normalization. This involves adjusting the data to a common scale
so as to accurately compare predicted and actual values. Failure to
normalize the data will typically result in the prediction value
remaining the same across all observations, regardless of the input
values.

We can do this in two ways in R:

-   Scale the data frame automatically using the scale function in R
-   Transform the data using a max-min normalization technique

In this example We implement the max-min normalization technique.

See [this
link](https://vitalflux.com/data-science-scale-normalize-numeric-data-using-r/)
for further details on how to use the normalization function.

```{r}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
normData <- as.data.frame(lapply(mydata, normalize))
```

As usually, the dataset is separated in a training and a test set. The
training set contains a random selection with and (arbitrary) 66% of the
observations.

```{r}
perc2Train <- 2/3
ssize <- nrow(normData)
set.seed(12345)
data_rows <- floor(perc2Train *ssize)
train_indices <- sample(c(1:ssize), data_rows)
trainset <- normData[train_indices,]
testset <- normData[-train_indices,]
```

The `trainset` set will be used to train the network and the `testset`
set one will be used to evaluate it.

## Training a neural network

Setting the parameters of a neural network requires experience and
understanding of their meaning, and even so, changes in the parameters
can lead to similar results.

We create a simple NN with two hidden layers, with 3 and 2 neurons
respectively. This is specified in the `hidden` parameter. For other
parameters see [the package
help](https://www.rdocumentation.org/packages/neuralnet/versions/1.44.2/topics/neuralnet).

```{r}
# Neural Network
library(neuralnet)
nn <- neuralnet(dividend ~ fcfps + earnings_growth + de + mcap + current_ratio, 
                data=trainset, 
                hidden=c(3,2), 
                linear.output=FALSE, 
                threshold=0.01)
```

The output of the procedure is a neural network with estimated weights.

This can be seen with a `plot` function (including the `rep` argument).

```{r}
plot(nn, rep = "best")
```

The object `nn`contains information the weights and the results although
it is not particularly clear or useful.

```{r}
summary(nn)
nn$result.matrix
```

## Model evaluation

A prediction for each value in the `testset` dataset can be built with
the `compute` function.

```{r}
#Test the resulting output
temp_test <- subset(testset, select =
                      c("fcfps","earnings_growth", 
                        "de", "mcap", "current_ratio"))
head(temp_test)
nn.results <- compute(nn, temp_test)
results <- data.frame(actual = 
                  testset$dividend, 
                  prediction = nn.results$net.result)
head(results)
```

A confusion matrix can be built to evaluate the predictive ability of
the network:

```{r}

roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)
attach(roundedresultsdf)
confMat<- caret::confusionMatrix(table(actual, prediction))
confMat
```


# References and resources
